"use strict";(self.webpackChunkfluss_website=self.webpackChunkfluss_website||[]).push([[3592],{6212:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"engine-flink/options","title":"Connector Options","description":"\x3c!--","source":"@site/versioned_docs/version-0.7/engine-flink/options.md","sourceDirName":"engine-flink","slug":"/engine-flink/options","permalink":"/docs/engine-flink/options","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/fluss/edit/main/website/docs/engine-flink/options.md","tags":[],"version":"0.7","sidebarPosition":7,"frontMatter":{"title":"Connector Options","sidebar_position":7},"sidebar":"docsSidebar","previous":{"title":"DataStream API","permalink":"/docs/engine-flink/datastream"},"next":{"title":"Lakehouse Overview","permalink":"/docs/streaming-lakehouse/overview"}}');var n=i(4848),s=i(8453);const l={title:"Connector Options",sidebar_position:7},o="Connector Options",d={},a=[{value:"How to Configure Options",id:"how-to-configure-options",level:2},{value:"Configure options when creating a table",id:"configure-options-when-creating-a-table",level:3},{value:"Dynamically apply options via SQL hints",id:"dynamically-apply-options-via-sql-hints",level:3},{value:"Configure options by altering table",id:"configure-options-by-altering-table",level:3},{value:"Storage Options",id:"storage-options",level:2},{value:"Read Options",id:"read-options",level:2},{value:"Lookup Options",id:"lookup-options",level:2},{value:"Write Options",id:"write-options",level:2},{value:"Other Options",id:"other-options",level:2}];function c(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"connector-options",children:"Connector Options"})}),"\n",(0,n.jsx)(t.p,{children:"The following table lists the options that can be used to configure the Fluss table in Flink, including the storage format, the read/write behaviors."}),"\n",(0,n.jsx)(t.h2,{id:"how-to-configure-options",children:"How to Configure Options"}),"\n",(0,n.jsxs)(t.p,{children:["You can configure the Fluss table options in Flink SQL DDL statements. For example, the following SQL statement creates a Fluss table with the ",(0,n.jsx)(t.code,{children:"table.log.ttl"})," set to 7 days:"]}),"\n",(0,n.jsx)(t.h3,{id:"configure-options-when-creating-a-table",children:"Configure options when creating a table"}),"\n",(0,n.jsxs)(t.p,{children:["All the options in this page can be set when creating a table. Once a table is created, the defined options will be stored as a part of the metadata of the Fluss table. The stored options will be used when reading or writing the table.\nFor example, the following SQL statement creates a Fluss table with the ",(0,n.jsx)(t.code,{children:"table.log.ttl"})," set to 7 days and disables the CRC check for the log reading:"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-sql",children:"CREATE TABLE log_table (\n  order_id BIGINT,\n  item_id BIGINT,\n  amount INT,\n  address STRING\n) WITH (\n  'table.log.ttl' = '7d',\n  'client.scanner.log.check-crc' = 'false'\n);\n"})}),"\n",(0,n.jsx)(t.h3,{id:"dynamically-apply-options-via-sql-hints",children:"Dynamically apply options via SQL hints"}),"\n",(0,n.jsx)(t.p,{children:"You can dynamically apply the options via SQL hints. The dynamically applied options will not be stored as metadata of the table.\nThey will only be used for the current query and will not affect other queries on the table. The dynamically applied options have a higher priority than the options stored in the metadata of the table."}),"\n",(0,n.jsx)(t.admonition,{type:"note",children:(0,n.jsxs)(t.p,{children:["The ",(0,n.jsx)(t.a,{href:"#storage-options",children:"Storage Options"})," doesn't supported to be dynamically configured via SQL hints, because the storage behavior is determined when the table is created."]})}),"\n",(0,n.jsx)(t.p,{children:"For example, the following SQL statements temporarily disables the CRC check for the log reading and ignores deletes for writing:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-sql",children:"-- SQL hints on source tables\nSELECT * FROM log_table /*+ OPTIONS('client.scanner.log.check-crc' = 'false') */;\n\n-- SQL hints on sink tables\nINSERT INTO pk_table2 /*+ OPTIONS('sink.ignore-delete'='true') */ select * from pk_table1;\n"})}),"\n",(0,n.jsx)(t.h3,{id:"configure-options-by-altering-table",children:"Configure options by altering table"}),"\n",(0,n.jsxs)(t.p,{children:["This is not supported yet, but is planned in the near future.\nFor example, the following SQL statement alters the Fluss table with the ",(0,n.jsx)(t.code,{children:"table.log.ttl"})," set to 7 days:"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-sql",children:"ALTER TABLE log_table SET ('table.log.ttl' = '7d');\n"})}),"\n",(0,n.jsx)(t.h2,{id:"storage-options",children:"Storage Options"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Option"}),(0,n.jsx)(t.th,{children:"Type"}),(0,n.jsx)(t.th,{children:"Default"}),(0,n.jsx)(t.th,{children:"Description"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"bucket.num"}),(0,n.jsx)(t.td,{children:"int"}),(0,n.jsx)(t.td,{children:"The bucket number of Fluss cluster."}),(0,n.jsx)(t.td,{children:"The number of buckets of a Fluss table."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"bucket.key"}),(0,n.jsx)(t.td,{children:"String"}),(0,n.jsx)(t.td,{children:"(None)"}),(0,n.jsxs)(t.td,{children:["Specific the distribution policy of the Fluss table. Data will be distributed to each bucket according to the hash value of bucket-key (It must be a subset of the primary keys excluding partition keys of the primary key table). If you specify multiple fields, delimiter is ",(0,n.jsx)(t.code,{children:","}),". If the table has a primary key and a bucket key is not specified, the bucket key will be used as primary key(excluding the partition key). If the table has no primary key and the bucket key is not specified, the data will be distributed to each bucket randomly."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.log.ttl"}),(0,n.jsx)(t.td,{children:"Duration"}),(0,n.jsx)(t.td,{children:"7 days"}),(0,n.jsx)(t.td,{children:"The time to live for log segments. The configuration controls the maximum time we will retain a log before we will delete old segments to free up space. If set to -1, the log will not be deleted."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.auto-partition.enabled"}),(0,n.jsx)(t.td,{children:"Boolean"}),(0,n.jsx)(t.td,{children:"false"}),(0,n.jsx)(t.td,{children:"Whether enable auto partition for the table. Disable by default. When auto partition is enabled, the partitions of the table will be created automatically."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.auto-partition.key"}),(0,n.jsx)(t.td,{children:"String"}),(0,n.jsx)(t.td,{children:"(None)"}),(0,n.jsx)(t.td,{children:"This configuration defines the time-based partition key to be used for auto-partitioning when a table is partitioned with multiple keys. Auto-partitioning utilizes a time-based partition key to handle partitions automatically, including creating new ones and removing outdated ones, by comparing the time value of the partition with the current system time. In the case of a table using multiple partition keys (such as a composite partitioning strategy), this feature determines which key should serve as the primary time dimension for making auto-partitioning decisions. And If the table has only one partition key, this config is not necessary. Otherwise, it must be specified."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.auto-partition.time-unit"}),(0,n.jsx)(t.td,{children:"ENUM"}),(0,n.jsx)(t.td,{children:"DAY"}),(0,n.jsxs)(t.td,{children:["The time granularity for auto created partitions. The default value is ",(0,n.jsx)(t.code,{children:"DAY"}),". Valid values are ",(0,n.jsx)(t.code,{children:"HOUR"}),", ",(0,n.jsx)(t.code,{children:"DAY"}),", ",(0,n.jsx)(t.code,{children:"MONTH"}),", ",(0,n.jsx)(t.code,{children:"QUARTER"}),", ",(0,n.jsx)(t.code,{children:"YEAR"}),". If the value is ",(0,n.jsx)(t.code,{children:"HOUR"}),", the partition format for auto created is yyyyMMddHH. If the value is ",(0,n.jsx)(t.code,{children:"DAY"}),", the partition format for auto created is yyyyMMdd. If the value is ",(0,n.jsx)(t.code,{children:"MONTH"}),", the partition format for auto created is yyyyMM. If the value is ",(0,n.jsx)(t.code,{children:"QUARTER"}),", the partition format for auto created is yyyyQ. If the value is ",(0,n.jsx)(t.code,{children:"YEAR"}),", the partition format for auto created is yyyy."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.auto-partition.num-precreate"}),(0,n.jsx)(t.td,{children:"Integer"}),(0,n.jsx)(t.td,{children:"2"}),(0,n.jsxs)(t.td,{children:["The number of partitions to pre-create for auto created partitions in each check for auto partition. For example, if the current check time is 2024-11-11 and the value is configured as 3, then partitions 20241111, 20241112, 20241113 will be pre-created. If any one partition exists, it'll skip creating the partition. The default value is 2, which means 2 partitions will be pre-created. If the ",(0,n.jsx)(t.code,{children:"table.auto-partition.time-unit"})," is ",(0,n.jsx)(t.code,{children:"DAY"}),"(default), one precreated partition is for today and another one is for tomorrow. For a partition table with multiple partition keys, pre-create is unsupported and will be set to 0 automatically when creating table if it is not explicitly specified."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.auto-partition.num-retention"}),(0,n.jsx)(t.td,{children:"Integer"}),(0,n.jsx)(t.td,{children:"7"}),(0,n.jsx)(t.td,{children:"The number of history partitions to retain for auto created partitions in each check for auto partition. For example, if the current check time is 2024-11-11, time-unit is DAY, and the value is configured as 3, then the history partitions 20241108, 20241109, 20241110 will be retained. The partitions earlier than 20241108 will be deleted. The default value is 7, which means that 7 partitions will be retained."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.auto-partition.time-zone"}),(0,n.jsx)(t.td,{children:"String"}),(0,n.jsx)(t.td,{children:"the system time zone"}),(0,n.jsx)(t.td,{children:"The time zone for auto partitions, which is by default the same as the system time zone."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.replication.factor"}),(0,n.jsx)(t.td,{children:"Integer"}),(0,n.jsx)(t.td,{children:"(None)"}),(0,n.jsx)(t.td,{children:"The replication factor for the log of the new table. When it's not set, Fluss will use the cluster's default replication factor configured by default.replication.factor. It should be a positive number and not larger than the number of tablet servers in the Fluss cluster. A value larger than the number of tablet servers in Fluss cluster will result in an error when the new table is created."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.log.format"}),(0,n.jsx)(t.td,{children:"Enum"}),(0,n.jsx)(t.td,{children:"ARROW"}),(0,n.jsxs)(t.td,{children:["The format of the log records in log store. The default value is ",(0,n.jsx)(t.code,{children:"ARROW"}),". The supported formats are ",(0,n.jsx)(t.code,{children:"ARROW"})," and ",(0,n.jsx)(t.code,{children:"INDEXED"}),"."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.log.arrow.compression.type"}),(0,n.jsx)(t.td,{children:"Enum"}),(0,n.jsx)(t.td,{children:"ZSTD"}),(0,n.jsxs)(t.td,{children:["The compression type of the log records if the log format is set to ",(0,n.jsx)(t.code,{children:"ARROW"}),". The candidate compression type is ",(0,n.jsx)(t.code,{children:"NONE"}),", ",(0,n.jsx)(t.code,{children:"LZ4_FRAME"}),", ",(0,n.jsx)(t.code,{children:"ZSTD"}),". The default value is ",(0,n.jsx)(t.code,{children:"ZSTD"}),"."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.log.arrow.compression.zstd.level"}),(0,n.jsx)(t.td,{children:"Integer"}),(0,n.jsx)(t.td,{children:"3"}),(0,n.jsxs)(t.td,{children:["The compression level of the log records if the log format is set to ",(0,n.jsx)(t.code,{children:"ARROW"})," and the compression type is set to ",(0,n.jsx)(t.code,{children:"ZSTD"}),". The valid range is 1 to 22. The default value is 3."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.kv.format"}),(0,n.jsx)(t.td,{children:"Enum"}),(0,n.jsx)(t.td,{children:"COMPACTED"}),(0,n.jsxs)(t.td,{children:["The format of the kv records in kv store. The default value is ",(0,n.jsx)(t.code,{children:"COMPACTED"}),". The supported formats are ",(0,n.jsx)(t.code,{children:"COMPACTED"})," and ",(0,n.jsx)(t.code,{children:"INDEXED"}),"."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.log.tiered.local-segments"}),(0,n.jsx)(t.td,{children:"Integer"}),(0,n.jsx)(t.td,{children:"2"}),(0,n.jsx)(t.td,{children:"The number of log segments to retain in local for each table when log tiered storage is enabled. It must be greater that 0. The default is 2."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.datalake.enabled"}),(0,n.jsx)(t.td,{children:"Boolean"}),(0,n.jsx)(t.td,{children:"false"}),(0,n.jsx)(t.td,{children:"Whether enable lakehouse storage for the table. Disabled by default. When this option is set to ture and the datalake tiering service is up, the table will be tiered and compacted into datalake format stored on lakehouse storage."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.datalake.format"}),(0,n.jsx)(t.td,{children:"Enum"}),(0,n.jsx)(t.td,{children:"(None)"}),(0,n.jsxs)(t.td,{children:["The data lake format of the table specifies the tiered Lakehouse storage format, such as Paimon, Iceberg, DeltaLake, or Hudi. Currently, only ",(0,n.jsx)(t.code,{children:"paimon"})," is supported. Once the ",(0,n.jsx)(t.code,{children:"table.datalake.format"})," property is configured, Fluss adopts the key encoding and bucketing strategy used by the corresponding data lake format. This ensures consistency in key encoding and bucketing, enabling seamless ",(0,n.jsx)(t.strong,{children:"Union Read"})," functionality across Fluss and Lakehouse. The ",(0,n.jsx)(t.code,{children:"table.datalake.format"})," can be pre-defined before enabling ",(0,n.jsx)(t.code,{children:"table.datalake.enabled"}),". This allows the data lake feature to be dynamically enabled on the table without requiring table recreation. If ",(0,n.jsx)(t.code,{children:"table.datalake.format"})," is not explicitly set during table creation, the table will default to the format specified by the ",(0,n.jsx)(t.code,{children:"datalake.format"})," configuration in the Fluss cluster"]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.datalake.freshness"}),(0,n.jsx)(t.td,{children:"Duration"}),(0,n.jsx)(t.td,{children:"3min"}),(0,n.jsx)(t.td,{children:"It defines the maximum amount of time that the datalake table's content should lag behind updates to the Fluss table. Based on this target freshness, the Fluss service automatically moves data from the Fluss table and updates to the datalake table, so that the data in the datalake table is kept up to date within this target. If the data does not need to be as fresh, you can specify a longer target freshness time to reduce costs."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.merge-engine"}),(0,n.jsx)(t.td,{children:"Enum"}),(0,n.jsx)(t.td,{children:"(None)"}),(0,n.jsxs)(t.td,{children:["Defines the merge engine for the primary key table. By default, primary key table uses the ",(0,n.jsx)(t.a,{href:"/docs/table-design/table-types/pk-table/merge-engines/default",children:"default merge engine(last_row)"}),". It also supports two merge engines are ",(0,n.jsx)(t.code,{children:"first_row"})," and ",(0,n.jsx)(t.code,{children:"versioned"}),". The ",(0,n.jsx)(t.a,{href:"/docs/table-design/table-types/pk-table/merge-engines/first-row",children:"first_row merge engine"})," will keep the first row of the same primary key. The ",(0,n.jsx)(t.a,{href:"/docs/table-design/table-types/pk-table/merge-engines/versioned",children:"versioned merge engine"})," will keep the row with the largest version of the same primary key."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"table.merge-engine.versioned.ver-column"}),(0,n.jsx)(t.td,{children:"String"}),(0,n.jsx)(t.td,{children:"(None)"}),(0,n.jsxs)(t.td,{children:["The column name of the version column for the ",(0,n.jsx)(t.code,{children:"versioned"})," merge engine. If the merge engine is set to ",(0,n.jsx)(t.code,{children:"versioned"}),", the version column must be set."]})]})]})]}),"\n",(0,n.jsx)(t.h2,{id:"read-options",children:"Read Options"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Option"}),(0,n.jsx)(t.th,{children:"Type"}),(0,n.jsx)(t.th,{children:"Default"}),(0,n.jsx)(t.th,{children:"Description"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"scan.startup.mode"}),(0,n.jsx)(t.td,{children:"Enum"}),(0,n.jsx)(t.td,{children:"full"}),(0,n.jsxs)(t.td,{children:["The scan startup mode enables you to specify the starting point for data consumption. Fluss currently supports the following ",(0,n.jsx)(t.code,{children:"scan.startup.mode"})," options: ",(0,n.jsx)(t.code,{children:"full"})," (default), earliest, latest, timestamp. See the ",(0,n.jsx)(t.a,{href:"/docs/engine-flink/reads#start-reading-position",children:"Start Reading Position"})," for more details."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"scan.startup.timestamp"}),(0,n.jsx)(t.td,{children:"Long"}),(0,n.jsx)(t.td,{children:"(None)"}),(0,n.jsxs)(t.td,{children:["The timestamp to start reading the data from. This option is only valid when ",(0,n.jsx)(t.code,{children:"scan.startup.mode"})," is set to ",(0,n.jsx)(t.code,{children:"timestamp"}),". The format is 'milli-second-since-epoch' or ",(0,n.jsx)(t.code,{children:"yyyy-MM-dd HH:mm:ss"}),", like ",(0,n.jsx)(t.code,{children:"1678883047356"})," or ",(0,n.jsx)(t.code,{children:"2023-12-09 23:09:12"}),"."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"scan.partition.discovery.interval"}),(0,n.jsx)(t.td,{children:"Duration"}),(0,n.jsx)(t.td,{children:"10s"}),(0,n.jsx)(t.td,{children:"The time interval for the Fluss source to discover the new partitions for partitioned table while scanning. A non-positive value disables the partition discovery."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.scanner.log.check-crc"}),(0,n.jsx)(t.td,{children:"Boolean"}),(0,n.jsx)(t.td,{children:"true"}),(0,n.jsx)(t.td,{children:"Automatically check the CRC3 of the read records for LogScanner. This ensures no on-the-wire or on-disk corruption to the messages occurred. This check adds some overhead, so it may be disabled in cases seeking extreme performance."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.scanner.log.max-poll-records"}),(0,n.jsx)(t.td,{children:"Integer"}),(0,n.jsx)(t.td,{children:"500"}),(0,n.jsx)(t.td,{children:"The maximum number of records returned in a single call to poll() for LogScanner. Note that this config doesn't impact the underlying fetching behavior. The Scanner will cache the records from each fetch request and returns them incrementally from each poll."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.scanner.log.fetch.max-bytes"}),(0,n.jsx)(t.td,{children:"MemorySize"}),(0,n.jsx)(t.td,{children:"16mb"}),(0,n.jsx)(t.td,{children:"The maximum amount of data the server should return for a fetch request from client. Records are fetched in batches, and if the first record batch in the first non-empty bucket of the fetch is larger than this value, the record batch will still be returned to ensure that the fetch can make progress. As such, this is not a absolute maximum."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.scanner.log.fetch.max-bytes-for-bucket"}),(0,n.jsx)(t.td,{children:"MemorySize"}),(0,n.jsx)(t.td,{children:"1mb"}),(0,n.jsx)(t.td,{children:"The maximum amount of data the server should return for a table bucket in fetch request fom client. Records are fetched in batches, and the max bytes size is config by this option."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.scanner.log.fetch.min-bytes"}),(0,n.jsx)(t.td,{children:"MemorySize"}),(0,n.jsx)(t.td,{children:"1b"}),(0,n.jsx)(t.td,{children:"The minimum bytes expected for each fetch log request from client to response. If not enough bytes, wait up to client.scanner.log.fetch-wait-max-time time to return."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.scanner.log.fetch.wait-max-time"}),(0,n.jsx)(t.td,{children:"Duration"}),(0,n.jsx)(t.td,{children:"500ms"}),(0,n.jsx)(t.td,{children:"The maximum time to wait for enough bytes to be available for a fetch log request from client to response."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.scanner.io.tmpdir"}),(0,n.jsx)(t.td,{children:"String"}),(0,n.jsx)(t.td,{children:'System.getProperty("java.io.tmpdir") + "/fluss"'}),(0,n.jsx)(t.td,{children:"Local directory that is used by client for storing the data files (like kv snapshot, log segment files) to read temporarily"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.scanner.remote-log.prefetch-num"}),(0,n.jsx)(t.td,{children:"Integer"}),(0,n.jsx)(t.td,{children:"4"}),(0,n.jsx)(t.td,{children:"The number of remote log segments to keep in local temp file for LogScanner, which download from remote storage. The default setting is 4."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.remote-file.download-thread-num"}),(0,n.jsx)(t.td,{children:"Integer"}),(0,n.jsx)(t.td,{children:"3"}),(0,n.jsx)(t.td,{children:"The number of threads the client uses to download remote files."})]})]})]}),"\n",(0,n.jsx)(t.h2,{id:"lookup-options",children:"Lookup Options"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Option"}),(0,n.jsx)(t.th,{children:"Type"}),(0,n.jsx)(t.th,{children:"Default"}),(0,n.jsx)(t.th,{children:"Description"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"lookup.async"}),(0,n.jsx)(t.td,{children:"Boolean"}),(0,n.jsx)(t.td,{children:"true"}),(0,n.jsx)(t.td,{children:"Whether to use asynchronous lookup. Asynchronous lookup has better throughput performance than synchronous lookup."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"lookup.cache"}),(0,n.jsx)(t.td,{children:"Enum"}),(0,n.jsx)(t.td,{children:"NONE"}),(0,n.jsx)(t.td,{children:"The caching strategy for this lookup table, including NONE, PARTIAL."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"lookup.max-retries"}),(0,n.jsx)(t.td,{children:"Integer"}),(0,n.jsx)(t.td,{children:"3"}),(0,n.jsx)(t.td,{children:"The maximum allowed retries if a lookup operation fails."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"lookup.partial-cache.expire-after-access"}),(0,n.jsx)(t.td,{children:"Duration"}),(0,n.jsx)(t.td,{children:"(None)"}),(0,n.jsx)(t.td,{children:"Duration to expire an entry in the cache after accessing."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"lookup.partial-cache.expire-after-write"}),(0,n.jsx)(t.td,{children:"Duration"}),(0,n.jsx)(t.td,{children:"(None)"}),(0,n.jsx)(t.td,{children:"Duration to expire an entry in the cache after writing."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"lookup.partial-cache.cache-missing-key"}),(0,n.jsx)(t.td,{children:"Boolean"}),(0,n.jsx)(t.td,{children:"true"}),(0,n.jsx)(t.td,{children:"Whether to store an empty value into the cache if the lookup key doesn't match any rows in the table."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"lookup.partial-cache.max-rows"}),(0,n.jsx)(t.td,{children:"Long"}),(0,n.jsx)(t.td,{children:"(None)"}),(0,n.jsx)(t.td,{children:"The maximum number of rows to store in the cache."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.lookup.queue-size"}),(0,n.jsx)(t.td,{children:"Integer"}),(0,n.jsx)(t.td,{children:"25600"}),(0,n.jsx)(t.td,{children:"The maximum number of pending lookup operations."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.lookup.max-batch-size"}),(0,n.jsx)(t.td,{children:"Integer"}),(0,n.jsx)(t.td,{children:"128"}),(0,n.jsx)(t.td,{children:"The maximum batch size of merging lookup operations to one lookup request."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.lookup.max-inflight-requests"}),(0,n.jsx)(t.td,{children:"Integer"}),(0,n.jsx)(t.td,{children:"128"}),(0,n.jsx)(t.td,{children:"The maximum number of unacknowledged lookup requests for lookup operations."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.lookup.batch-timeout"}),(0,n.jsx)(t.td,{children:"Duration"}),(0,n.jsx)(t.td,{children:"100ms"}),(0,n.jsx)(t.td,{children:"The maximum time to wait for the lookup batch to full, if this timeout is reached, the lookup batch will be closed to send."})]})]})]}),"\n",(0,n.jsx)(t.h2,{id:"write-options",children:"Write Options"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Option"}),(0,n.jsx)(t.th,{children:"Type"}),(0,n.jsx)(t.th,{children:"Default"}),(0,n.jsx)(t.th,{children:"Description"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"sink.ignore-delete"}),(0,n.jsx)(t.td,{children:"Boolean"}),(0,n.jsx)(t.td,{children:"false"}),(0,n.jsx)(t.td,{children:"If set to true, the sink will ignore DELETE and UPDATE_BEFORE changelog events."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"sink.bucket-shuffle"}),(0,n.jsx)(t.td,{children:"Boolean"}),(0,n.jsx)(t.td,{children:"true"}),(0,n.jsxs)(t.td,{children:["Whether to shuffle by bucket id before write to sink. Shuffling the data with the same bucket id to be processed by the same task can improve the efficiency of client processing and reduce resource consumption. For Log Table, bucket shuffle will only take effect when the ",(0,n.jsx)(t.code,{children:"bucket.key"})," is defined. For Primary Key table, it is enabled by default."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.writer.buffer.memory-size"}),(0,n.jsx)(t.td,{children:"MemorySize"}),(0,n.jsx)(t.td,{children:"64mb"}),(0,n.jsx)(t.td,{children:"The total bytes of memory the writer can use to buffer internal rows."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.writer.buffer.page-size"}),(0,n.jsx)(t.td,{children:"MemorySize"}),(0,n.jsx)(t.td,{children:"128kb"}),(0,n.jsxs)(t.td,{children:["Size of every page in memory buffers (",(0,n.jsx)(t.code,{children:"client.writer.buffer.memory-size"}),")."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.writer.buffer.per-request-memory-size"}),(0,n.jsx)(t.td,{children:"MemorySize"}),(0,n.jsx)(t.td,{children:"16mb"}),(0,n.jsx)(t.td,{children:"The minimum number of bytes that will be allocated by the writer rounded down to the closest multiple of client.writer.buffer.page-size. It must be greater than or equal to client.writer.buffer.page-size. This option allows to allocate memory in batches to have better CPU-cached friendliness due to contiguous segments."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.writer.batch-size"}),(0,n.jsx)(t.td,{children:"MemorySize"}),(0,n.jsx)(t.td,{children:"2mb"}),(0,n.jsx)(t.td,{children:"The writer or walBuilder will attempt to batch records together into one batch for the same bucket. This helps performance on both the client and the server."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.writer.dynamic-batch-size.enabled"}),(0,n.jsx)(t.td,{children:"Boolean"}),(0,n.jsx)(t.td,{children:"true"}),(0,n.jsxs)(t.td,{children:["Controls whether the client writer dynamically adjusts the batch size based on actual write throughput. Enabled by default. With dynamic batch sizing enabled, the writer adapts memory allocation per batch according to historical write sizes for the target table or partition. This ensures better memory utilization and performance under varying throughput conditions. The dynamic batch size is bounded: it will not exceed ",(0,n.jsx)(t.code,{children:"client.writer.batch-size"}),", nor fall below ",(0,n.jsx)(t.code,{children:"client.writer.buffer.page-size"}),". When disabled, the writer uses a fixed batch size (",(0,n.jsx)(t.code,{children:"client.writer.batch-size"}),") for all batches, this may lead to frequent memory waits and suboptimal write performance if the incoming data rate is inconsistent across partitions."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.writer.buffer.wait-timeout"}),(0,n.jsx)(t.td,{children:"Duration"}),(0,n.jsx)(t.td,{children:"2^(63)-1ns"}),(0,n.jsx)(t.td,{children:"Defines how long the writer will block when waiting for segments to become available."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.writer.batch-timeout"}),(0,n.jsx)(t.td,{children:"Duration"}),(0,n.jsx)(t.td,{children:"100ms"}),(0,n.jsx)(t.td,{children:"The writer groups ay rows that arrive in between request sends into a single batched request. Normally this occurs only under load when rows arrive faster than they can be sent out. However in some circumstances the writer may want to reduce the number of requests even under moderate load. This setting accomplishes this by adding a small amount of artificial delay, that is, rather than immediately sending out a row, the writer will wait for up to the given delay to allow other records to be sent so that the sends can be batched together. This can be thought of as analogous to Nagle's algorithm in TCP. This setting gives the upper bound on the delay for batching: once we get client.writer.batch-size worth of rows for a bucket it will be sent immediately regardless of this setting, however if we have fewer than this many bytes accumulated for this bucket we will delay for the specified time waiting for more records to show up."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.writer.bucket.no-key-assigner"}),(0,n.jsx)(t.td,{children:"Enum"}),(0,n.jsx)(t.td,{children:"STICKY"}),(0,n.jsxs)(t.td,{children:["The bucket assigner for no key table. For table with bucket key or primary key, we choose a bucket based on a hash of the key. For these table without bucket key and primary key, we can use this option to specify bucket assigner, the candidate assigner is ROUND_ROBIN, STICKY, the default assigner is STICKY.",(0,n.jsx)("br",{}),"ROUND_ROBIN: this strategy will assign the bucket id for the input row by round robin.",(0,n.jsx)("br",{}),"STICKY: this strategy will assign new bucket id only if the batch changed in record accumulator, otherwise the bucket id will be the same as the front record."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.writer.acks"}),(0,n.jsx)(t.td,{children:"String"}),(0,n.jsx)(t.td,{children:"all"}),(0,n.jsxs)(t.td,{children:["The number of acknowledgments the writer requires the leader to have received before considering a request complete. This controls the durability of records that are sent. The following settings are allowed:",(0,n.jsx)("br",{}),"acks=0: If set to 0, then the writer will not wait for any acknowledgment from the server at all. No guarantee can be mode that the server has received the record in this case.",(0,n.jsx)("br",{}),"acks=1: This will mean the leader will write the record to its local log but will respond without awaiting full acknowledge the record but before the followers have replicated it then the record will be lost.",(0,n.jsx)("br",{}),"acks=-1 (all): This will mean the leader will wait for the full ser of in-sync replicas to acknowledge the record. This guarantees that the record will not be lost as long as at least one in-sync replica remains alive, This is the strongest available guarantee."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.writer.request-max-size"}),(0,n.jsx)(t.td,{children:"MemorySize"}),(0,n.jsx)(t.td,{children:"10mb"}),(0,n.jsx)(t.td,{children:"The maximum size of a request in bytes. This setting will limit the number of record batches the writer will send in a single request to avoid sending huge requests. Note that this retry is no different than if the writer resent the row upon receiving the error."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.writer.retries"}),(0,n.jsx)(t.td,{children:"Integer"}),(0,n.jsx)(t.td,{children:"Integer.MAX_VALUE"}),(0,n.jsx)(t.td,{children:"Setting a value greater than zero will cause the client to resend any record whose send fails with a potentially transient error."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.writer.enable-idempotence"}),(0,n.jsx)(t.td,{children:"Boolean"}),(0,n.jsx)(t.td,{children:"true"}),(0,n.jsx)(t.td,{children:"Writer idempotence is enabled by default if no conflicting config are set. If conflicting config are set and writer idempotence is not explicitly enabled, idempotence is disabled. If idempotence is explicitly enabled and conflicting config are set, a ConfigException is thrown"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.writer.max-inflight-requests-per-bucket"}),(0,n.jsx)(t.td,{children:"Integer"}),(0,n.jsx)(t.td,{children:"5"}),(0,n.jsxs)(t.td,{children:["The maximum number of unacknowledged requests per bucket for writer. This configuration can work only if ",(0,n.jsx)(t.code,{children:"client.writer.enable-idempotence"})," is set to true. When the number of inflight requests per bucket exceeds this setting, the writer will wait for the inflight requests to complete before sending out new requests."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.writer.dynamic-create-partition.enabled"}),(0,n.jsx)(t.td,{children:"Boolean"}),(0,n.jsx)(t.td,{children:"true"}),(0,n.jsx)(t.td,{children:"Whether to enable dynamic partition creation for the client writer. When enabled, new partitions are automatically created if they don't already exist during data writes."})]})]})]}),"\n",(0,n.jsx)(t.h2,{id:"other-options",children:"Other Options"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Option"}),(0,n.jsx)(t.th,{children:"Type"}),(0,n.jsx)(t.th,{children:"Default"}),(0,n.jsx)(t.th,{children:"Description"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"bootstrap.servers"}),(0,n.jsx)(t.td,{children:"List"}),(0,n.jsx)(t.td,{children:"(None)"}),(0,n.jsxs)(t.td,{children:["A list of host/port pairs to use for establishing the initial connection to the Fluss cluster. The list should be in the form host1",":port1",",host2",":port2",",.... Since these servers are just used for the initial connection to discover the full cluster membership (which may change dynamically), this list need not contain the full set of servers (you may want more than one, though, in case a server is down)"]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.id"}),(0,n.jsx)(t.td,{children:"String"}),(0,n.jsx)(t.td,{children:'""'}),(0,n.jsx)(t.td,{children:"An id string to pass to the server when making requests. The purpose of this is to be able to track the source of requests beyond just ip/port by allowing a logical application name to be included in server-side request logging."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.connect-timeout"}),(0,n.jsx)(t.td,{children:"Duration"}),(0,n.jsx)(t.td,{children:"120s"}),(0,n.jsx)(t.td,{children:"The Netty client connect timeout."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.request-timeout"}),(0,n.jsx)(t.td,{children:"Duration"}),(0,n.jsx)(t.td,{children:"30s"}),(0,n.jsx)(t.td,{children:"The timeout for a request to complete. If user set the write ack to -1, this timeout is the max time that delayed write try to complete. The default setting is 30 seconds."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.filesystem.security.token.renewal.backoff"}),(0,n.jsx)(t.td,{children:"Duration"}),(0,n.jsx)(t.td,{children:"1h"}),(0,n.jsx)(t.td,{children:"The time period how long to wait before retrying to obtain new security tokens for filesystem after a failure."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.filesystem.security.token.renewal.time-ratio"}),(0,n.jsx)(t.td,{children:"Double"}),(0,n.jsx)(t.td,{children:"0.75"}),(0,n.jsx)(t.td,{children:"Ratio of the token's expiration time when new credentials for access filesystem should be re-obtained."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.metrics.enabled"}),(0,n.jsx)(t.td,{children:"Boolean"}),(0,n.jsx)(t.td,{children:"false"}),(0,n.jsx)(t.td,{children:"Enable metrics for client. When metrics is enabled, the client will collect metrics and report by the JMX metrics reporter."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.security.protocol"}),(0,n.jsx)(t.td,{children:"String"}),(0,n.jsx)(t.td,{children:"PLAINTEXT"}),(0,n.jsxs)(t.td,{children:["The security protocol used to communicate with brokers. Currently, only ",(0,n.jsx)(t.code,{children:"PLAINTEXT"})," and ",(0,n.jsx)(t.code,{children:"SASL"})," are supported, the configuration value is case insensitive."]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"client.security.{protocol}.*"}),(0,n.jsx)(t.td,{children:"optional"}),(0,n.jsx)(t.td,{children:"(none)"}),(0,n.jsxs)(t.td,{children:["Client-side configuration properties for a specific authentication protocol. E.g., client.security.sasl.jaas.config. More Details in ",(0,n.jsx)(t.a,{href:"/docs/security/authentication",children:"authentication"})]})]})]})]})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},8453:(e,t,i)=>{i.d(t,{R:()=>l,x:()=>o});var r=i(6540);const n={},s=r.createContext(n);function l(e){const t=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:l(e.components),r.createElement(s.Provider,{value:t},e.children)}}}]);