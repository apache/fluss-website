"use strict";(self.webpackChunkfluss_website=self.webpackChunkfluss_website||[]).push([[1430],{4609:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"table-design/table-types/log-table","title":"Log Table","description":"\x3c!--","source":"@site/versioned_docs/version-0.7/table-design/table-types/log-table.md","sourceDirName":"table-design/table-types","slug":"/table-design/table-types/log-table","permalink":"/docs/table-design/table-types/log-table","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/fluss/edit/main/website/docs/table-design/table-types/log-table.md","tags":[],"version":"0.7","sidebarPosition":1,"frontMatter":{"title":"Log Table","sidebar_position":1},"sidebar":"docsSidebar","previous":{"title":"Overview","permalink":"/docs/table-design/overview"},"next":{"title":"PrimaryKey Table","permalink":"/docs/table-design/table-types/pk-table/"}}');var i=t(4848),o=t(8453);const r={title:"Log Table",sidebar_position:1},a="Log Table",l={},c=[{value:"Basic Concept",id:"basic-concept",level:2},{value:"Bucket Assigning",id:"bucket-assigning",level:2},{value:"Data Consumption",id:"data-consumption",level:2},{value:"Column Pruning",id:"column-pruning",level:2},{value:"Log Compression",id:"log-compression",level:2},{value:"Log Tiering",id:"log-tiering",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"log-table",children:"Log Table"})}),"\n",(0,i.jsx)(n.h2,{id:"basic-concept",children:"Basic Concept"}),"\n",(0,i.jsx)(n.p,{children:"Log Table is a type of table in Fluss that is used to store data in the order in which it was written. Log Table only supports append records, and doesn't support Update/Delete operations.\nUsually, Log Table is used to store logs in very high-throughput, like the typical use cases of Apache Kafka."}),"\n",(0,i.jsxs)(n.p,{children:["Log Table is created by not specifying the ",(0,i.jsx)(n.code,{children:"PRIMARY KEY"})," clause in the ",(0,i.jsx)(n.code,{children:"CREATE TABLE"})," statement. For example, the following Flink SQL statement will create a log table with 3 buckets."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",metastring:'title="Flink SQL"',children:"CREATE TABLE log_table (\n  order_id BIGINT,\n  item_id BIGINT,\n  amount INT,\n  address STRING,\n  dt DATE\n)\nWITH ('bucket.num' = '3');\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"bucket.num"})," should be a positive integer. If this value is not provided, a default value from the cluster will be used as the bucket number for the table."]})}),"\n",(0,i.jsx)(n.h2,{id:"bucket-assigning",children:"Bucket Assigning"}),"\n",(0,i.jsxs)(n.p,{children:["Bucketing is the fundamental unit of parallelism and scalability in Fluss.  A single table in Fluss is divided into multiple buckets. A bucket is the smallest storage unit for reads and writes. See more details about ",(0,i.jsx)(n.a,{href:"/docs/table-design/data-distribution/bucketing",children:"Bucketing"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"When writing records into log table, Fluss will assign each record to a specific bucket based on the bucket assigning strategy. There are 3 strategies for bucket assigning in Fluss:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sticky Bucket Strategy"}),": As the default strategy, randomly select a bucket and consistently write into that bucket until a record batch is full. Sets ",(0,i.jsx)(n.code,{children:"client.writer.bucket.no-key-assigner=sticky"})," to the table property to enable this strategy."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Round-Robin Strategy"}),": Select a bucket in round-robin for each record before writing it in. Sets ",(0,i.jsx)(n.code,{children:"client.writer.bucket.no-key-assigner=round_robin"})," to the table property to enable this strategy."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hash-based Bucketing"}),": If ",(0,i.jsx)(n.code,{children:"bucket.key"})," property is set in the table property, Fluss will determine to assign records to bucket based on the hash value of the specified bucket keys, and the property ",(0,i.jsx)(n.code,{children:"client.writer.bucket.no-key-assigner"})," will be ignored. For example, setting ",(0,i.jsx)(n.code,{children:"'bucket.key' = 'c1,c2'"})," will assign buckets based on the values of columns ",(0,i.jsx)(n.code,{children:"c1"})," and ",(0,i.jsx)(n.code,{children:"c2"}),". Different column names should be separated by commas."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"data-consumption",children:"Data Consumption"}),"\n",(0,i.jsx)(n.p,{children:"Log Tables in Fluss allow real-time data consumption, preserving the order of data within each bucket as it was written to the Fluss table. Specifically:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"For two data records from the same table and the same bucket, the data that was written to the Fluss table first will be consumed first."}),"\n",(0,i.jsx)(n.li,{children:"For two data records from the same partition but different buckets, the consumption order is not guaranteed because different buckets may be processed concurrently by different data consumption jobs."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"column-pruning",children:"Column Pruning"}),"\n",(0,i.jsx)(n.p,{children:"Column pruning is a technique used to reduce the amount of data that needs to be read from storage by eliminating unnecessary columns from the query.\nFluss supports column pruning for Log Tables and the changelog of PrimaryKey Tables, which can significantly improve query performance by reducing the amount of data that needs to be read from storage and lowering networking costs."}),"\n",(0,i.jsxs)(n.p,{children:["What sets Fluss apart is its ability to apply ",(0,i.jsx)(n.strong,{children:"column pruning during streaming reads"}),", a capability that is both unique and industry-leading. This ensures that even in real-time streaming scenarios, only the required columns are processed, minimizing resource usage and maximizing efficiency."]}),"\n",(0,i.jsx)(n.p,{children:"In Fluss, Log Tables are stored in a columnar format by default (i.e., Apache Arrow).\nThis format stores data column-by-column rather than row-by-row, making it highly efficient for column pruning.\nWhen a query specifies only a subset of columns, Fluss skips reading irrelevant columns entirely.\nThis enables efficient column pruning during query execution and ensures that only the required columns are read,\nminimizing I/O overhead and improving overall system efficiency."}),"\n",(0,i.jsx)(n.p,{children:"During query execution, query engines like Flink analyzes the query to identify the columns required for processing and tells Fluss to only read the necessary columns.\nFor example the following streaming query:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"SELECT order_id, item_id FROM log_table WHERE dt > '2023-01-01';\n"})}),"\n",(0,i.jsxs)(n.p,{children:["In this query, only the ",(0,i.jsx)(n.code,{children:"order_id"}),", ",(0,i.jsx)(n.code,{children:"item_id"}),", and ",(0,i.jsx)(n.code,{children:"dt"})," columns are accessed. Other columns (e.g., ",(0,i.jsx)(n.code,{children:"address"}),", ",(0,i.jsx)(n.code,{children:"amount"}),") are pruned and not read from storage."]}),"\n",(0,i.jsx)(n.h2,{id:"log-compression",children:"Log Compression"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Log Table"})," supports end-to-end compression for the Arrow log format. Fluss leverages ",(0,i.jsx)(n.a,{href:"https://arrow.apache.org/docs/format/Columnar.html#compression",children:"Arrow native compression"})," to implement this feature,\nensuring that compressed data remains compliant with the Arrow format. As a result, the compressed data can be seamlessly decompressed by any Arrow-compatible library.\nAdditionally, compression is applied to each column independently, preserving the ability to perform column pruning on the compressed data without performance degradation."]}),"\n",(0,i.jsx)(n.p,{children:"When compression is enabled:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["For ",(0,i.jsx)(n.strong,{children:"Log Tables"}),", data is compressed by the writer on the client side, written in a compressed format, and decompressed by the log scanner on the client side."]}),"\n",(0,i.jsxs)(n.li,{children:["For ",(0,i.jsx)(n.strong,{children:"PrimaryKey Table changelogs"}),", compression is performed server-side since the changelog is generated on the server."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Log compression significantly reduces networking and storage costs. Benchmark results demonstrate that using the ZSTD compression with level 3 achieves a compression ratio of approximately ",(0,i.jsx)(n.strong,{children:"5x"})," (e.g., reducing 5GB of data to 1GB).\nFurthermore, read/write throughput improves substantially due to reduced networking overhead."]}),"\n",(0,i.jsxs)(n.p,{children:["By default, the Log Table uses the ",(0,i.jsx)(n.code,{children:"ZSTD"})," compression codec with a compression level of ",(0,i.jsx)(n.code,{children:"3"}),".\nYou can change the compression codec by setting the ",(0,i.jsx)(n.code,{children:"table.log.arrow.compression.type"})," property to ",(0,i.jsx)(n.code,{children:"NONE"}),", ",(0,i.jsx)(n.code,{children:"LZ4_FRAME"}),", or ",(0,i.jsx)(n.code,{children:"ZSTD"}),".\nYou can also adjust the compression level for ",(0,i.jsx)(n.code,{children:"ZSTD"})," by setting the ",(0,i.jsx)(n.code,{children:"table.log.arrow.compression.zstd.level"})," property to a value between ",(0,i.jsx)(n.code,{children:"1"})," and ",(0,i.jsx)(n.code,{children:"22"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"For example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",metastring:'title="Flink SQL"',children:"-- Set the compression codec to LZ4_FRAME\nCREATE TABLE log_table (\n  order_id BIGINT,\n  item_id BIGINT,\n  amount INT,\n  address STRING\n)\nWITH (\n  'table.log.arrow.compression.type' = 'LZ4_FRAME'\n);\n\n-- Set the 'ZSTD' compression level to 2\nCREATE TABLE log_table (\n  order_id BIGINT,\n  item_id BIGINT,\n  amount INT,\n  address STRING\n)\nWITH (\n  'table.log.arrow.compression.zstd.level' = '2'\n);\n"})}),"\n",(0,i.jsxs)(n.p,{children:["In the above example, we set the compression codec to ",(0,i.jsx)(n.code,{children:"LZ4_FRAME"})," and the compression level to ",(0,i.jsx)(n.code,{children:"2"}),"."]}),"\n",(0,i.jsxs)(n.admonition,{type:"note",children:[(0,i.jsx)(n.mdxAdmonitionTitle,{}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Currently, the compression codec and compression level are only supported for arrow format. If you set ",(0,i.jsx)(n.code,{children:"'table.log.format'='indexed'"}),", the compression codec and compression level will be ignored."]}),"\n",(0,i.jsxs)(n.li,{children:["The valid range of ",(0,i.jsx)(n.code,{children:"table.log.arrow.compression.zstd.level"})," is 1 to 22."]}),"\n"]})]}),"\n",(0,i.jsx)(n.h2,{id:"log-tiering",children:"Log Tiering"}),"\n",(0,i.jsxs)(n.p,{children:["Log Table supports tiering data to different storage tiers. See more details about ",(0,i.jsx)(n.a,{href:"/docs/maintenance/tiered-storage/remote-storage",children:"Remote Log"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var s=t(6540);const i={},o=s.createContext(i);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);