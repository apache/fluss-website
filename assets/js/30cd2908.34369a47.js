"use strict";(self.webpackChunkfluss_website=self.webpackChunkfluss_website||[]).push([[4616],{275:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/streaming_join-761699227c5633f3575e2036b9e2ac61.png"},843:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>d});var r=s(7086),t=s(4848),i=s(8453);const a={slug:"partial-updates",title:"Understanding Partial Updates",authors:["giannis"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Partial Updates: A Different Approach with Fluss",id:"partial-updates-a-different-approach-with-fluss",level:3},{value:"Example: Building a Unified Wide Table",id:"example-building-a-unified-wide-table",level:3},{value:"Conclusion",id:"conclusion",level:3}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Banner",src:s(7114).A+"",width:"1348",height:"958"})}),"\n",(0,t.jsxs)(n.p,{children:["Traditional streaming data pipelines often need to join many tables or streams on a primary key to create a wide view.\nFor example, imagine you\u2019re building a real-time recommendation engine for an e-commerce platform.\nTo serve highly personalized recommendations, your system needs a complete 360\xb0 view of each user, including:\n",(0,t.jsx)(n.em,{children:"user preferences"}),", ",(0,t.jsx)(n.em,{children:"past purchases"}),", ",(0,t.jsx)(n.em,{children:"clickstream behavior"}),", ",(0,t.jsx)(n.em,{children:"cart activity"}),", ",(0,t.jsx)(n.em,{children:"product reviews"}),", ",(0,t.jsx)(n.em,{children:"support tickets"}),", ",(0,t.jsx)(n.em,{children:"ad impressions"}),", and ",(0,t.jsx)(n.em,{children:"loyalty status"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["That\u2019s at least ",(0,t.jsx)(n.strong,{children:"8 different data sources"}),", each producing updates independently."]}),"\n",(0,t.jsx)(n.p,{children:"Joining multiple data streams at scale, although it works with Apache Flink it can be really challenging and resource-intensive.\nMore specifically, it can lead to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Really large state sizes in Flink:"})," as it needs to buffer all incoming events until they can be joined. In many case states need to be kept around for a long period of time if not indefinitely."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deal with checkpoints overhead and backpressure:"})," as the join operation and large state uploading can create a bottleneck in the pipeline."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"States are not easy to inspect and debug:"})," as they are often large and complex. This can make it difficult to understand what is happening in the pipeline and why certain events are not being processed correctly."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"State TTL can lead to inconsistent results:"})," as events may be dropped before they can be joined. This can lead to data loss and incorrect results in the final output."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Overall, this approach not only consumes a lot of memory and CPU, but also complicates the job design and maintenance."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Streaming Joins",src:s(275).A+"",width:"1375",height:"550"})}),"\n",(0,t.jsx)(n.h3,{id:"partial-updates-a-different-approach-with-fluss",children:"Partial Updates: A Different Approach with Fluss"}),"\n",(0,t.jsxs)(n.p,{children:["Fluss introduces a more elegant solution: ",(0,t.jsx)(n.strong,{children:"partial updates"})," on a primary key table."]}),"\n",(0,t.jsxs)(n.p,{children:["Instead of performing multi-way joins in the streaming job, Fluss allows each data stream source to independently update only its relevant columns into a shared wide table identified by the primary key.\nIn Fluss, you can define a wide table (for example, a user_profile table based on a ",(0,t.jsx)(n.code,{children:"user_id"}),") that contains all possible fields from all sources.\nEach source stream then writes partial rows \u2013 only the fields it knows about \u2013 into this table."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Partial Update",src:s(5549).A+"",width:"1488",height:"669"})}),"\n",(0,t.jsx)(n.p,{children:"Fluss\u2019s storage engine automatically merges these partial updates together based on the primary key.\nEssentially, Fluss maintains the latest combined value for each key, so you don\u2019t have to manage large join states in Flink."}),"\n",(0,t.jsxs)(n.p,{children:["Under the hood, when a new partial update for a key arrives, Fluss will look up the existing record for that primary key, update the specific columns provided, and leave other columns unchanged.\nThe result is written back as the new version of the record.\nThis happens in ",(0,t.jsx)(n.em,{children:"real-time"}),", so the table is ",(0,t.jsx)(n.strong,{children:"always up-to-date"})," with the latest information from all streams."]}),"\n",(0,t.jsx)(n.p,{children:"Next, let's try and better understand how this works in practice with a concrete example."}),"\n",(0,t.jsx)(n.h3,{id:"example-building-a-unified-wide-table",children:"Example: Building a Unified Wide Table"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["You can find the full source code on github ",(0,t.jsx)(n.a,{href:"https://github.com/ververica/ververica-fluss-examples/tree/main/partial_updates",children:"here"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Start by cloning the repository, run ",(0,t.jsx)(n.code,{children:"docker compose up"})," to spin up the development enviroment and finally grab a terminal\ninto the ",(0,t.jsx)(n.code,{children:"jobmanager"})," and start the Flink SQL cli, by running the following command:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"./bin/sql-client.sh\n"})}),"\n",(0,t.jsx)(n.p,{children:"Great so far ! \ud83d\udc4d"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 1:"})," The first thing we need to do is to create a Flink catalog that will be used to store the tables we are going to create.\nLet's create a catalog called ",(0,t.jsx)(n.code,{children:"fluss_catalog"})," and use this catalog."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"CREATE CATALOG fluss_catalog WITH (\n    'type' = 'fluss',\n    'bootstrap.servers' = 'coordinator-server:9123'\n);\n\nUSE CATALOG fluss_catalog;\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 2:"})," Then let's create ",(0,t.jsx)(n.code,{children:"3 tables"})," to represent the different data sources that will be used to build the recommendations wide table."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"-- Recommendations \u2013 model scores\nCREATE TABLE recommendations (\n    user_id  STRING,\n    item_id  STRING,\n    rec_score DOUBLE,\n    rec_ts   TIMESTAMP(3),\n    PRIMARY KEY (user_id, item_id) NOT ENFORCED\n) WITH ('bucket.num' = '3', 'table.datalake.enabled' = 'true');\n\n\n-- Impressions \u2013 how often we showed something\nCREATE TABLE impressions (\n    user_id STRING,\n    item_id STRING,\n    imp_cnt INT,\n    imp_ts  TIMESTAMP(3),\n    PRIMARY KEY (user_id, item_id) NOT ENFORCED\n) WITH ('bucket.num' = '3', 'table.datalake.enabled' = 'true');\n\n-- Clicks \u2013 user engagement\nCREATE TABLE clicks (\n    user_id  STRING,\n    item_id  STRING,\n    click_cnt INT,\n    clk_ts    TIMESTAMP(3),\n    PRIMARY KEY (user_id, item_id) NOT ENFORCED\n) WITH ('bucket.num' = '3', 'table.datalake.enabled' = 'true');\n\nCREATE TABLE user_rec_wide (\n    user_id   STRING,\n    item_id   STRING,\n    rec_score DOUBLE,   -- updated by recs stream\n    imp_cnt   INT,      -- updated by impressions stream\n    click_cnt INT,      -- updated by clicks stream\n    PRIMARY KEY (user_id, item_id) NOT ENFORCED\n) WITH ('bucket.num' = '3', 'table.datalake.enabled' = 'true');\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 3:"})," Of course, we will need some sample data to work with , so let's go on and insert some records into the tables. \ud83d\udcbb"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"-- Recommendations \u2013 model scores\nINSERT INTO recommendations VALUES\n    ('user_101','prod_501',0.92 , TIMESTAMP '2025-05-16 09:15:02'),\n    ('user_101','prod_502',0.78 , TIMESTAMP '2025-05-16 09:15:05'),\n    ('user_102','prod_503',0.83 , TIMESTAMP '2025-05-16 09:16:00'),\n    ('user_103','prod_501',0.67 , TIMESTAMP '2025-05-16 09:16:20'),\n    ('user_104','prod_504',0.88 , TIMESTAMP '2025-05-16 09:16:45');\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"-- Impressions \u2013 how often each (user,item) was shown\nINSERT INTO impressions VALUES\n    ('user_101','prod_501', 3, TIMESTAMP '2025-05-16 09:17:10'),\n    ('user_101','prod_502', 1, TIMESTAMP '2025-05-16 09:17:15'),\n    ('user_102','prod_503', 7, TIMESTAMP '2025-05-16 09:18:22'),\n    ('user_103','prod_501', 4, TIMESTAMP '2025-05-16 09:18:30'),\n    ('user_104','prod_504', 2, TIMESTAMP '2025-05-16 09:18:55');\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"-- Clicks \u2013 user engagement\nINSERT INTO clicks VALUES\n    ('user_101','prod_501', 1, TIMESTAMP '2025-05-16 09:19:00'),\n    ('user_101','prod_502', 2, TIMESTAMP '2025-05-16 09:19:07'),\n    ('user_102','prod_503', 1, TIMESTAMP '2025-05-16 09:19:12'),\n    ('user_103','prod_501', 1, TIMESTAMP '2025-05-16 09:19:20'),\n    ('user_104','prod_504', 1, TIMESTAMP '2025-05-16 09:19:25');\n"})}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note:"})," \ud83d\udea8 So far the jobs we run were bounded jobs, so they will finish after inserting the records. Moving forward we will run some streaming jobs.\nSo keep in mind that each job runs with a ",(0,t.jsx)(n.code,{children:"parallelism of 3"})," and our environment is set up ",(0,t.jsx)(n.code,{children:"with 10 slots total"}),".\nSo make sure to keep an eye to the Flink Web UI to see how many slots are used and how many are available and stop some jobs when are no longer needed to free up resourecs."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 4:"})," At this point let's open up a separate terminal and start the Flink SQL CLI.\nIn this new terminal, make sure to run set the ",(0,t.jsx)(n.code,{children:"result-mode"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"SET 'sql-client.execution.result-mode' = 'tableau';\n"})}),"\n",(0,t.jsx)(n.p,{children:"and then run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"SELECT * FROM user_rec_wide;\n"})}),"\n",(0,t.jsxs)(n.p,{children:["to observe the output of the table, as we insert ",(0,t.jsx)(n.code,{children:"partially"})," records into the it from the different sources."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 5:"})," Let's insert the records from the ",(0,t.jsx)(n.code,{children:"recommendations"})," table into the ",(0,t.jsx)(n.code,{children:"user_rec_wide"})," table."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"-- Apply recommendation scores\nINSERT INTO user_rec_wide (user_id, item_id, rec_score)\nSELECT\n    user_id,\n    item_id,\n    rec_score\nFROM recommendations;\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Output:"})," Notice, how only the related columns are updated in the ",(0,t.jsx)(n.code,{children:"user_rec_wide"})," table and the rest of the columns are ",(0,t.jsx)(n.code,{children:"NULL"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"Flink SQL> SELECT * FROM user_rec_wide;\n+----+--------------------------------+--------------------------------+--------------------------------+-------------+-------------+\n| op |                        user_id |                        item_id |                      rec_score |     imp_cnt |   click_cnt |\n+----+--------------------------------+--------------------------------+--------------------------------+-------------+-------------+\n| +I |                       user_101 |                       prod_501 |                           0.92 |      <NULL> |      <NULL> |\n| +I |                       user_101 |                       prod_502 |                           0.78 |      <NULL> |      <NULL> |\n| +I |                       user_104 |                       prod_504 |                           0.88 |      <NULL> |      <NULL> |\n| +I |                       user_102 |                       prod_503 |                           0.83 |      <NULL> |      <NULL> |\n| +I |                       user_103 |                       prod_501 |                           0.67 |      <NULL> |      <NULL> |\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 5:"})," Next, let's insert the records from the ",(0,t.jsx)(n.code,{children:"impressions"})," table into the ",(0,t.jsx)(n.code,{children:"user_rec_wide"})," table."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"-- Apply impression counts\nINSERT INTO user_rec_wide (user_id, item_id, imp_cnt)\nSELECT\n    user_id,\n    item_id,\n    imp_cnt\nFROM impressions;\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Output:"})," Notice how the ",(0,t.jsx)(n.code,{children:"impressions"})," records are inserted into the ",(0,t.jsx)(n.code,{children:"user_rec_wide"})," table and the ",(0,t.jsx)(n.code,{children:"imp_cnt"})," column is updated."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"Flink SQL> SELECT * FROM user_rec_wide;\n+----+--------------------------------+--------------------------------+--------------------------------+-------------+-------------+\n| op |                        user_id |                        item_id |                      rec_score |     imp_cnt |   click_cnt |\n+----+--------------------------------+--------------------------------+--------------------------------+-------------+-------------+\n| +I |                       user_101 |                       prod_501 |                           0.92 |      <NULL> |      <NULL> |\n| +I |                       user_101 |                       prod_502 |                           0.78 |      <NULL> |      <NULL> |\n| +I |                       user_104 |                       prod_504 |                           0.88 |      <NULL> |      <NULL> |\n| +I |                       user_102 |                       prod_503 |                           0.83 |      <NULL> |      <NULL> |\n| +I |                       user_103 |                       prod_501 |                           0.67 |      <NULL> |      <NULL> |\n\n\n\n| -U |                       user_101 |                       prod_501 |                           0.92 |      <NULL> |      <NULL> |\n| +U |                       user_101 |                       prod_501 |                           0.92 |           3 |      <NULL> |\n| -U |                       user_101 |                       prod_502 |                           0.78 |      <NULL> |      <NULL> |\n| +U |                       user_101 |                       prod_502 |                           0.78 |           1 |      <NULL> |\n| -U |                       user_104 |                       prod_504 |                           0.88 |      <NULL> |      <NULL> |\n| +U |                       user_104 |                       prod_504 |                           0.88 |           2 |      <NULL> |\n| -U |                       user_102 |                       prod_503 |                           0.83 |      <NULL> |      <NULL> |\n| +U |                       user_102 |                       prod_503 |                           0.83 |           7 |      <NULL> |\n| -U |                       user_103 |                       prod_501 |                           0.67 |      <NULL> |      <NULL> |\n| +U |                       user_103 |                       prod_501 |                           0.67 |           4 |      <NULL> |\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 6:"})," Finally, let's insert the records from the ",(0,t.jsx)(n.code,{children:"clicks"})," table into the ",(0,t.jsx)(n.code,{children:"user_rec_wide"})," table."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"-- Apply click counts\nINSERT INTO user_rec_wide (user_id, item_id, click_cnt)\nSELECT\n    user_id,\n    item_id,\n    click_cnt\nFROM clicks;\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Output:"})," Notice how the ",(0,t.jsx)(n.code,{children:"clicks"})," records are inserted into the ",(0,t.jsx)(n.code,{children:"user_rec_wide"})," table and the ",(0,t.jsx)(n.code,{children:"click_cnt"})," column is updated."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"Flink SQL> SELECT * FROM user_rec_wide;\n+----+--------------------------------+--------------------------------+--------------------------------+-------------+-------------+\n| op |                        user_id |                        item_id |                      rec_score |     imp_cnt |   click_cnt |\n+----+--------------------------------+--------------------------------+--------------------------------+-------------+-------------+\n| +I |                       user_101 |                       prod_501 |                           0.92 |      <NULL> |      <NULL> |\n| +I |                       user_101 |                       prod_502 |                           0.78 |      <NULL> |      <NULL> |\n| +I |                       user_104 |                       prod_504 |                           0.88 |      <NULL> |      <NULL> |\n| +I |                       user_102 |                       prod_503 |                           0.83 |      <NULL> |      <NULL> |\n| +I |                       user_103 |                       prod_501 |                           0.67 |      <NULL> |      <NULL> |\n\n\n\n| -U |                       user_101 |                       prod_501 |                           0.92 |      <NULL> |      <NULL> |\n| +U |                       user_101 |                       prod_501 |                           0.92 |           3 |      <NULL> |\n| -U |                       user_101 |                       prod_502 |                           0.78 |      <NULL> |      <NULL> |\n| +U |                       user_101 |                       prod_502 |                           0.78 |           1 |      <NULL> |\n| -U |                       user_104 |                       prod_504 |                           0.88 |      <NULL> |      <NULL> |\n| +U |                       user_104 |                       prod_504 |                           0.88 |           2 |      <NULL> |\n| -U |                       user_102 |                       prod_503 |                           0.83 |      <NULL> |      <NULL> |\n| +U |                       user_102 |                       prod_503 |                           0.83 |           7 |      <NULL> |\n| -U |                       user_103 |                       prod_501 |                           0.67 |      <NULL> |      <NULL> |\n| +U |                       user_103 |                       prod_501 |                           0.67 |           4 |      <NULL> |\n\n\n| -U |                       user_103 |                       prod_501 |                           0.67 |           4 |      <NULL> |\n| +U |                       user_103 |                       prod_501 |                           0.67 |           4 |           1 |\n| -U |                       user_101 |                       prod_501 |                           0.92 |           3 |      <NULL> |\n| +U |                       user_101 |                       prod_501 |                           0.92 |           3 |           1 |\n| -U |                       user_101 |                       prod_502 |                           0.78 |           1 |      <NULL> |\n| +U |                       user_101 |                       prod_502 |                           0.78 |           1 |           2 |\n| -U |                       user_104 |                       prod_504 |                           0.88 |           2 |      <NULL> |\n| +U |                       user_104 |                       prod_504 |                           0.88 |           2 |           1 |\n| -U |                       user_102 |                       prod_503 |                           0.83 |           7 |      <NULL> |\n| +U |                       user_102 |                       prod_503 |                           0.83 |           7 |           1 |\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Reminder:"})," \u203c\ufe0fAs mentioned before make sure to stop the jobs that are no longer needed to free up resources."]}),"\n",(0,t.jsxs)(n.p,{children:["Now let's switch to ",(0,t.jsx)(n.code,{children:"batch"})," mode and query the current snapshot of the ",(0,t.jsx)(n.code,{children:"user_rec_wide"})," table."]}),"\n",(0,t.jsxs)(n.p,{children:["But before that, let's start the ",(0,t.jsx)(n.a,{href:"https://alibaba.github.io/fluss-docs/docs/maintenance/tiered-storage/lakehouse-storage/#start-the-datalake-tiering-service",children:"Tiering Service"})," that allows offloading the tables as ",(0,t.jsx)(n.code,{children:"Lakehouse"})," tables."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 7:"})," Open a new terminal \ud83d\udcbb in the ",(0,t.jsx)(n.code,{children:"Coordinator Server"})," and run the following command to start the ",(0,t.jsx)(n.code,{children:"Tiering Service"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"./bin/lakehouse.sh -D flink.rest.address=jobmanager -D flink.rest.port=8081 -D flink.execution.checkpointing.interval=30s -D flink.parallelism.default=2\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The configured checkpoint is ",(0,t.jsx)(n.code,{children:"flink.execution.checkpointing.interval=30s"})," so wait a bit until the first checkpoint is created\nand data gets offloading to the ",(0,t.jsx)(n.code,{children:"Lakehouse"})," tables."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Step 8:"})," Finally let's switch to ",(0,t.jsx)(n.code,{children:"batch"})," mode and query the current snapshot of the ",(0,t.jsx)(n.code,{children:"user_rec_wide"})," table."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"SET 'execution.runtime-mode' = 'batch';\n\nFlink SQL> SELECT * FROM user_rec_wide;\n+----------+----------+-----------+---------+-----------+\n|  user_id |  item_id | rec_score | imp_cnt | click_cnt |\n+----------+----------+-----------+---------+-----------+\n| user_102 | prod_503 |      0.83 |       7 |         1 |\n| user_103 | prod_501 |      0.67 |       4 |         1 |\n| user_101 | prod_501 |      0.92 |       3 |         1 |\n| user_101 | prod_502 |      0.78 |       1 |         2 |\n| user_104 | prod_504 |      0.88 |       2 |         1 |\n+----------+----------+-----------+---------+-----------+\n5 rows in set (2.63 seconds)\n"})}),"\n",(0,t.jsx)(n.p,{children:"\ud83c\udf89 That's it! You have successfully created a unified wide table using partial updates in Fluss."}),"\n",(0,t.jsx)(n.h3,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"Partial updates in Fluss enable an alternative approach in how we design streaming data pipelines for enriching or joining data."}),"\n",(0,t.jsxs)(n.p,{children:["When all your sources share a primary key - otherwise you can mix & match ",(0,t.jsx)(n.a,{href:"https://alibaba.github.io/fluss-docs/docs/engine-flink/lookups/#lookup",children:"streaming lookup joins"})," - you can turn the problem on its head: update a unified table incrementally, rather than joining streams on the fly."]}),"\n",(0,t.jsx)(n.p,{children:"The result is a more scalable, maintainable, and efficient pipeline.\nEngineers can spend less time wrestling with Flink\u2019s state, checkpoints and join mechanics, and more time delivering fresh, integrated data to power real-time analytics and applications.\nWith Fluss handling the merge logic, achieving a single, up-to-date view from multiple disparate streams becomes way more elegant. \ud83d\ude01"}),"\n",(0,t.jsxs)(n.p,{children:["And before you go \ud83d\ude0a don\u2019t forget to give Fluss \ud83c\udf0a some \u2764\ufe0f via \u2b50 on ",(0,t.jsx)(n.a,{href:"https://github.com/alibaba/fluss",children:"GitHub"})]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},5549:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/partial_update-d529bf24211a76b31444bc7918872d59.png"},7086:e=>{e.exports=JSON.parse('{"permalink":"/blog/partial-updates","source":"@site/blog/2025-06-01-partial-updates.md","title":"Understanding Partial Updates","description":"\x3c!--","date":"2025-06-01T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"name":"Giannis Polyzos","title":"PMC member of Apache Fluss","url":"https://github.com/polyzos","imageURL":"https://github.com/polyzos.png","key":"giannis","page":null}],"frontMatter":{"slug":"partial-updates","title":"Understanding Partial Updates","authors":["giannis"]},"unlisted":false,"prevItem":{"title":"Announcing Fluss 0.7","permalink":"/blog/releases/0.7"},"nextItem":{"title":"The Story of Fluss Logo","permalink":"/blog/unveil-fluss-logo"}}')},7114:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/banner-a09e6bac14dc2bee3dbc698347cb2d7f.png"},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var r=s(6540);const t={},i=r.createContext(t);function a(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);