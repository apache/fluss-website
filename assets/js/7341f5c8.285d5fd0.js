"use strict";(self.webpackChunkfluss_website=self.webpackChunkfluss_website||[]).push([[2394],{895:(e,a,i)=>{i.d(a,{A:()=>s});const s=i.p+"assets/images/img2-d2d1a314768a1be3da0308e20851d3f3.jpg"},2581:(e,a,i)=>{i.d(a,{A:()=>s});const s=i.p+"assets/images/img8-87cfb5887e9a49febaacd805de5216fe.jpg"},2896:(e,a,i)=>{i.d(a,{A:()=>s});const s=i.p+"assets/images/img1-a29d423ce0b5f952b79ad2babb903cf5.jpg"},3793:(e,a,i)=>{i.d(a,{A:()=>s});const s=i.p+"assets/images/img4-5a57c638bb5dd478a86a9fcd1683734f.jpg"},4712:(e,a,i)=>{i.d(a,{A:()=>s});const s=i.p+"assets/images/img9-fb689d41f612ef840508c4c20399822c.jpg"},5604:(e,a,i)=>{i.r(a),i.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var s=i(6990),t=i(4848),n=i(8453);const r={slug:"why-fluss",title:"Why Fluss? Top 4 Challenges of Using Kafka for Real-Time Analytics",sidebar_label:"Why Fluss? Top 4 Challenges of Kafka",authors:["jark"]},o=void 0,l={authorsImageUrls:[void 0]},c=[{value:"Kafka Falls Short in Real-Time Analytics",id:"kafka-falls-short-in-real-time-analytics",level:2},{value:"No Support for Updates",id:"no-support-for-updates",level:3},{value:"Lack of Querying Capabilities",id:"lack-of-querying-capabilities",level:3},{value:"Difficulty with Data Backfilling",id:"difficulty-with-data-backfilling",level:3},{value:"Excessive Network Costs",id:"excessive-network-costs",level:3},{value:"Kafka is not designed for Analytics",id:"kafka-is-not-designed-for-analytics",level:2},{value:"Introducing Fluss",id:"introducing-fluss",level:2}];function d(e){const a={a:"a",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,n.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.p,{children:"The industry is undergoing a clear and significant shift as big data computing transitions from offline to real-time processing.\nThis transition is revolutionizing various sectors, including the E-commerce, automotive networking, finance, and beyond,\nwhere real-time data applications are becoming integral to operations. This evolution enables organizations to unlock greater\nvalue by leveraging real-time insights to drive business impact and enhance decision-making."}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.img,{alt:"FF Announcement",src:i(2896).A+"",width:"3656",height:"1240"}),"\nThe evolution of big data technology is becoming increasingly impactful, reshaping the computing architecture landscape.\nThe traditional Hive-based data warehouse has given way to modern architectures, starting with Lakehouse models and progressing\nto the Paimon streaming Lakehouse, which has gained significant traction in markets like China and more recently started expanding to EU/US markets.\nThe core driver behind these architectural innovations is the need to improve data processing timeliness. The data freshness is improved from traditional\nT+1 (next-day readiness) to T+1 hours, and now T+1 minutes. However, Lakehouse architectures, being file-system-based,\ninherently face minute-level latency as their practical upper limit."]}),"\n",(0,t.jsx)(a.p,{children:"Yet, many critical use cases, such as search and recommendation systems, advertisement attribution, and anomaly detection, demand second-level latency.\nWhile big data technologies have advanced significantly, there remains a notable gap: a user-friendly, second-level storage solution tailored for big data analytics."}),"\n",(0,t.jsx)(a.p,{children:"In most real-time data scenarios, Apache Kafka has emerged as the go-to second-level storage solution. Its integration with Apache Flink represents the dominant architecture for building real-time data warehouses. However, while widely adopted, this combination presents significant challenges for achieving true real-time analytics at scale. The limitations of using Kafka for big data analytics highlight the need for a more robust solution to meet the demands of modern real-time use cases."}),"\n",(0,t.jsx)(a.h2,{id:"kafka-falls-short-in-real-time-analytics",children:"Kafka Falls Short in Real-Time Analytics"}),"\n",(0,t.jsx)(a.h3,{id:"no-support-for-updates",children:"No Support for Updates"}),"\n",(0,t.jsx)(a.p,{children:"The first significant challenge with Apache Kafka is its lack of support for updates, a critical feature for data warehouses. In data warehousing, updates are often essential to correct or amend data. However, Kafka's inability to handle updates results in duplicate records for the same primary key. When consumed by a computing engine, this duplication necessitates costly deduplication processes to ensure accurate results."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"FF Announcement",src:i(895).A+"",width:"3656",height:"1172"})}),"\n",(0,t.jsx)(a.p,{children:"In Apache Flink, for instance, handling this issue requires materializing all upstream data in state, which is resource-intensive. Every time data is consumed from Kafka, the deduplication process incurs a substantial overhead, significantly increasing computational and storage costs. This limitation not only impacts performance but also hampers the reusability of Kafka-stored data for downstream business processes."}),"\n",(0,t.jsx)(a.h3,{id:"lack-of-querying-capabilities",children:"Lack of Querying Capabilities"}),"\n",(0,t.jsx)(a.p,{children:"The second major limitation of Apache Kafka is hard to debug, primarily due to its lack of native querying capabilities. In data warehousing, querying is a fundamental feature that facilitates troubleshooting and ad-hoc analysis to understand data trends. Unfortunately, Kafka operates much like a black box, making it challenging to perform these critical tasks without additional tools or layers."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"FF Announcement",src:i(9186).A+"",width:"3696",height:"1214"})}),"\n",(0,t.jsx)(a.p,{children:"To address this limitation, the industry has adopted two primary approaches, each with its own trade-offs:"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Synchronizing Kafka Data to an OLAP System:"})," This allows for querying data using the OLAP system\u2019s capabilities. However, this approach introduces additional components into the architecture, increasing both complexity and cost. Moreover, it risks data inconsistencies due to synchronization delays."]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Querying Kafka Directly Using Trino:"})," While Trino can query Kafka, it relies solely on full scans, which are inefficient for large-scale operations. For instance, a simple query on just 1GB of Kafka data can take up to one minute, rendering it impractical for large datasets or real-time requirements."]}),"\n"]}),"\n",(0,t.jsx)(a.p,{children:"These limitations make Kafka unsuitable for efficient and scalable data exploration in modern data warehousing workflows."}),"\n",(0,t.jsx)(a.h3,{id:"difficulty-with-data-backfilling",children:"Difficulty with Data Backfilling"}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.img,{alt:"FF Announcement",src:i(3793).A+"",width:"3526",height:"1210"}),"\nThe third significant issue with Apache Kafka is processing historical data, also known as data backfilling, a common requirement in data warehousing.\nFor instance, in logistics, it usually needs to process and analyze historical data from several months ago. However, Kafka only retain data for a few days due to the high cost of storage.\nEven though Kafka community has introduced ",(0,t.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/display/KAFKA/KIP-405%3A+Kafka+Tiered+Storage",children:"Tiered Storage"})," to address the long-term data issue, it still has limitations.\nReading historical data still require all the data to pass through Kafka brokers, which can lead to brokers unstable and ",(0,t.jsx)(a.a,{href:"https://www.warpstream.com/blog/tiered-storage-wont-fix-kafka#increased-complexity-and-operational-burden",children:"disrupt live traffics"}),"."]}),"\n",(0,t.jsx)(a.p,{children:"These limitations highlight Kafka\u2019s inefficiency as a solution for data backfilling in large-scale and long-term analytical use cases, further underscoring the need for more robust alternatives in real-time data warehousing."}),"\n",(0,t.jsx)(a.h3,{id:"excessive-network-costs",children:"Excessive Network Costs"}),"\n",(0,t.jsx)(a.p,{children:"The final major challenge with Apache Kafka lies in its high network costs, which account for an estimated 88% of Kafka's overall operational expenses. In data warehousing, the \"one write, multiple read\" pattern is common, with each consumer often requiring only a subset of the data. However, Kafka's design mandates that consumers read the entire dataset, regardless of how much is actually needed."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"FF Announcement",src:i(9460).A+"",width:"3636",height:"1206"})}),"\n",(0,t.jsx)(a.p,{children:"For instance, among the tens of thousands of Flink SQL jobs at Alibaba, only 49% of the upstream columns are utilized per job on average. Despite this, consumers must read all the columns and pay 100% networking cost, which is highly inefficient and wasteful."}),"\n",(0,t.jsxs)(a.p,{children:["In summary, using Kafka for real-time analytics presents several critical issues: ",(0,t.jsx)(a.strong,{children:"(1) lack of support for updates"}),", ",(0,t.jsx)(a.strong,{children:"(2) absence of querying capabilities"}),", ",(0,t.jsx)(a.strong,{children:"(3) difficulty with data backfilling"}),", and ",(0,t.jsx)(a.strong,{children:"(4) excessive network costs"}),". These limitations make the combination of Flink and Kafka less than ideal for real-time data warehousing.\nBut why Kafka lacks these abilities? Is it possible to add the abilities to Kafka?"]}),"\n",(0,t.jsx)(a.h2,{id:"kafka-is-not-designed-for-analytics",children:"Kafka is not designed for Analytics"}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.img,{alt:"FF Announcement",src:i(9171).A+"",width:"3620",height:"1148"}),"\nThe root cause of these challenges lies in the fundamental design philosophy of Kafka: ",(0,t.jsx)(a.strong,{children:"Kafka is designed for streaming events, NOT for streaming analytics"}),". Each system has its own focus and strengths, and Kafka's design is optimized for message queue scenarios rather than analytical workloads."]}),"\n",(0,t.jsx)(a.p,{children:"In Kafka, data is typically stored in a row-oriented format, such as CSV, JSON, or Avro.\nWhile this is highly efficient for use cases involving message streaming, it becomes a bottleneck for analytical scenarios.\nData analysis requires handling large volumes of data, and therefore heavily relies on data skipping capabilities from storage, such as column pruning and predicate pushdown.\nFor this reason, columnar storage is far better suited for analytical workloads, whereas Kafka's row-based storage is not designed to meet these demands effectively."}),"\n",(0,t.jsx)(a.h2,{id:"introducing-fluss",children:"Introducing Fluss"}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.img,{alt:"FF Announcement",src:i(6038).A+"",width:"3644",height:"1210"}),"\nWhen visualizing the data ecosystem as a four-quadrant matrix, an intriguing pattern emerges.\nThe left side of the matrix represents operational systems, while the right side represents analytical systems.\nThe top half is dedicated to stream storage, and the bottom half to table storage.\nIn operational systems, both databases and stream storage predominantly use row-based formats, as row storage is more efficient for transactional workloads. Conversely, in analytical systems like Apache Iceberg and Snowflake, columnar storage is preferred due to its superior performance in analytical scenarios."]}),"\n",(0,t.jsx)(a.p,{children:"Interestingly, the upper-right quadrant of the matrix remains empty, indicating a significant gap in the market: a streaming storage for analytical scenarios. Unsurprisingly, such a storage would likely adopt a columnar format to effectively address the needs of real-time analytics."}),"\n",(0,t.jsxs)(a.p,{children:['To fill the gap and resolve problems in Flink and streaming analytics, we embarked on a journey two years ago to create a streaming storage. We named the project "',(0,t.jsx)(a.strong,{children:"FL"}),"ink ",(0,t.jsx)(a.strong,{children:"U"}),"nified ",(0,t.jsx)(a.strong,{children:"S"}),"treaming ",(0,t.jsx)(a.strong,{children:"S"}),'torage", and from its initials, we derived the name ',(0,t.jsx)(a.strong,{children:"Fluss"}),".\n",(0,t.jsx)(a.img,{alt:"FF Announcement",src:i(2581).A+"",width:"3700",height:"946"}),'\nInterestingly, Flink is derived from the German word for "',(0,t.jsx)(a.strong,{children:"agile"}),'", and Fluss, which translates to "',(0,t.jsx)(a.strong,{children:"river"}),"\" in German, resonates deeply with the project\u2019s vision.\nIt symbolizes the streaming data is continuously flowing, distributing and converging into data lakes, just like a river.\nThe launch of Fluss coincides with Flink's 10th anniversary, making it a fitting tribute to the project\u2019s heritage and its enduring contribution to the streaming data landscape."]}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"FF Announcement",src:i(4712).A+"",width:"3686",height:"1242"})}),"\n",(0,t.jsxs)(a.p,{children:["Learn more details about Fluss in the next ",(0,t.jsx)(a.a,{href:"/blog/fluss-intro/",children:"blog post"}),", where we will delve deeper into Fluss's architecture, design principles, key features, and explore how it addresses the challenges of using Kafka for real-time analytics."]})]})}function h(e={}){const{wrapper:a}={...(0,n.R)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},6038:(e,a,i)=>{i.d(a,{A:()=>s});const s=i.p+"assets/images/img7-06886bca9797751895c82d707cb04b2d.jpg"},6990:e=>{e.exports=JSON.parse('{"permalink":"/blog/why-fluss","source":"@site/blog/2024-12-11-why-fluss.md","title":"Why Fluss? Top 4 Challenges of Using Kafka for Real-Time Analytics","description":"\x3c!--","date":"2024-12-11T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"name":"Jark Wu","title":"PPMC member of Apache Fluss (Incubating)","url":"https://github.com/wuchong","imageURL":"https://github.com/wuchong.png","key":"jark","page":null}],"frontMatter":{"slug":"why-fluss","title":"Why Fluss? Top 4 Challenges of Using Kafka for Real-Time Analytics","sidebar_label":"Why Fluss? Top 4 Challenges of Kafka","authors":["jark"]},"unlisted":false,"prevItem":{"title":"Introducing Fluss: Streaming Storage for Real-Time Analytics","permalink":"/blog/fluss-intro"},"nextItem":{"title":"Fluss is Now Open Source","permalink":"/blog/fluss-open-source"}}')},8453:(e,a,i)=>{i.d(a,{R:()=>r,x:()=>o});var s=i(6540);const t={},n=s.createContext(t);function r(e){const a=s.useContext(n);return s.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(n.Provider,{value:a},e.children)}},9171:(e,a,i)=>{i.d(a,{A:()=>s});const s=i.p+"assets/images/img6-4741286b7e5d1410d5a8d7036e3cad8d.jpg"},9186:(e,a,i)=>{i.d(a,{A:()=>s});const s=i.p+"assets/images/img3-32732f50a5420dad7d1dd6c5b3d17d6f.jpg"},9460:(e,a,i)=>{i.d(a,{A:()=>s});const s=i.p+"assets/images/img5-e185731d5e249a6c80b962b77b5a16ad.jpg"}}]);