"use strict";(self.webpackChunkfluss_website=self.webpackChunkfluss_website||[]).push([[8749],{1895:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"fluss-joins-asf","metadata":{"permalink":"/blog/fluss-joins-asf","source":"@site/blog/2025-07-10-fluss-joins-asf.md","title":"Fluss Joins the Apache Incubator","description":"\x3c!--","date":"2025-07-10T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"name":"Jark Wu","title":"PMC member of Apache Fluss","url":"https://github.com/wuchong","imageURL":"https://github.com/wuchong.png","key":"jark","page":null}],"frontMatter":{"slug":"fluss-joins-asf","title":"Fluss Joins the Apache Incubator","sidebar_label":"Fluss Joins the Apache Incubator","authors":["jark"]},"unlisted":false,"nextItem":{"title":"Apache Fluss Java Client: A Deep Dive","permalink":"/blog/fluss-java-client"}},"content":"\x3c!--\\n Licensed to the Apache Software Foundation (ASF) under one\\n or more contributor license agreements.  See the NOTICE file\\n distributed with this work for additional information\\n regarding copyright ownership.  The ASF licenses this file\\n to you under the Apache License, Version 2.0 (the\\n \\"License\\"); you may not use this file except in compliance\\n with the License.  You may obtain a copy of the License at\\n\\n      http://www.apache.org/licenses/LICENSE-2.0\\n\\n Unless required by applicable law or agreed to in writing, software\\n distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n See the License for the specific language governing permissions and\\n limitations under the License.\\n--\x3e\\n\\nOn June 5th, Fluss, the next-generation streaming storage project open-sourced and donated by Alibaba, successfully passed the [vote](https://lists.apache.org/thread/mnol4wxovpz6klt196d3x239t4mp6z5o) and officially became an incubator project of the Apache Software Foundation (ASF). This marks a significant milestone in the development of the Fluss community, symbolizing that the project has entered a new phase that is more open,\\nneutral, and standardized. Moving forward, Fluss will leverage the ASF ecosystem to accelerate the building of a global developer community, continuously driving innovation and adoption of next-generation real-time data infrastructure.\\n\\n![ASF](assets/fluss_asf/asf.png)\\n\x3c!-- truncate --\x3e\\n\\nThe Fluss community has recently completed all donation procedures and successfully transferred the project to the Apache Software Foundation.\\nDuring the keynote speech at Flink Forward Asia 2025, held on July 3rd in Singapore, project creator Jark Wu officially announced the exciting news, \\nsharing the new [repository address](https://github.com/apache/fluss/) and the [official website domain](https://fluss.apache.org/).\\n\\n![FF Announcement](assets/fluss_asf/announcement.png)\\n\\n### What is Fluss?\\n\\n![Architecture](assets/fluss_asf/architecture.png)\\n\\nApache Fluss (incubating) is a next-generation streaming storage designed for real-time analytics scenarios. \\nIt aims to address the high costs and inefficiencies of traditional streaming storage technologies in stream processing and Lakehouse architectures. \\nIt offers the following core features:\\n\\n* **Columnar Streaming Storage:** Supports real-time streaming read and write with millisecond-level latency. Real-time streaming data is stored in the Apache Arrow columnar format to leverage query pushdown technologies such as column pruning and partition pruning during streaming read. It improves read performance by up to 10 times and reduces network costs.\\n* **Real-Time Updates and Lookup Queries:** Innovatively introduces real-time update capabilities into stream storage. Through high-performance streaming updates, partial updates, changelog feed, key-value lookup, and DeltaJoin features, it collaborates efficiently with Flink to build a cost-effective, real-time streaming data warehouse.\\n* **Streaming Lakehouse:** Achieves unified storage of data lakehouse and data streams, enabling data sharing between them. The Lakehouse provides low-cost historical data support for streams, while streams inject real-time data capabilities into the Lakehouse, delivering real-time data analysis experiences to the Lakehouse.\\n\\n### The Two-Year Journey\\nIn July 2023, the Flink team at Alibaba Cloud launched the Fluss project. \\nThe name **\\"Fluss\\"** is derived from the abbreviation of \\"**Fl**ink **U**nified **S**treaming **S**torage\\", signifying its mission to build a unified streaming storage foundation for Apache Flink.\\nCoincidentally, \\"Fluss\\" means **\\"river\\"** in German, symbolizing the continuous flow of data.\\n\\nAfter more than a year of internal incubation and refinement, Alibaba officially announced the open-sourcing of the Fluss project on November 29, 2024, during the keynote speech at Flink Forward Asia 2024 in Shanghai.\\nSince then, Fluss has embarked on a path of diverse and international development, attracting contributions from more than 60 developers worldwide.\\nThe community\u2019s activity has been steadily growing, with a major version released approximately every three months.\\n\\nAt the same time, Fluss has achieved large-scale adoption within Alibaba Group. \\nCurrently, it supports **data scales of over 3 PB**, with a cluster **throughput peak of 40 GB/s**, and a maximum single-table **lookup query QPS of up to 500,000 per second**, and single-table data volume **reaching up to 500 billion rows**. \\nIn key business scenarios such as log collection and analysis, search recommendation, and real-time data warehouses, Fluss has demonstrated outstanding performance and capabilities.\\n\\n![Alibaba Production](assets/fluss_asf/alibaba.png)\\n\\n### Why Apache?\\nThe Apache Software Foundation (ASF) is the cradle of global open-source big data technologies, nurturing numerous world-changing projects such as Hadoop, Spark, Iceberg, Kafka, and Flink. Fluss looks forward to joining the ASF and becoming a part of the movement that shapes the future of real-time infrastructure. At the same time, Fluss has a strong need for deep integration with these Apache projects, and joining the ASF will accelerate the integration process within the ecosystem. More importantly, the ASF\'s core values of openness, collaboration, and neutrality align closely with Fluss\'s vision. By joining the Apache Incubator, we align with this spirit and gain access to a larger community, better governance, and long-term sustainability.\\n\\n### Special Thanks\\nSpecial thanks to the Fluss incubation mentors for their valuable support and guidance during the project\'s journey into the ASF Incubator.\\n\\n* **@Yu Li (Champion):** PMC member of Flink and HBase projects, experienced mentor of multiple open-source projects, and successfully guided top-level projects such as Apache Paimon and Apache Celeborn.\\n* **@Jingsong Lee:** Chair of the Apache Paimon PMC and member of the Apache Flink PMC. \\n* **@Zili Chen:** A seasoned mentor of multiple open-source projects, PMC member of Pulsar, Zookeeper and Curator. He also serves as a member of the Apache Board in 2025. \\n* **@Becket Qin:** An active mentor of multiple open-source projects and PMC member of projects including Apache Flink and Apache Kafka. \\n* **@Jean-Baptiste Onofr\xe9:** Karaf PMC Chair, PMC on ACE, ActiveMQ, Archiva, Aries, Beam, Brooklyn, Camel, Felix, Incubator. He is also the incubation champion for Apache Polaris.\\n\\nWe would also like to express our gratitude to all contributors of the Fluss community!\\n\\n### Join the Surfing\\nWe sincerely invite developers and users who are interested in Fluss to join our open-source community and help drive the project forward. We look forward to your participation!\\n\\n* GitHub Repository: https://github.com/apache/fluss/  (give it some \u2764\ufe0f via \u2b50)\\n* Official Website: https://fluss.apache.org/\\n* Slack: [Apache Fluss ](https://join.slack.com/t/apache-fluss/shared_invite/zt-33wlna581-QAooAiCmnYboJS8D_JUcYw)\\n* Mailing List: `dev@fluss.apache.org` (by sending any mail to `dev-subscribe@fluss.apache.org` to subscribe)"},{"id":"fluss-java-client","metadata":{"permalink":"/blog/fluss-java-client","source":"@site/blog/2025-07-07-fluss-java-client.md","title":"Apache Fluss Java Client: A Deep Dive","description":"\x3c!--","date":"2025-07-07T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"name":"Giannis Polyzos","title":"PMC member of Apache Fluss","url":"https://github.com/polyzos","imageURL":"https://github.com/polyzos.png","key":"giannis","page":null}],"frontMatter":{"slug":"fluss-java-client","title":"Apache Fluss Java Client: A Deep Dive","authors":["giannis"]},"unlisted":false,"prevItem":{"title":"Fluss Joins the Apache Incubator","permalink":"/blog/fluss-joins-asf"},"nextItem":{"title":"Tiering Service Deep Dive","permalink":"/blog/tiering-service"}},"content":"\x3c!--\\n Licensed to the Apache Software Foundation (ASF) under one\\n or more contributor license agreements.  See the NOTICE file\\n distributed with this work for additional information\\n regarding copyright ownership.  The ASF licenses this file\\n to you under the Apache License, Version 2.0 (the\\n \\"License\\"); you may not use this file except in compliance\\n with the License.  You may obtain a copy of the License at\\n\\n      http://www.apache.org/licenses/LICENSE-2.0\\n\\n Unless required by applicable law or agreed to in writing, software\\n distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n See the License for the specific language governing permissions and\\n limitations under the License.\\n--\x3e\\n\\n![Banner](assets/java_client/banner.png)\\n\\n## Introduction\\nApache Fluss is a streaming data storage system built for real-time analytics, serving as a low-latency data layer in modern data Lakehouses.\\nIt supports sub-second streaming reads and writes, storing data in a columnar format for efficiency, and offers two flexible table types: **append-only Log Tables** and **updatable Primary Key Tables**. \\nIn practice, this means Fluss can ingest high-throughput event streams *(using log tables)* while also maintaining *up-to-date* reference data or state *(using primary key tables)*, a combination ideal for \\nscenarios like IoT, where you might stream sensor readings and look up information for those sensors in real-time, without\\nthe need for external K/V stores.\\n\x3c!-- truncate --\x3e\\n\\nIn this tutorial, we\'ll introduce the **Fluss Java Client** by walking through a simple home IoT system example. \\nWe will use `Fluss\'s Admin client` to create a primary key table for sensor information and a log table for sensor readings, then use the client \\nto write data to these tables and read/enrich the streaming sensor data. \\n\\nBy the end, you\'ll see how a sensor reading can be ingested into a log table and immediately enriched with information from a primary key table (essentially performing a real-time lookup join for streaming data enrichment).\\n\\n## Preflight Check\\nThe full source code can be found [here](https://github.com/ververica/ververica-fluss-examples/tree/main/fluss-java-client).\\n\\n```shell\\ndocker compose up\\n```\\n\\nThe first thing we need to do is establish a connection to the Fluss cluster. \\nThe `Connection` is the main entry point for the Fluss client, from which we obtain an `Admin` (for metadata operations) and Table instances (for data operations)\\n\\n```java\\n// Configure connection to Fluss cluster\\nConfiguration conf = new Configuration();\\nconf.setString(\\"bootstrap.servers\\", \\"localhost:9123\\");  // Fluss server endpoint\\nConnection connection = ConnectionFactory.createConnection(conf);\\n\\n// Get Admin client for managing databases and tables\\nAdmin admin = connection.getAdmin();\\n```\\nThe above code snippet shows the bare minimum requirements for connecting and interacting with a Fluss Cluster.\\nFor our example we will use the following mock data - to keep things simple - which you can find below:\\n```java\\npublic static final List<SensorReading> readings = List.of(\\n        new SensorReading(1, LocalDateTime.of(2025, 6, 23, 9, 15), 22.5, 45.0, 1013.2, 87.5),\\n        new SensorReading(2, LocalDateTime.of(2025, 6, 23, 9, 30), 23.1, 44.5, 1013.1, 88.0),\\n        new SensorReading(3, LocalDateTime.of(2025, 6, 23, 9, 45), 21.8, 46.2, 1012.9, 86.9),\\n        new SensorReading(4, LocalDateTime.of(2025, 6, 23, 10, 0), 24.0, 43.8, 1013.5, 89.2),\\n        new SensorReading(5, LocalDateTime.of(2025, 6, 23, 10, 15), 22.9, 45.3, 1013.0, 87.8),\\n        new SensorReading(6, LocalDateTime.of(2025, 6, 23, 10, 30), 23.4, 44.9, 1013.3, 88.3),\\n        new SensorReading(7, LocalDateTime.of(2025, 6, 23, 10, 45), 21.7, 46.5, 1012.8, 86.5),\\n        new SensorReading(8, LocalDateTime.of(2025, 6, 23, 11, 0), 24.2, 43.5, 1013.6, 89.5),\\n        new SensorReading(9, LocalDateTime.of(2025, 6, 23, 11, 15), 23.0, 45.1, 1013.2, 87.9),\\n        new SensorReading(10, LocalDateTime.of(2025, 6, 23, 11, 30), 22.6, 45.7, 1013.0, 87.4)\\n);\\n```\\n\\n```java\\npublic static final List<SensorInfo> sensorInfos = List.of(\\n        new SensorInfo(1, \\"Outdoor Temp Sensor\\", \\"Temperature\\", \\"Roof\\", LocalDate.of(2024, 1, 15), \\"OK\\", LocalDateTime.of(2025, 6, 23, 9, 15)),\\n        new SensorInfo(2, \\"Main Lobby Sensor\\", \\"Humidity\\", \\"Lobby\\", LocalDate.of(2024, 2, 20), \\"ERROR\\", LocalDateTime.of(2025, 6, 23, 9, 30)),\\n        new SensorInfo(3, \\"Server Room Sensor\\", \\"Temperature\\", \\"Server Room\\", LocalDate.of(2024, 3, 10), \\"MAINTENANCE\\", LocalDateTime.of(2025, 6, 23, 9, 45)),\\n        new SensorInfo(4, \\"Warehouse Sensor\\", \\"Pressure\\", \\"Warehouse\\", LocalDate.of(2024, 4, 5), \\"OK\\", LocalDateTime.of(2025, 6, 23, 10, 0)),\\n        new SensorInfo(5, \\"Conference Room Sensor\\", \\"Humidity\\", \\"Conference Room\\", LocalDate.of(2024, 5, 25), \\"OK\\", LocalDateTime.of(2025, 6, 23, 10, 15)),\\n        new SensorInfo(6, \\"Office 1 Sensor\\", \\"Temperature\\", \\"Office 1\\", LocalDate.of(2024, 6, 18), \\"LOW_BATTERY\\", LocalDateTime.of(2025, 6, 23, 10, 30)),\\n        new SensorInfo(7, \\"Office 2 Sensor\\", \\"Humidity\\", \\"Office 2\\", LocalDate.of(2024, 7, 12), \\"OK\\", LocalDateTime.of(2025, 6, 23, 10, 45)),\\n        new SensorInfo(8, \\"Lab Sensor\\", \\"Temperature\\", \\"Lab\\", LocalDate.of(2024, 8, 30), \\"ERROR\\", LocalDateTime.of(2025, 6, 23, 11, 0)),\\n        new SensorInfo(9, \\"Parking Lot Sensor\\", \\"Pressure\\", \\"Parking Lot\\", LocalDate.of(2024, 9, 14), \\"OK\\", LocalDateTime.of(2025, 6, 23, 11, 15)),\\n        new SensorInfo(10, \\"Backyard Sensor\\", \\"Temperature\\", \\"Backyard\\", LocalDate.of(2024, 10, 3), \\"OK\\", LocalDateTime.of(2025, 6, 23, 11, 30)),\\n\\n        // SEND SOME UPDATES\\n        new SensorInfo(2, \\"Main Lobby Sensor\\", \\"Humidity\\", \\"Lobby\\", LocalDate.of(2024, 2, 20), \\"ERROR\\", LocalDateTime.of(2025, 6, 23, 9, 48)),\\n        new SensorInfo(8, \\"Lab Sensor\\", \\"Temperature\\", \\"Lab\\", LocalDate.of(2024, 8, 30), \\"ERROR\\", LocalDateTime.of(2025, 6, 23, 11, 16))\\n);\\n```\\n\\n## Operating The Cluster\\nLet\'s create a database for our IoT data, and within it define two tables:\\n* **Sensor Readings Table:** A log table that will collect time-series readings from sensors (like temperature and humidity readings). This table is append-only (new records are added continuously, with no updates/deletes) which is ideal for immutable event streams\\n* **Sensor Information Table:** A primary key table that stores metadata for each sensor (like sensor ID, location, type). Each `sensorId` will be unique and acts as the primary key. This table can be updated as sensor info changes (e.g., sensor relocated or reconfigured). \\n\\nUsing the Admin client, we can programmatically create these tables. \\n\\nFirst, we\'ll ensure the database exists (creating it if not), then define schemas for each table and create them:\\n\\n### Schema Definitions\\n#### Log table (sensor readings)\\n\\n```java\\npublic static Schema getSensorReadingsSchema() {\\n    return Schema.newBuilder()\\n            .column(\\"sensorId\\", DataTypes.INT())\\n            .column(\\"timestamp\\", DataTypes.TIMESTAMP())\\n            .column(\\"temperature\\", DataTypes.DOUBLE())\\n            .column(\\"humidity\\", DataTypes.DOUBLE())\\n            .column(\\"pressure\\", DataTypes.DOUBLE())\\n            .column(\\"batteryLevel\\", DataTypes.DOUBLE())\\n            .build();\\n}\\n```\\n\\n#### Primary Key table (sensor information)\\n```java\\npublic static Schema getSensorInfoSchema() {\\n    return Schema.newBuilder()\\n            .column(\\"sensorId\\", DataTypes.INT())\\n            .column(\\"name\\", DataTypes.STRING())\\n            .column(\\"type\\", DataTypes.STRING())\\n            .column(\\"location\\", DataTypes.STRING())\\n            .column(\\"installationDate\\", DataTypes.DATE())\\n            .column(\\"state\\", DataTypes.STRING())\\n            .column(\\"lastUpdated\\", DataTypes.TIMESTAMP())\\n            .primaryKey(\\"sensorId\\")             <-- Define a Primary Key\\n            .build();\\n}\\n```\\n\\n### Table Creation\\n```java\\npublic static void setupTables(Admin admin) throws ExecutionException, InterruptedException {\\n    TableDescriptor readingsDescriptor = TableDescriptor.builder()\\n            .schema(getSensorReadingsSchema())\\n            .distributedBy(3, \\"sensorId\\")\\n            .comment(\\"This is the sensor readings table\\")\\n            .build();\\n\\n    // drop the tables or ignore if they exist\\n    admin.dropTable(readingsTablePath, true).get();\\n    admin.dropTable(sensorInfoTablePath, true).get();\\n     \\n    admin.createTable(readingsTablePath, readingsDescriptor, true).get();\\n    \\n    TableDescriptor sensorInfoDescriptor = TableDescriptor.builder()\\n            .schema(getSensorInfoSchema())\\n            .distributedBy(3, \\"sensorId\\")\\n            .comment(\\"This is the sensor information table\\")\\n            .build();\\n     \\n    admin.createTable(sensorInfoTablePath, sensorInfoDescriptor, true).get();\\n}\\n```\\nWe specify a distribution with `.distributedBy(3, \\"sensorId\\")`. \\nFluss tables are partitioned into buckets (similar to partitions in Kafka topics) for scalability. \\nHere we use 3 buckets, meaning data gets distributed across 3 buckets. Multiple buckets allow for higher throughput or to parallelize reads/writes. \\nIf using multiple buckets, Fluss would hash on the bucket key (`sensorId` in our case) to assign records to buckets.\\n\\nFor the `sensor_readings` table, we define a schema without any primary key. In Fluss, a table created without a primary key clause is a Log Table. \\nA log table only supports appending new records (no updates or deletes), making it perfect for immutable time-series data or logs.\\n\\nIn the log table, specifying a bucket key like `sensorId` ensures all readings from the same sensor end up to the same bucket providing strict ordering guarantees.\\n\\nWith our tables created let\'s go and write some data.\\n\\n## Table Writes\\nWith our tables in place, let\'s insert some data using the Fluss Java API. \\nThe client allows us to write or read data from it. \\nWe\'ll demonstrate two patterns:\\n* **Upserting** into the primary key table (sensor information). \\n* **Appending** to the log table (sensor readings).\\n\\nFluss provides specialized writer interfaces for each table type: an **UpsertWriter** for primary key tables and an **AppendWriter** for log tables. \\nUnder the hood, the Fluss client currently expects data as **GenericRow** objects (a generic row data format). \\n\\n> **Note:** Internally Fluss uses **InternalRow** as an optimized, binary representation of data for better performance and memory efficiency. \\n> **GenericRow** is a generic implementation of InternalRow. This allows developers to interact with data easily while Fluss processes it efficiently using the underlying binary format. \\n\\nSince we are creating **Pojos** though this means that we need to convert these into a GenericRow in order to write them into Fluss.\\n\\n```java\\npublic static GenericRow energyReadingToRow(SensorReading reading) {\\n    GenericRow row = new GenericRow(SensorReading.class.getDeclaredFields().length);\\n    row.setField(0, reading.sensorId());\\n    row.setField(1, TimestampNtz.fromLocalDateTime(reading.timestamp()));\\n    row.setField(2, reading.temperature());\\n    row.setField(3, reading.humidity());\\n    row.setField(4, reading.pressure());\\n    row.setField(5, reading.batteryLevel());\\n    return row;\\n}\\npublic static GenericRow sensorInfoToRow(SensorInfo sensorInfo) {\\n    GenericRow row = new GenericRow(SensorInfo.class.getDeclaredFields().length);\\n    row.setField(0, sensorInfo.sensorId());\\n    row.setField(1, BinaryString.fromString(sensorInfo.name()));\\n    row.setField(2, BinaryString.fromString(sensorInfo.type()));\\n    row.setField(3, BinaryString.fromString(sensorInfo.location()));\\n    row.setField(4, (int) sensorInfo.installationDate().toEpochDay());\\n    row.setField(5, BinaryString.fromString(sensorInfo.state()));\\n    row.setField(6, TimestampNtz.fromLocalDateTime(sensorInfo.lastUpdated()));     \\n    return row;\\n}\\n```\\n**Note:** For certain data types like `String` or `LocalDateTime` we need to use certain functions like\\n`BinaryString.fromString(\\"string_value\\")` or `TimestampNtz.fromLocalDateTime(datetime)` otherwise you might\\ncome across some conversion exceptions.\\n\\nLet\'s start by writing data to the `Log Table`. This requires getting an `AppendWriter` as follows:\\n\\n```java\\nlogger.info(\\"Creating table writer for table {} ...\\", AppUtils.SENSOR_READINGS_TBL);\\nTable table = connection.getTable(AppUtils.getSensorReadingsTablePath());\\nAppendWriter writer = table.newAppend().createWriter();\\n\\nAppUtils.readings.forEach(reading -> {\\n    GenericRow row = energyReadingToRow(reading);\\n    writer.append(row);\\n});\\nwriter.flush();\\n\\nlogger.info(\\"Sensor Readings Written Successfully.\\");\\n```\\nAt this point we have successfully written 10 sensor readings to our table.\\n\\n\\nNext, let\'s write data to the `Primary Key Table`. This requires getting an `UpsertWriter` as follows:\\n```java\\nlogger.info(\\"Creating table writer for table {} ...\\", AppUtils.SENSOR_INFORMATION_TBL);\\nTable sensorInfoTable = connection.getTable(AppUtils.getSensorInfoTablePath());\\nUpsertWriter upsertWriter = sensorInfoTable.newUpsert().createWriter();\\n\\nAppUtils.sensorInfos.forEach(sensorInfo -> {\\n    GenericRow row = sensorInfoToRow(sensorInfo);\\n    upsertWriter.upsert(row);\\n});\\n\\nupsertWriter.flush();\\n```\\nAt this point we have successfully written 10 sensor information records to our table, because \\nupdates will be handled on the primary key and merged.\\n\\n## Scans & Lookups\\nNow comes the real-time data enrichment part of our example. \\nWe want to simulate a process where each incoming sensor reading is immediately looked up against the sensor information table to add context (like location and type) to the raw reading.\\nThis is a common pattern in streaming systems, often achieved with lookup joins. \\n\\nWith the Fluss Java client, we can do this by combining a **log scanner on the readings table** with **point lookups on the sensor information table**.\\n\\nTo consume data from a Fluss table, we use a **Scanner*. \\nFor a log table, Fluss provides a **LogScanner** that allows us to **subscribe to one or more buckets** and poll for new records.\\n\\n```java\\nLogScanner logScanner = readingsTable.newScan()         \\n        .createLogScanner();\\n```\\n\\n```java\\nLookuper sensorInforLookuper = sensorInfoTable\\n        .newLookup()\\n        .createLookuper();\\n```\\n\\nWe set up a scanner on the `sensor_readings` table, and next we need to subscribe to all its buckets, and then poll for any available records:\\n```java\\nint numBuckets = readingsTable.getTableInfo().getNumBuckets();\\nfor (int i = 0; i < numBuckets; i++) {     \\n    logger.info(\\"Subscribing to Bucket {}.\\", i);\\n    logScanner.subscribeFromBeginning(i);\\n}\\n```\\n\\nStart polling for records. For each incoming record we will use the **Lookuper** to `lookup` sensor information from the primary key table,\\nand creating a **SensorReadingEnriched** record. \\n```java\\n while (true) {\\n    logger.info(\\"Polling for records...\\");\\n    ScanRecords scanRecords = logScanner.poll(Duration.ofSeconds(1));\\n    for (TableBucket bucket : scanRecords.buckets()) {\\n        for (ScanRecord record : scanRecords.records(bucket)) {\\n            InternalRow row = record.getRow();\\n            \\n            logger.info(\\"Received reading from sensor \'{}\' at \'{}\'.\\", row.getInt(0), row.getTimestampNtz(1, 6).toString());\\n            logger.info(\\"Performing lookup to get the information for sensor \'{}\'. \\", row.getInt(0));\\n            LookupResult lookupResult = sensorInforLookuper.lookup(row).get();\\n            SensorInfo sensorInfo = lookupResult.getRowList().stream().map(r -> new SensorInfo(\\n                    r.getInt(0),\\n                    r.getString(1).toString(),\\n                    r.getString(2).toString(),\\n                    r.getString(3).toString(),\\n                    LocalDate.ofEpochDay(r.getInt(4)),\\n                    r.getString(5).toString(),\\n                    LocalDateTime.parse(r.getTimestampNtz(6, 6).toString(), formatter)\\n            )).findFirst().get();\\n            logger.info(\\"Retrieved information for \'{}\' with id: {}\\", sensorInfo.name(), sensorInfo.sensorId());\\n\\n            SensorReading reading = new SensorReading(\\n                    row.getInt(0),\\n                    LocalDateTime.parse(row.getTimestampNtz(1, 6).toString(), formatter),\\n                    row.getDouble(2),\\n                    row.getDouble(3),\\n                    row.getDouble(4),\\n                    row.getDouble(5)\\n            );\\n\\n            SensorReadingEnriched readingEnriched = new SensorReadingEnriched(\\n                    reading.sensorId(),\\n                    reading.timestamp(),\\n                    reading.temperature(),\\n                    reading.humidity(),\\n                    reading.pressure(),\\n                    reading.batteryLevel(),\\n                    sensorInfo.name(),\\n                    sensorInfo.type(),\\n                    sensorInfo.location(),\\n                    sensorInfo.state()\\n            );\\n            logger.info(\\"Bucket: {} - {}\\", bucket, readingEnriched);\\n            logger.info(\\"---------------------------------------\\");\\n        }\\n    }\\n}\\n```\\nLet\'s summarize what\'s happening here:\\n* We create a LogScanner for the `sensor_readings` table using *table.newScan().createLogScanner()*. \\n* We subscribe to each bucket of the table from the beginning (offset 0). Subscribing `from beginning` means we\'ll read all existing data from the start; alternatively, one could subscribe from the latest position to only get new incoming data or based on other attributes like time. In our case, since we just inserted data, from-beginning will capture those inserts. \\n* We then call `poll(Duration)` on the scanner to retrieve available records, waiting up to the given timeout (1 second here). This returns a `ScanRecords` batch containing any records that were present. We iterate over each `TableBucket` and then over each `ScanRecord` within that bucket. \\n* For each record, we extract the fields via the InternalRow interface (which provides typed access to each column in the row) and **convert them into a Pojo**. \\n* Next, for each reading, we perform a **lookup** on the **sensor_information** table to get the sensor\'s info. We construct a key (GenericRow with just the sensor_id) and use **sensorTable.newLookup().createLookuper().lookup(key)**. This performs a point lookup by primary key and returns a `LookupResult future`; we call `.get()` to get the result synchronously. If present, we retrieve the InternalRow of the sensor information and **convert it into a Pojo**. \\n* We then combine the data: logging an enriched message that includes the sensor\'s information alongside the reading values. \\n\\nFluss\'s lookup API gives us quick primary-key retrieval from a table, which is exactly what we need to enrich the streaming data. \\nIn a real application, this enrichment could be done on the fly in a streaming job (and indeed **Fluss is designed to support high-QPS lookup joins in real-time pipelines**), but here we\'re simulating it with client calls for clarity.\\n\\nIf you run the above code found [here](https://github.com/ververica/ververica-fluss-examples), you should see an output like the following:\\n```shell\\n16:07:13.594 INFO  [DownloadRemoteLog-[sensors_db.sensor_readings_tbl]] c.a.f.c.t.s.l.RemoteLogDownloader$DownloadRemoteLogThread - Starting\\n16:07:13.599 INFO  [main] com.ververica.scanner.FlussScanner - Subscribing to Bucket 0.\\n16:07:13.599 INFO  [main] com.ververica.scanner.FlussScanner - Subscribing to Bucket 1.\\n16:07:13.600 INFO  [main] com.ververica.scanner.FlussScanner - Subscribing to Bucket 2.\\n16:07:13.600 INFO  [main] com.ververica.scanner.FlussScanner - Polling for records...\\n16:07:13.965 INFO  [main] com.ververica.scanner.FlussScanner - Received reading from sensor \'3\' at \'2025-06-23T09:45\'.\\n16:07:13.966 INFO  [main] com.ververica.scanner.FlussScanner - Performing lookup to get the information for sensor \'3\'. \\n16:07:14.032 INFO  [main] com.ververica.scanner.FlussScanner - Retrieved information for \'Server Room Sensor\' with id: 3\\n16:07:14.033 INFO  [main] com.ververica.scanner.FlussScanner - Bucket: TableBucket{tableId=2, bucket=1} - SensorReadingEnriched[sensorId=3, timestamp=2025-06-23T09:45, temperature=21.8, humidity=46.2, pressure=1012.9, batteryLevel=86.9, name=Server Room Sensor, type=Temperature, location=Server Room, state=MAINTENANCE]\\n16:07:14.045 INFO  [main] com.ververica.scanner.FlussScanner - ---------------------------------------\\n16:07:14.046 INFO  [main] com.ververica.scanner.FlussScanner - Received reading from sensor \'4\' at \'2025-06-23T10:00\'.\\n16:07:14.046 INFO  [main] com.ververica.scanner.FlussScanner - Performing lookup to get the information for sensor \'4\'. \\n16:07:14.128 INFO  [main] com.ververica.scanner.FlussScanner - Retrieved information for \'Warehouse Sensor\' with id: 4\\n16:07:14.128 INFO  [main] com.ververica.scanner.FlussScanner - Bucket: TableBucket{tableId=2, bucket=1} - SensorReadingEnriched[sensorId=4, timestamp=2025-06-23T10:00, temperature=24.0, humidity=43.8, pressure=1013.5, batteryLevel=89.2, name=Warehouse Sensor, type=Pressure, location=Warehouse, state=OK]\\n16:07:14.129 INFO  [main] com.ververica.scanner.FlussScanner - ---------------------------------------\\n16:07:14.129 INFO  [main] com.ververica.scanner.FlussScanner - Received reading from sensor \'8\' at \'2025-06-23T11:00\'.\\n16:07:14.129 INFO  [main] com.ververica.scanner.FlussScanner - Performing lookup to get the information for sensor \'8\'. \\n16:07:14.229 INFO  [main] com.ververica.scanner.FlussScanner - Retrieved information for \'Lab Sensor\' with id: 8\\n16:07:14.229 INFO  [main] com.ververica.scanner.FlussScanner - Bucket: TableBucket{tableId=2, bucket=1} - SensorReadingEnriched[sensorId=8, timestamp=2025-06-23T11:00, temperature=24.2, humidity=43.5, pressure=1013.6, batteryLevel=89.5, name=Lab Sensor, type=Temperature, location=Lab, state=ERROR]\\n16:07:14.229 INFO  [main] com.ververica.scanner.FlussScanner - ---------------------------------------\\n```\\n\\n## Column Pruning Scans\\nColumn pruning lets you fetch only the columns you need, **reducing network overhead and improving read performance**. With Fluss\u2019s Java client, you can specify a subset of columns in your scan:\\n```java\\nLogScanner logScanner = readingsTable.newScan()\\n    .project(List.of(\\"sensorId\\", \\"timestamp\\", \\"temperature\\"))\\n    .createLogScanner();\\n```\\n\\nLet\'s break this down:\\n* `.project(...)` instructs the client to request only the specified columns (sensorId,timestamp and temperature) from the server. \\n* Fluss\u2019s columnar storage means non-requested columns (e.g., humidity, etc.) **aren\u2019t transmitted, saving bandwidth and reducing client-side parsing overhead**. \\n* You can combine projection with filters or lookups to further optimize your data access patterns.\\n\\nExample output:\\n```shell\\n16:12:35.114 INFO  [main] com.ververica.scanner.FlussScanner - Subscribing to Bucket 0.\\n16:12:35.114 INFO  [main] com.ververica.scanner.FlussScanner - Subscribing to Bucket 1.\\n16:12:35.114 INFO  [main] com.ververica.scanner.FlussScanner - Subscribing to Bucket 2.\\n16:12:35.114 INFO  [main] com.ververica.scanner.FlussScanner - Polling for records...\\n16:12:35.171 INFO  [main] com.ververica.scanner.FlussScanner - Bucket: TableBucket{tableId=2, bucket=1} - (3,2025-06-23T09:45,21.8)\\n16:12:35.172 INFO  [main] com.ververica.scanner.FlussScanner - ---------------------------------------\\n16:12:35.172 INFO  [main] com.ververica.scanner.FlussScanner - Bucket: TableBucket{tableId=2, bucket=1} - (4,2025-06-23T10:00,24.0)\\n16:12:35.172 INFO  [main] com.ververica.scanner.FlussScanner - ---------------------------------------\\n16:12:35.172 INFO  [main] com.ververica.scanner.FlussScanner - Bucket: TableBucket{tableId=2, bucket=1} - (8,2025-06-23T11:00,24.2)\\n16:12:35.172 INFO  [main] com.ververica.scanner.FlussScanner - ---------------------------------------\\n16:12:35.172 INFO  [main] com.ververica.scanner.FlussScanner - Bucket: TableBucket{tableId=2, bucket=1} - (10,2025-06-23T11:30,22.6)\\n```\\nNotice, how only the requested columns are returned from the server.\\n\\n## Conclusion\\nIn this blog post, we\'ve introduced the Fluss Java Client by guiding you through a full example of creating tables, writing data, and reading/enriching data in real-time. \\nWe covered how to use the `Admin` client to define a **Primary Key table** (for reference data that can be updated) and a **Log table** (for immutable event streams), and how to use the Fluss client to upsert and append data accordingly. \\nWe also demonstrated reading from a log table using a scanner and performing a lookup on a primary key table to enrich the streaming data on the fly. \\n\\nThis IoT sensor scenario is just one example of Fluss in action and also highlights the **Stream/Table duality** within the same system. \\nFluss\'s ability to handle high-throughput append streams and fast key-based lookups makes it well-suited for real-time analytics use cases like this and many others. \\nWith this foundation, you can explore more advanced features of Fluss to build robust real-time data applications. Happy streaming! \ud83c\udf0a \\n\\nAnd before you go \ud83d\ude0a don\u2019t forget to give Fluss \ud83c\udf0a some \u2764\ufe0f via \u2b50 on [GitHub](https://github.com/alibaba/fluss)"},{"id":"tiering-service","metadata":{"permalink":"/blog/tiering-service","source":"@site/blog/2025-07-01-tiering-service.md","title":"Tiering Service Deep Dive","description":"\x3c!--","date":"2025-07-01T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"name":"GUO Yang","title":"Fluss Contributor","url":"https://github.com/gyang94","imageURL":"https://github.com/gyang94.png","key":"gyang94","page":null}],"frontMatter":{"slug":"tiering-service","title":"Tiering Service Deep Dive","authors":["gyang94"],"toc_max_heading_level":5},"unlisted":false,"prevItem":{"title":"Apache Fluss Java Client: A Deep Dive","permalink":"/blog/fluss-java-client"},"nextItem":{"title":"Announcing Fluss 0.7","permalink":"/blog/releases/0.7"}},"content":"\x3c!--\\n Licensed to the Apache Software Foundation (ASF) under one\\n or more contributor license agreements.  See the NOTICE file\\n distributed with this work for additional information\\n regarding copyright ownership.  The ASF licenses this file\\n to you under the Apache License, Version 2.0 (the\\n \\"License\\"); you may not use this file except in compliance\\n with the License.  You may obtain a copy of the License at\\n\\n      http://www.apache.org/licenses/LICENSE-2.0\\n\\n Unless required by applicable law or agreed to in writing, software\\n distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n See the License for the specific language governing permissions and\\n limitations under the License.\\n--\x3e\\n\\n# Tiering Service Deep Dive\\n\\n## Background\\n\\n![](assets/tiering_service/background.png)\\n\\nAt the core of Fluss\u2019s Lakehouse architecture sits the **Tiering Service:** a smart,\\npolicy-driven data pipeline that seamlessly bridges your real-time Fluss cluster and your cost-efficient lakehouse storage. It continuously ingests fresh events from the fluss cluster, automatically migrating older or less-frequently accessed data into colder storage tiers without interrupting ongoing queries. By balancing hot, warm, and cold storage according to configurable rules, the Tiering Service ensures that recent data remains instantly queryable while historical records are archived economically. \\n\\nIn this blog post we will take a deep dive and explore how Fluss\u2019s Tiering Service `orchestrates data movement`, `preserves consistency`, and empowers `scalable`, `high-performance` analytics at `optimized costs`.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Flink Tiering Service\\n\\nFluss tiering service is an Apache Flink job, which keeps moving data from fluss cluster to data lake. \\nThe execution plan is quite straight forward. It has a three operators: a `source`, a `committer` and an empty `sink writer`.\\n\\n```\\n Source: TieringSource -> TieringCommitter -> Sink: Writer\\n```\\n\\n- **TieringSource**: Reads records from the Fluss tiering table and writes them to the data lake.\\n- **TieringCommitter**: Commits each sync batch by advancing offsets in both the lakehouse and the Fluss cluster.\\n- **No-Op Sink**: A dummy sink that performs no action.\\n\\nIn the sections that follow, we\u2019ll dive into the **TieringSource** and **TieringCommitter** to see exactly how they orchestrate seamless data movement between real-time and historical storage.\\n\\n## TieringSource\\n\\n![](assets/tiering_service/tiering-source.png)\\n\\nThe **TieringSource** operator reads records from the Fluss tiering table and writes them into your data lake. \\nBuilt on Flink\u2019s Source V2 API ([FLIP-27](https://cwiki.apache.org/confluence/display/FLINK/FLIP-238:+Introduce+FLIP-27-based+Data+Generator+Source)), it breaks down into two core components: the **TieringSourceEnumerator** and the **TieringSourceReader**. \\nThe high-level workflow is as follows:\\n\\n1. The **Enumerator** queries the **CoordinatorService** for current tiering table metadata.\\n2. Once it receives the table information, the Enumerator generates `\u201csplits\u201d` (data partitions) and assigns them to the **Reader**.\\n3. The **Reader** fetches the actual data for each split.\\n4. Finally the **Reader** writes those records into the data lake.\\n\\nIn the following sections, we\u2019ll explore how the **TieringSourceEnumerator** and **TieringSourceReader** work under the hood to deliver reliable, scalable ingestion from Fluss into your lakehouse.\\n\\n### TieringSourceEnumerator\\n\\n![](assets/tiering_service/tiering-source-enumerator.png)\\n\\nThe **TieringSourceEnumerator** orchestrates split creation and assignment in five key steps:\\n\\n1. **Heartbeat Request**: Uses an RPC client to send a `lakeTieringHeartbeatRequest` to the Fluss server.\\n2. **Heartbeat Response**: Receives a `lakeTieringHeartbeatResponse` that contains the tiering table metadata and sync statuses for `completed`, `failed`, and `in-progress` tables.\\n3. **Lake Tiering Info**: Forwards the returned `lakeTieringInfo` to the `TieringSplitGenerator`.\\n4. **Split Generation**: The `TieringSplitGenerator` produces a set of `TieringSplits`, each representing a data partition to process.\\n5. **Split Assignment**: Assigns those `TieringSplits` to `TieringSourceReader` instances for downstream ingestion into the data lake.\\n\\n#### RpcClient\\n\\nThe `RpcClient` inside the `TieringSourceEnumerator` handles all RPC communication with the Fluss CoordinatorService. Its responsibilities include:\\n\\n- **Sending Heartbeats**: It constructs and sends a `LakeTieringHeartbeatRequest`, which carries three lists of tables\u2014`tiering_tables` (in-progress), `finished_tables`, and `failed_tables`\u2014along with an optional `request_table` flag to request new tiering work.\\n- **Receiving Responses**: It awaits a `LakeTieringHeartbeatResponse` that contains:\\n    - `coordinator_epoch`: the current epoch of the coordinator.\\n    - `tiering_table` (optional): a `PbLakeTieringTableInfo` message (with `table_id`, `table_path`, and `tiering_epoch`) describing the next table to tier.\\n    - `tiering_table_resp`, `finished_table_resp`, and `failed_table_resp`: lists of heartbeat responses reflecting the status of each table.\\n- **Forwarding Metadata**: It parses the returned `PbLakeTieringTableInfo` and the sync-status responses, then forwards the assembled `lakeTieringInfo` to the `TieringSplitGenerator` for split creation.\\n\\n#### TieringSplitGenerator\\n\\n![](assets/tiering_service/tiering-split-generator.png)\\n\\nThe **TieringSplitGenerator** is an important component that orchestrates efficient data synchronization between your real-time Fluss cluster and your lakehouse.\\nIt precisely calculates the data `\\"delta\\"`, i.e what\'s new or changed in Fluss but not yet committed to the lake and then generates **TieringSplit** tasks for each segment requiring synchronization.\\n\\nTo achieve this, the `TieringSplitGenerator` leverages the `FlussAdminClient` to fetch three essential pieces of metadata:\\n\\n**Lake Snapshot**\\n\\nThe generator first invokes the lake metadata API to retrieve a **LakeSnapshot** object. This snapshot provides a complete picture of the current state of your data in the lakehouse, including:\\n   * `snapshotId:` The identifier for the latest committed snapshot in your data lake.\\n   * `tableBucketsOffset:` A map that details the log offset in the lakehouse for each `TableBucket`.\\n\\n**Current Bucket Offsets**\\n\\nNext, the `TieringSplitGenerator` queries the Fluss server to determine the **current log end offset** for each bucket. This effectively captures the high-water mark of incoming data streams in real time within your Fluss cluster.\\n\\n**KV Snapshots (for primary-keyed tables)**\\n\\nFor tables that utilize primary keys, the generator also retrieves a **KvSnapshots** record. This record contains vital information for maintaining consistency with key-value stores:\\n     * `tableId` and an optional `partitionId`.\\n     * `snapshotIds:` The latest snapshot ID specific to each bucket.\\n     * `logOffsets:` The exact log position from which to resume reading after that snapshot, ensuring seamless data ingestion.\\n\\nWith the `LakeSnapshot`, the live bucket offsets from the Fluss cluster, and (where applicable) the `KvSnapshots`, the `TieringSplitGenerator` performs its core function: it computes which log segments are present in Fluss but have not yet been committed to the lakehouse.\\n\\nFinally, for each identified segment, it produces a distinct **TieringSplit**. Each `TieringSplit` precisely defines the specific bucket and the exact offset range that needs to be ingested. This meticulous process ensures incremental, highly efficient synchronization, seamlessly bridging your real-time operational data with your historical, cost-optimized storage.\\n\\n#### TieringSplit\\n\\nThe **TieringSplit** abstraction defines exactly which slice of a table bucket needs to be synchronized. It captures three common fields:\\n\\n- **tablePath**: the full path to the target table.\\n- **tableBucket**: the specific bucket (shard) within that table.\\n- **partitionName** (optional): the partition key, if the table is partitioned.\\n\\nThere are two concrete split types:\\n\\n1. **TieringLogSplit** (for append-only \u201clog\u201d tables)\\n    - **startingOffset**: the last committed log offset in the lake.\\n    - **stoppingOffset**: the current end offset in the live Fluss bucket.\\n    - This split defines a contiguous range of new log records to ingest.\\n2. **TieringSnapshotSplit** (for primary-keyed tables)\\n    - **snapshotId**: the identifier of the latest snapshot in Fluss.\\n    - **logOffsetOfSnapshot**: the log offset at which that snapshot was taken.\\n    - This split lets the TieringSourceReader replay all CDC (change-data-capture) events since the snapshot, ensuring up-to-date state.\\n\\nBy breaking each table into these well-defined splits, the Tiering Service can incrementally, reliably, and in parallel sync exactly the data that\u2019s missing from your data lake.\\n\\n### TieringSourceReader\\n\\n![](assets/tiering_service/tiering-source-reader.png)\\n\\nThe **TieringSourceReader** pulls assigned splits from the enumerator, uses a `TieringSplitReader` to fetch the corresponding records from the Fluss server, and then writes them into the data lake. Its workflow breaks down as follows:\\n\\n- **Split Selection:** The reader picks an assigned `TieringSplit` from its queue.\\n- **Reader Dispatch:** Depending on the split type, it instantiates either:\\n  - **LogScanner** for `TieringLogSplit` (append-only tables)\\n  - **BoundedSplitReader** for `TieringSnapshotSplit` (primary-keyed tables)\\n- **Data Fetch:** The chosen reader fetches the records defined by the split\u2019s offset or snapshot boundaries from the Fluss server.\\n- **Lake Writing\\"** Retrieved records are handed off to the lake writer, which persists them into the data lake.\\n\\nBy cleanly separating split assignment, reader selection, data fetching, and lake writing, the TieringSourceReader ensures scalable, parallel ingestion of streaming and snapshot data into your lakehouse.\\n\\n#### LakeWriter & LakeTieringFactory\\n\\nThe LakeWriter is responsible for persisting Fluss records into your data lake, and it\u2019s instantiated via a pluggable LakeTieringFactory. This interface defines how Fluss interacts with various lake formats (e.g., Paimon, Iceberg):\\n\\n```java\\npublic interface LakeTieringFactory {\\n\\n\\tLakeWriter<WriteResult> createLakeWriter(WriterInitContext writerInitContext);\\n\\n\\tSimpleVersionedSerializer<WriteResult> getWriteResultSerializer();\\n\\n\\tLakeCommitter<WriteResult, CommitableT> createLakeCommitter(\\n            CommitterInitContext committerInitContext);\\n\\n\\tSimpleVersionedSerializer<CommitableT> getCommitableSerializer();\\n}\\n```\\n- **createLakeWriter(WriterInitContext)**: builds a `LakeWriter` to convert Fluss rows into the target table format.\\n- **getWriteResultSerializer()**: supplies a serializer for the writer\u2019s output.\\n- **createLakeCommitter(CommitterInitContext)**: constructs a `LakeCommitter` to finalize and atomically commit data files.\\n- **getCommitableSerializer()**: provides a serializer for committable tokens.```\\n\\nBy default, Fluss includes a Paimon-backed tiering factory; Iceberg support is coming soon. Once the `TieringSourceReader` writes a batch of records through the `LakeWriter`, it emits the resulting write metadata downstream to the **TieringCommitOperator**, which then commits those changes both in the lakehouse and back to the Fluss cluster.\\n\\n#### Stateless\\n\\nThe `TieringSourceReader` is designed to be completely stateless\u2014it does not checkpoint or store any `TieringSplit` information itself. Instead, every checkpoint simply returns an empty list, leaving all split-tracking to the `TieringSourceEnumerator`:\\n\\n```java\\n@Override\\npublic List<TieringSplit> snapshotState(long checkpointId) {\\n    // Stateless: no splits are held in reader state\\n    return Collections.emptyList();\\n}\\n```\\n\\nBy delegating split assignment entirely to the Enumerator, the reader remains lightweight and easily scalable, always fetching its next work unit afresh from the coordinator.\\n\\n## TieringCommitter\\n\\n![](assets/tiering_service/tiering-committer.png)\\n\\nThe **TieringCommitter** operator wraps up each sync cycle by taking the `WriteResult` outputs from the TieringSourceReader and committing them in two phases:\\nfirst to the data lake, then back to Fluss, before emitting status events to the Flink coordinator. It leverages two  components:\\n\\n- **LakeCommitter**: Provided by the pluggable `LakeTieringFactory`, this component atomically commits the written files into the lakehouse and returns the new snapshot ID.\\n- **FlussTableLakeSnapshotCommitter**: Using that snapshot ID, it updates the Fluss cluster\u2019s tiering table status so that the Fluss server and lakehouse remain in sync.\\n\\nThe end-to-end flow is:\\n\\n1. **Collect Write Results** from the TieringSourceReader for the current checkpoint.\\n2. **Lake Commit** via the `LakeCommitter`, which finalizes files and advances the lake snapshot.\\n3. **Fluss Update** using the `FlussTableLakeSnapshotCommitter`, acknowledging success or failure back to the Fluss CoordinatorService.\\n4. **Event Emission** of either `FinishedTieringEvent` (on success or completion) or `FailedTieringEvent` (on errors) to the Flink `OperatorCoordinator`.\\n\\nThis TieringCommitter operator ensures exactly-once consistent synchronization between your real-time Fluss cluster and your analytical lakehouse.\\n\\n## Conclusion\\n\\nIn this deep dive, we thoroughly explored every facet of Fluss\'s **Tiering Service**. \\nWe began by dissecting the **TieringSource**, understanding the critical roles of its Enumerator, RpcClient, and SplitGenerator. From there, we examined the various split types and the efficiency of the stateless **TieringSourceReader**.\\n\\nOur journey then led us to the flexible, pluggable integration of the **LakeWriter** and **LakeCommitter**. Finally, we saw how the **TieringCommitter**, with its LakeCommitter and FlussTableLakeSnapshotCommitter, orchestrates **atomic**, **exactly-once commits** across both your data lake and Fluss cluster.\\n\\nTogether, these components form a robust pipeline. This pipeline reliably synchronizes real-time streams with historical snapshots, ensuring **seamless**, **scalable consistency** between your live workloads and analytical storage."},{"id":"/releases/0.7","metadata":{"permalink":"/blog/releases/0.7","source":"@site/blog/releases/0.7.md","title":"Announcing Fluss 0.7","description":"\x3c!--","date":"2025-06-18T00:00:00.000Z","tags":[{"inline":false,"label":"releases","permalink":"/blog/tags/releases","description":"Content related release announcement."}],"hasTruncateMarker":true,"authors":[{"name":"Jark Wu","title":"PMC member of Apache Fluss","url":"https://github.com/wuchong","imageURL":"https://github.com/wuchong.png","key":"jark","page":null}],"frontMatter":{"title":"Announcing Fluss 0.7","authors":["jark"],"date":"2025-06-18T00:00:00.000Z","tags":["releases"]},"unlisted":false,"prevItem":{"title":"Tiering Service Deep Dive","permalink":"/blog/tiering-service"},"nextItem":{"title":"Understanding Partial Updates","permalink":"/blog/partial-updates"}},"content":"\x3c!--\\n Licensed to the Apache Software Foundation (ASF) under one\\n or more contributor license agreements.  See the NOTICE file\\n distributed with this work for additional information\\n regarding copyright ownership.  The ASF licenses this file\\n to you under the Apache License, Version 2.0 (the\\n \\"License\\"); you may not use this file except in compliance\\n with the License.  You may obtain a copy of the License at\\n\\n      http://www.apache.org/licenses/LICENSE-2.0\\n\\n Unless required by applicable law or agreed to in writing, software\\n distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n See the License for the specific language governing permissions and\\n limitations under the License.\\n--\x3e\\n\\n![Banner](../assets/0.7/banner.png)\\n\\n\ud83c\udf0a We are excited to announce the official release of **Fluss 0.7**!\\n\\nThis version has undergone extensive improvements in **stability**, **architecture**, **performance optimization**, and **security**, further enhancing its readiness for **production environments**. Over the past three months, we have completed more than **250 commits**, making this release a significant milestone toward becoming a mature, production-grade streaming storage platform.\\n\x3c!-- truncate --\x3e\\n\\n![Improvements Diagram](../assets/0.7/overview.png)\\n\\n## Stability Enhancements\\nIn this release, we have dedicated significant effort to enhancing system stability. By building a comprehensive stability testing framework covering end-to-end processes and multi-scenario fault simulations, combined with large-scale stress testing using real business data, and through rigorous production-level stability validation in Alibaba, we have addressed and resolved over 50 issues. This has significantly improved the stability of Fluss\' core modules and the overall robustness of the system. The major improvements include:\\n\\n* **Online/Offline Node Optimization:** Refactored the ISR (In-Sync Replica) update mechanism and replica synchronization logic and enhanced idempotency guarantees. This significantly improves system stability and reduces the risk of data loss during node failures.\\n* **Server Metadata Caching:** Introduced a consistent metadata caching layer on the server side. Clients now fetch metadata from the local server cache instead of ZooKeeper, significantly reducing request latency and pressure on ZooKeeper.\\n* **Server Rack-Aware Support:** During replica assignment, the system automatically avoids placing multiple replicas within the same \\"rack\\", thereby significantly improving fault tolerance and high availability. This feature is especially beneficial in multi-AZ deployments and large-scale data center disaster recovery scenarios. You can configure the \\"rack\\" of the TabletServer via setting `tablet-server.rack` in `server.yaml`.\\n* **Accelerated Table Creation:** Leveraged batched metadata updates to reduce table initialization time. For example, the time for a table creation with 1024 buckets was reduced from minutes to milliseconds.\\n* **Optimized Read/Write Pipelines:**\\n  * Introduce bucket-id-based shuffle for primary key tables to improve write throughput. \\n  * Dynamically estimate the batch size based on incoming traffic to optimize memory usage and throughput. \\n  * Optimize the Arrow memory release logic to enhance job stability.\\n\\nThese enhancements make Fluss 0.7 ready for most production use cases.\\n\\n## New Architecture of the Streaming Lakehouse\\nIn Fluss 0.5, we first introduced the Streaming Lakehouse feature, with the powerful Union Read ability, we can significantly reduce the cost of streaming storage and improve the data freshness of Lakehouse.\\n> Note: Union Reads allows querying and combining the results from both Lakehouse (historical) and Fluss (real-time) data.\\n\\nHowever, the initial implementation had architectural limitations affecting scalability and operability in production. In Fluss 0.7, we\u2019ve completely re-architected the Streaming Lakehouse feature to address these challenges.\\n\\n### Elastic Stateless Service\\nPreviously, the lake tiering service was implemented as a Flink job encapsulating Fluss as a Source and Paimon as a Sink, storing sync offsets state in Flink\u2019s State, this making it a stateful service. \\nThis led to several operational issues:\\n* **Single Point of Failure:** The lifecycle of each table is bound to a specific Flink job. Once a job fails, the synchronization of all tables hosted by the job will be blocked. \\n* **Limited Scalability:** Could only scale vertically (increase job resources), not horizontally (start multiple jobs). \\n* **Inflexible Scheduling:** Unable to prioritize tables and dynamically assign to dedicated Flink clusters.\\n* **Opaque State:** Flink State being a black box made monitoring and troubleshooting the offset difficult.\\n\\nTo address these challenges, we\'ve re-architected the lake tiering service into a truly elastic and stateless component.\\nThe sync offset is now persisted directly within Fluss metadata, with future plans for integration into Paimon v1.2 snapshot properties.\\nFurthermore, Flink jobs have transitioned from maintaining persistent table subscriptions to a more efficient model where they process one table at a time,\\ndynamically requesting the next table upon completion of the current one.\\n\\n\\n![New Architecture Diagram](../assets/0.7/new_tiering_service.png)\\n\\nThis optimization significantly reduces the load on Fluss and boosts batch processing efficiency. As a result, cluster operators gain:\\n* **Enhanced Service Robustness:** Operators can launch multiple Flink jobs to distribute workload and increase overall system resilience.\\n* **Flexible Resource Management:** The ability to stop idle jobs at any time allows for immediate reclamation of valuable cluster resources.\\n* **Dynamic Task Orchestration:** Sync tasks can now be dynamically scheduled across all active Flink jobs, optimizing resource utilization. \\n* **Actionable Offset Visibility:** The offset state is now queryable, providing greater insight and control over data processing.\\n\\nThis design ensures end-to-end consistency across all operations.\\nFurthermore, this stateless design also decouples us from the tight Flink dependency, paving the way for future lightweight execution models, such as running on FaaS (Function as a Service).\\n\\n### Pluggable Lake Format\\nThe previous implementation had a tight coupling with Apache Paimon, which restricted our ability to integrate with other lake formats, such as Iceberg. \\nWith version 0.7, we have abstracted and modularized all lake format interfaces.\\nThis enhancement enables easy, plugin-style support for Iceberg and other emerging formats.\\n\\n### Native Flink Command Submission\\nPreviously, users were limited to the `lakehouse.sh` script for initiating the lake tiering service, which submitted jobs to the Flink cluster.\\nWhile convenient, this approach restricted deployment flexibility, particularly when encountering diverse Flink deployment modes and internal product platforms. \\nFluss 0.7 now addresses this by supporting **native job submission via standard Flink commands** (`flink run`). \\nThis enhancement ensures broad compatibility with various deployment modes while significantly lowering learning and integration costs.\\nThe following is an example of how to submit the lake tiering service using the native Flink command:\\n\\n```bash\\nflink run /path/to/fluss-flink-tiering-0.7.0.jar \\\\\\n    --fluss.bootstrap.servers localhost:9123 \\\\\\n    --datalake.format paimon \\\\\\n    --datalake.paimon.metastore filesystem \\\\\\n    --datalake.paimon.warehouse /path/to/warehouse\\n```\\n\\nSee more details in the [Streaming Lakehouse documentation](/docs/maintenance/tiered-storage/lakehouse-storage/).\\n\\n## Streaming Partition Pruning\\nPartitioning is a foundational technique in modern data warehouses and Lakehouse architectures for optimizing query performance by \\nlogically dividing datasets along meaningful dimensions (e.g., time, region, business line).\\n\\nFluss 0.7 introduces **streaming partition pruning**, enabling selective reading of relevant partitions based on query conditions. \\nFor example, if a query filters on `nation_key = \'US\'`, the Fluss Source will only read matching partitions,\\nsignificantly reducing network I/O and compute overhead. \\nApart from that, we also support the following advanced partition features to make partition pruning adaptable and easy to use:\\n* **Multi-level Partitioning:** Supports nested partition strategies (e.g., `dt=20250617/region=US/business=sale`).\\n* **Dynamic Partition Creation:** Automatically creates required partitions based on incoming data, no manual pre-creation is required.\\n* **Automatic Partition Discovery:** Fluss source adds matched new partitions to the subscription in real-time.\\n\\nUsing real business data from **Taobao - the largest online shopping platform in China,** we tested the read and write performance between non-partitioned and partitioned tables (with 20 auto-created partitions). The write results show that the multi-level partition and dynamic partition creation mechanism do not have a significant impact on the write performance.\\n![Write Perf](../assets/0.7/write_perf.jpg)\\n\\nAt the same time, under the same data scale, we tested the streaming read performance of non-partitioned tables and partitioned tables under three partition conditions: \\n**unconditional**, **medium matching** (hitting five partitions), and **exact matching** (hitting one partition). \\nFrom the results, we can observe that when the partition condition only matches 1/20 of the partitions,\\n**the network traffic is reduced by about 20x** and the **processing time is reduced by nearly 9x**, \\ndemonstrating the huge performance benefit of partition pruning in streaming reads.\\n\\n\\n![Read Perf](../assets/0.7/read_perf.jpg)\\n\\nStreaming partition pruning is the second pushdown feature introduced after streaming column pruning in Fluss.\\nLooking ahead, we plan to introduce **predicate pushdown with Arrow batch-level I/O pruning** to further enhance query efficiency.\\n\\n## Enterprise-Grade Security\\nTo meet enterprise-grade security requirements, Fluss 0.7 fully supports Authentication and Authorization mechanisms.\\n\\n**Authentication** is the process of confirming the identity of the client. \\nFluss introduced a plugin-based authentication with built-in **SASL/PLAIN** support, compatible with **JAAS** (Java Authentication and Authorization Service) configuration for both client and server credentials.\\n\\n**Authorization** controls which resources the identity can access and which operations it can perform after identity confirmation. \\nFluss implements fine-grained permission control through the **ACL** (Access Control List) mechanism, supporting multi-level access control at the Cluster, Database, and Table levels.\\n\\n\\nAdditionally, we\'ve integrated Flink SQL\'s `CALL` statements to ease ACL permission management for users.\\nFor instance, granting read access to the `mydb` database for user `Tim` can now be accomplished with the following command:\\n\\n```sql\\nCALL admin_catalog.sys.add_acl(\\n    resource => \'cluster.mydb\', \\n    permission => \'ALLOW\',\\n    principal => \'User:Tim\', \\n    operation => \'READ\',\\n    host => \'*\'\\n);\\n```\\n\\nFor details, please refer to the [Security documentation](/docs/security/overview/) and quickstarts.\\n\\n## Flink DataStream Connector\\nFluss 0.7 officially introduces the DataStream Connector, supporting both Source and Sink for reading and writing log and primary key tables. Users can now seamlessly integrate Fluss tables into Flink DataStream pipelines.\\n\\nHere\u2019s an example of reading data from a Fluss table into a Flink DataStream:\\n```java\\nFlussSource<Order> source = FlussSource.<Order>builder()\\n    .setBootstrapServers(\\"localhost:9092\\")\\n    .setDatabase(\\"mydb\\")\\n    .setTable(\\"orders\\")\\n    // column pruning\\n    .setProjectedFields(\\"orderId\\", \\"amount\\")\\n    .setStartingOffsets(OffsetsInitializer.earliest())\\n    .setDeserializationSchema(new OrderDeserializationSchema())\\n    .build();\\n\\nDataStreamSource<Order> stream = env.fromSource(\\n    source,\\n    WatermarkStrategy.noWatermarks(),\\n    \\"Fluss Source\\"\\n);\\n```\\n\\nFor usage examples and configuration parameters, see the [DataStream Connector documentation](https://alibaba.github.io/fluss-docs/docs/engine-flink/datastream/).\\n\\n\\n## Fluss Java Client\\nIn this version, we officially release the Fluss Java Client, a client library designed for developers working with structured stream tables. The client includes two core API modules:\\n\\n* **Table API:** For table-based data operations, supporting streaming reads/writes, updates, deletions, and point queries.\\n* **Admin API:** For metadata management, including cluster management, table lifecycle, and access control.\\n\\nThe client supports forward and backward compatibility, ensuring smooth upgrades across Fluss versions. With the Fluss Java Client, developers can build online applications and data ingestion services based on Fluss, as well as enterprise-level components such as Fluss management platforms and operations monitoring systems. For detailed usage instructions, please refer to the official documentation: [Fluss Java Client User Guide](https://alibaba.github.io/fluss-docs/docs/apis/java-client/).\\n\\nFluss uses Apache Arrow as its underlying storage format, enabling efficient cross-language extensions. A **Fluss Python Client** is planned for future releases, leveraging the rich ecosystem of **PyArrow** to integrate with popular data analysis tools such as **Pandas** and **DuckDB**. \\nThis will further lower the barrier for real-time data exploration and analytics.\\n\\n## Future Roadmap\\nIn the next releases, we will continue to enhance system robustness and operational capabilities and plan to introduce **Rolling Upgrades** and **Cluster Rebalance**. \\nIn addition, with the new pluggable datalake format of the Streaming Lakehouse, we will further expand support for the mainstream datalake table formats, such as an **Apache Iceberg** integration. \\nMeanwhile, we will explore the possibility of using Fluss in **multimodal AI** use cases, supporting ingestion of multimodal data, \\nand integrating with **[Lance](https://github.com/lancedb/lance)** format in the Streaming Lakehouse architecture.\\n\\nFluss is under active development. Be sure to stay updated on the project, give it a try and if you like it, \\ndon\u2019t forget to give it some \u2764\ufe0f via \u2b50 on [GitHub](https://github.com/alibaba/fluss).\\n\\n## List of contributors\\nThe Fluss community would like to express gratitude to all the contributors who made this release possible:\\n\\nBenchao Li,  CaoZhen,  Feng Wang,  Giannis Polyzos,  HZY,  Hongshun Wang,  Jark Wu,  Junbo wang,  Kerwin,  Leonard Xu,  MehulBatra,  Michael Koepf,  Min Zhao,  Nicholas Jiang,  Radek Grebski,  Rohan Dubey,  Xiaojian Sun,  Yang Guo,  dao-jun,  gkatzioura,  luoyuxia,  majialong,  xiaozhou,  yunhong,  yuxia Luo,  yx9o,  zhangmang,  \u9053\u541b"},{"id":"partial-updates","metadata":{"permalink":"/blog/partial-updates","source":"@site/blog/2025-06-01-partial-updates.md","title":"Understanding Partial Updates","description":"\x3c!--","date":"2025-06-01T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"name":"Giannis Polyzos","title":"PMC member of Apache Fluss","url":"https://github.com/polyzos","imageURL":"https://github.com/polyzos.png","key":"giannis","page":null}],"frontMatter":{"slug":"partial-updates","title":"Understanding Partial Updates","authors":["giannis"]},"unlisted":false,"prevItem":{"title":"Announcing Fluss 0.7","permalink":"/blog/releases/0.7"},"nextItem":{"title":"The Story of Fluss Logo","permalink":"/blog/unveil-fluss-logo"}},"content":"\x3c!--\\n Licensed to the Apache Software Foundation (ASF) under one\\n or more contributor license agreements.  See the NOTICE file\\n distributed with this work for additional information\\n regarding copyright ownership.  The ASF licenses this file\\n to you under the Apache License, Version 2.0 (the\\n \\"License\\"); you may not use this file except in compliance\\n with the License.  You may obtain a copy of the License at\\n\\n      http://www.apache.org/licenses/LICENSE-2.0\\n\\n Unless required by applicable law or agreed to in writing, software\\n distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n See the License for the specific language governing permissions and\\n limitations under the License.\\n--\x3e\\n\\n![Banner](assets/partial_updates/banner.png)\\n\\nTraditional streaming data pipelines often need to join many tables or streams on a primary key to create a wide view.\\nFor example, imagine you\u2019re building a real-time recommendation engine for an e-commerce platform. \\nTo serve highly personalized recommendations, your system needs a complete 360\xb0 view of each user, including: \\n*user preferences*, *past purchases*, *clickstream behavior*, *cart activity*, *product reviews*, *support tickets*, *ad impressions*, and *loyalty status*.\\n\\nThat\u2019s at least **8 different data sources**, each producing updates independently.\\n\x3c!-- truncate --\x3e\\nJoining multiple data streams at scale, although it works with Apache Flink it can be really challenging and resource-intensive. \\nMore specifically, it can lead to:\\n* **Really large state sizes in Flink:** as it needs to buffer all incoming events until they can be joined. In many case states need to be kept around for a long period of time if not indefinitely.\\n* **Deal with checkpoints overhead and backpressure:** as the join operation and large state uploading can create a bottleneck in the pipeline.\\n* **States are not easy to inspect and debug:** as they are often large and complex. This can make it difficult to understand what is happening in the pipeline and why certain events are not being processed correctly.\\n* **State TTL can lead to inconsistent results:** as events may be dropped before they can be joined. This can lead to data loss and incorrect results in the final output.\\n\\nOverall, this approach not only consumes a lot of memory and CPU, but also complicates the job design and maintenance.\\n\\n![Streaming Joins](assets/partial_updates/streaming_join.png)\\n\\n### Partial Updates: A Different Approach with Fluss\\nFluss introduces a more elegant solution: **partial updates** on a primary key table. \\n\\nInstead of performing multi-way joins in the streaming job, Fluss allows each data stream source to independently update only its relevant columns into a shared wide table identified by the primary key. \\nIn Fluss, you can define a wide table (for example, a user_profile table based on a `user_id`) that contains all possible fields from all sources. \\nEach source stream then writes partial rows \u2013 only the fields it knows about \u2013 into this table.\\n\\n![Partial Update](assets/partial_updates/partial_update.png)\\n\\nFluss\u2019s storage engine automatically merges these partial updates together based on the primary key. \\nEssentially, Fluss maintains the latest combined value for each key, so you don\u2019t have to manage large join states in Flink. \\n\\nUnder the hood, when a new partial update for a key arrives, Fluss will look up the existing record for that primary key, update the specific columns provided, and leave other columns unchanged. \\nThe result is written back as the new version of the record. \\nThis happens in *real-time*, so the table is **always up-to-date** with the latest information from all streams. \\n\\nNext, let\'s try and better understand how this works in practice with a concrete example.\\n### Example: Building a Unified Wide Table\\n> You can find the full source code on github [here](https://github.com/ververica/ververica-fluss-examples/tree/main/partial_updates).\\n\\nStart by cloning the repository, run `docker compose up` to spin up the development enviroment and finally grab a terminal \\ninto the `jobmanager` and start the Flink SQL cli, by running the following command:\\n```shell\\n./bin/sql-client.sh\\n```\\n\\nGreat so far ! \ud83d\udc4d\\n\\n**Step 1:** The first thing we need to do is to create a Flink catalog that will be used to store the tables we are going to create.\\nLet\'s create a catalog called `fluss_catalog` and use this catalog.\\n```sql\\nCREATE CATALOG fluss_catalog WITH (\\n    \'type\' = \'fluss\',\\n    \'bootstrap.servers\' = \'coordinator-server:9123\'\\n);\\n\\nUSE CATALOG fluss_catalog;\\n```\\n\\n**Step 2:** Then let\'s create `3 tables` to represent the different data sources that will be used to build the recommendations wide table.\\n```sql\\n-- Recommendations \u2013 model scores\\nCREATE TABLE recommendations (\\n    user_id  STRING,\\n    item_id  STRING,\\n    rec_score DOUBLE,\\n    rec_ts   TIMESTAMP(3),\\n    PRIMARY KEY (user_id, item_id) NOT ENFORCED\\n) WITH (\'bucket.num\' = \'3\', \'table.datalake.enabled\' = \'true\');\\n\\n\\n-- Impressions \u2013 how often we showed something\\nCREATE TABLE impressions (\\n    user_id STRING,\\n    item_id STRING,\\n    imp_cnt INT,\\n    imp_ts  TIMESTAMP(3),\\n    PRIMARY KEY (user_id, item_id) NOT ENFORCED\\n) WITH (\'bucket.num\' = \'3\', \'table.datalake.enabled\' = \'true\');\\n\\n-- Clicks \u2013 user engagement\\nCREATE TABLE clicks (\\n    user_id  STRING,\\n    item_id  STRING,\\n    click_cnt INT,\\n    clk_ts    TIMESTAMP(3),\\n    PRIMARY KEY (user_id, item_id) NOT ENFORCED\\n) WITH (\'bucket.num\' = \'3\', \'table.datalake.enabled\' = \'true\');\\n\\nCREATE TABLE user_rec_wide (\\n    user_id   STRING,\\n    item_id   STRING,\\n    rec_score DOUBLE,   -- updated by recs stream\\n    imp_cnt   INT,      -- updated by impressions stream\\n    click_cnt INT,      -- updated by clicks stream\\n    PRIMARY KEY (user_id, item_id) NOT ENFORCED\\n) WITH (\'bucket.num\' = \'3\', \'table.datalake.enabled\' = \'true\');\\n```\\n\\n**Step 3:** Of course, we will need some sample data to work with , so let\'s go on and insert some records into the tables. \ud83d\udcbb\\n```sql\\n-- Recommendations \u2013 model scores\\nINSERT INTO recommendations VALUES\\n    (\'user_101\',\'prod_501\',0.92 , TIMESTAMP \'2025-05-16 09:15:02\'),\\n    (\'user_101\',\'prod_502\',0.78 , TIMESTAMP \'2025-05-16 09:15:05\'),\\n    (\'user_102\',\'prod_503\',0.83 , TIMESTAMP \'2025-05-16 09:16:00\'),\\n    (\'user_103\',\'prod_501\',0.67 , TIMESTAMP \'2025-05-16 09:16:20\'),\\n    (\'user_104\',\'prod_504\',0.88 , TIMESTAMP \'2025-05-16 09:16:45\');\\n```\\n\\n```sql\\n-- Impressions \u2013 how often each (user,item) was shown\\nINSERT INTO impressions VALUES\\n    (\'user_101\',\'prod_501\', 3, TIMESTAMP \'2025-05-16 09:17:10\'),\\n    (\'user_101\',\'prod_502\', 1, TIMESTAMP \'2025-05-16 09:17:15\'),\\n    (\'user_102\',\'prod_503\', 7, TIMESTAMP \'2025-05-16 09:18:22\'),\\n    (\'user_103\',\'prod_501\', 4, TIMESTAMP \'2025-05-16 09:18:30\'),\\n    (\'user_104\',\'prod_504\', 2, TIMESTAMP \'2025-05-16 09:18:55\');\\n```\\n\\n```sql\\n-- Clicks \u2013 user engagement\\nINSERT INTO clicks VALUES\\n    (\'user_101\',\'prod_501\', 1, TIMESTAMP \'2025-05-16 09:19:00\'),\\n    (\'user_101\',\'prod_502\', 2, TIMESTAMP \'2025-05-16 09:19:07\'),\\n    (\'user_102\',\'prod_503\', 1, TIMESTAMP \'2025-05-16 09:19:12\'),\\n    (\'user_103\',\'prod_501\', 1, TIMESTAMP \'2025-05-16 09:19:20\'),\\n    (\'user_104\',\'prod_504\', 1, TIMESTAMP \'2025-05-16 09:19:25\');\\n```\\n\\n> **Note:** \ud83d\udea8 So far the jobs we run were bounded jobs, so they will finish after inserting the records. Moving forward we will run some streaming jobs. \\nSo keep in mind that each job runs with a `parallelism of 3` and our environment is set up `with 10 slots total`. \\nSo make sure to keep an eye to the Flink Web UI to see how many slots are used and how many are available and stop some jobs when are no longer needed to free up resourecs.\\n\\n\\n**Step 4:** At this point let\'s open up a separate terminal and start the Flink SQL CLI.\\nIn this new terminal, make sure to run set the `result-mode`:\\n```shell\\nSET \'sql-client.execution.result-mode\' = \'tableau\';\\n```\\nand then run:\\n```sql\\nSELECT * FROM user_rec_wide;\\n```\\nto observe the output of the table, as we insert `partially` records into the it from the different sources.\\n\\n**Step 5:** Let\'s insert the records from the `recommendations` table into the `user_rec_wide` table.\\n```sql\\n-- Apply recommendation scores\\nINSERT INTO user_rec_wide (user_id, item_id, rec_score)\\nSELECT\\n    user_id,\\n    item_id,\\n    rec_score\\nFROM recommendations;\\n```\\n\\n**Output:** Notice, how only the related columns are updated in the `user_rec_wide` table and the rest of the columns are `NULL`.\\n```shell\\nFlink SQL> SELECT * FROM user_rec_wide;\\n+----+--------------------------------+--------------------------------+--------------------------------+-------------+-------------+\\n| op |                        user_id |                        item_id |                      rec_score |     imp_cnt |   click_cnt |\\n+----+--------------------------------+--------------------------------+--------------------------------+-------------+-------------+\\n| +I |                       user_101 |                       prod_501 |                           0.92 |      <NULL> |      <NULL> |\\n| +I |                       user_101 |                       prod_502 |                           0.78 |      <NULL> |      <NULL> |\\n| +I |                       user_104 |                       prod_504 |                           0.88 |      <NULL> |      <NULL> |\\n| +I |                       user_102 |                       prod_503 |                           0.83 |      <NULL> |      <NULL> |\\n| +I |                       user_103 |                       prod_501 |                           0.67 |      <NULL> |      <NULL> |\\n```\\n\\n**Step 5:** Next, let\'s insert the records from the `impressions` table into the `user_rec_wide` table.\\n```sql\\n-- Apply impression counts\\nINSERT INTO user_rec_wide (user_id, item_id, imp_cnt)\\nSELECT\\n    user_id,\\n    item_id,\\n    imp_cnt\\nFROM impressions;\\n```\\n\\n**Output:** Notice how the `impressions` records are inserted into the `user_rec_wide` table and the `imp_cnt` column is updated.\\n```shell\\nFlink SQL> SELECT * FROM user_rec_wide;\\n+----+--------------------------------+--------------------------------+--------------------------------+-------------+-------------+\\n| op |                        user_id |                        item_id |                      rec_score |     imp_cnt |   click_cnt |\\n+----+--------------------------------+--------------------------------+--------------------------------+-------------+-------------+\\n| +I |                       user_101 |                       prod_501 |                           0.92 |      <NULL> |      <NULL> |\\n| +I |                       user_101 |                       prod_502 |                           0.78 |      <NULL> |      <NULL> |\\n| +I |                       user_104 |                       prod_504 |                           0.88 |      <NULL> |      <NULL> |\\n| +I |                       user_102 |                       prod_503 |                           0.83 |      <NULL> |      <NULL> |\\n| +I |                       user_103 |                       prod_501 |                           0.67 |      <NULL> |      <NULL> |\\n\\n\\n\\n| -U |                       user_101 |                       prod_501 |                           0.92 |      <NULL> |      <NULL> |\\n| +U |                       user_101 |                       prod_501 |                           0.92 |           3 |      <NULL> |\\n| -U |                       user_101 |                       prod_502 |                           0.78 |      <NULL> |      <NULL> |\\n| +U |                       user_101 |                       prod_502 |                           0.78 |           1 |      <NULL> |\\n| -U |                       user_104 |                       prod_504 |                           0.88 |      <NULL> |      <NULL> |\\n| +U |                       user_104 |                       prod_504 |                           0.88 |           2 |      <NULL> |\\n| -U |                       user_102 |                       prod_503 |                           0.83 |      <NULL> |      <NULL> |\\n| +U |                       user_102 |                       prod_503 |                           0.83 |           7 |      <NULL> |\\n| -U |                       user_103 |                       prod_501 |                           0.67 |      <NULL> |      <NULL> |\\n| +U |                       user_103 |                       prod_501 |                           0.67 |           4 |      <NULL> |\\n```\\n\\n**Step 6:** Finally, let\'s insert the records from the `clicks` table into the `user_rec_wide` table.\\n```sql\\n-- Apply click counts\\nINSERT INTO user_rec_wide (user_id, item_id, click_cnt)\\nSELECT\\n    user_id,\\n    item_id,\\n    click_cnt\\nFROM clicks;\\n```\\n\\n**Output:** Notice how the `clicks` records are inserted into the `user_rec_wide` table and the `click_cnt` column is updated.\\n```shell\\nFlink SQL> SELECT * FROM user_rec_wide;\\n+----+--------------------------------+--------------------------------+--------------------------------+-------------+-------------+\\n| op |                        user_id |                        item_id |                      rec_score |     imp_cnt |   click_cnt |\\n+----+--------------------------------+--------------------------------+--------------------------------+-------------+-------------+\\n| +I |                       user_101 |                       prod_501 |                           0.92 |      <NULL> |      <NULL> |\\n| +I |                       user_101 |                       prod_502 |                           0.78 |      <NULL> |      <NULL> |\\n| +I |                       user_104 |                       prod_504 |                           0.88 |      <NULL> |      <NULL> |\\n| +I |                       user_102 |                       prod_503 |                           0.83 |      <NULL> |      <NULL> |\\n| +I |                       user_103 |                       prod_501 |                           0.67 |      <NULL> |      <NULL> |\\n\\n\\n\\n| -U |                       user_101 |                       prod_501 |                           0.92 |      <NULL> |      <NULL> |\\n| +U |                       user_101 |                       prod_501 |                           0.92 |           3 |      <NULL> |\\n| -U |                       user_101 |                       prod_502 |                           0.78 |      <NULL> |      <NULL> |\\n| +U |                       user_101 |                       prod_502 |                           0.78 |           1 |      <NULL> |\\n| -U |                       user_104 |                       prod_504 |                           0.88 |      <NULL> |      <NULL> |\\n| +U |                       user_104 |                       prod_504 |                           0.88 |           2 |      <NULL> |\\n| -U |                       user_102 |                       prod_503 |                           0.83 |      <NULL> |      <NULL> |\\n| +U |                       user_102 |                       prod_503 |                           0.83 |           7 |      <NULL> |\\n| -U |                       user_103 |                       prod_501 |                           0.67 |      <NULL> |      <NULL> |\\n| +U |                       user_103 |                       prod_501 |                           0.67 |           4 |      <NULL> |\\n\\n\\n| -U |                       user_103 |                       prod_501 |                           0.67 |           4 |      <NULL> |\\n| +U |                       user_103 |                       prod_501 |                           0.67 |           4 |           1 |\\n| -U |                       user_101 |                       prod_501 |                           0.92 |           3 |      <NULL> |\\n| +U |                       user_101 |                       prod_501 |                           0.92 |           3 |           1 |\\n| -U |                       user_101 |                       prod_502 |                           0.78 |           1 |      <NULL> |\\n| +U |                       user_101 |                       prod_502 |                           0.78 |           1 |           2 |\\n| -U |                       user_104 |                       prod_504 |                           0.88 |           2 |      <NULL> |\\n| +U |                       user_104 |                       prod_504 |                           0.88 |           2 |           1 |\\n| -U |                       user_102 |                       prod_503 |                           0.83 |           7 |      <NULL> |\\n| +U |                       user_102 |                       prod_503 |                           0.83 |           7 |           1 |\\n```\\n\\n**Reminder:** \u203c\ufe0fAs mentioned before make sure to stop the jobs that are no longer needed to free up resources.\\n\\nNow let\'s switch to `batch` mode and query the current snapshot of the `user_rec_wide` table.\\n\\nBut before that, let\'s start the [Tiering Service](https://alibaba.github.io/fluss-docs/docs/maintenance/tiered-storage/lakehouse-storage/#start-the-datalake-tiering-service) that allows offloading the tables as `Lakehouse` tables.\\n\\n**Step 7:** Open a new terminal \ud83d\udcbb in the `Coordinator Server` and run the following command to start the `Tiering Service`:\\n```shell\\n./bin/lakehouse.sh -D flink.rest.address=jobmanager -D flink.rest.port=8081 -D flink.execution.checkpointing.interval=30s -D flink.parallelism.default=2\\n```\\n\\nThe configured checkpoint is `flink.execution.checkpointing.interval=30s` so wait a bit until the first checkpoint is created\\nand data gets offloading to the `Lakehouse` tables.\\n\\n**Step 8:** Finally let\'s switch to `batch` mode and query the current snapshot of the `user_rec_wide` table.\\n```shell\\nSET \'execution.runtime-mode\' = \'batch\';\\n\\nFlink SQL> SELECT * FROM user_rec_wide;\\n+----------+----------+-----------+---------+-----------+\\n|  user_id |  item_id | rec_score | imp_cnt | click_cnt |\\n+----------+----------+-----------+---------+-----------+\\n| user_102 | prod_503 |      0.83 |       7 |         1 |\\n| user_103 | prod_501 |      0.67 |       4 |         1 |\\n| user_101 | prod_501 |      0.92 |       3 |         1 |\\n| user_101 | prod_502 |      0.78 |       1 |         2 |\\n| user_104 | prod_504 |      0.88 |       2 |         1 |\\n+----------+----------+-----------+---------+-----------+\\n5 rows in set (2.63 seconds)\\n```\\n\\n\ud83c\udf89 That\'s it! You have successfully created a unified wide table using partial updates in Fluss.\\n\\n### Conclusion\\nPartial updates in Fluss enable an alternative approach in how we design streaming data pipelines for enriching or joining data. \\n\\nWhen all your sources share a primary key - otherwise you can mix & match [streaming lookup joins](https://alibaba.github.io/fluss-docs/docs/engine-flink/lookups/#lookup) - you can turn the problem on its head: update a unified table incrementally, rather than joining streams on the fly. \\n\\nThe result is a more scalable, maintainable, and efficient pipeline. \\nEngineers can spend less time wrestling with Flink\u2019s state, checkpoints and join mechanics, and more time delivering fresh, integrated data to power real-time analytics and applications. \\nWith Fluss handling the merge logic, achieving a single, up-to-date view from multiple disparate streams becomes way more elegant. \ud83d\ude01\\n\\nAnd before you go \ud83d\ude0a don\u2019t forget to give Fluss \ud83c\udf0a some \u2764\ufe0f via \u2b50 on [GitHub](https://github.com/alibaba/fluss)"},{"id":"unveil-fluss-logo","metadata":{"permalink":"/blog/unveil-fluss-logo","source":"@site/blog/2025-05-28-unveil-fluss-logo.md","title":"The Story of Fluss Logo","description":"\x3c!--","date":"2025-05-28T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"name":"Jark Wu","title":"PMC member of Apache Fluss","url":"https://github.com/wuchong","imageURL":"https://github.com/wuchong.png","key":"jark","page":null}],"frontMatter":{"slug":"unveil-fluss-logo","title":"The Story of Fluss Logo","authors":["jark"]},"unlisted":false,"prevItem":{"title":"Understanding Partial Updates","permalink":"/blog/partial-updates"},"nextItem":{"title":"Announcing Fluss 0.6","permalink":"/blog/releases/0.6"}},"content":"\x3c!--\\n Licensed to the Apache Software Foundation (ASF) under one\\n or more contributor license agreements.  See the NOTICE file\\n distributed with this work for additional information\\n regarding copyright ownership.  The ASF licenses this file\\n to you under the Apache License, Version 2.0 (the\\n \\"License\\"); you may not use this file except in compliance\\n with the License.  You may obtain a copy of the License at\\n\\n      http://www.apache.org/licenses/LICENSE-2.0\\n\\n Unless required by applicable law or agreed to in writing, software\\n distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n See the License for the specific language governing permissions and\\n limitations under the License.\\n--\x3e\\n\\n![](@site/static/img/logo/png/colored_logo.png)\\n\\n## Introducing the Little Otter\\n\\nToday is [World Otter Day](https://www.otter.org/world-otter-day), and we are thrilled to introduce the little otter to the Fluss community! \ud83c\udf89\\n\\nSince open-sourced half a year ago, many community members and friends have asked us:\\n\\"When will Fluss get a logo?\\" After more than a month of careful design work and over 30 iterations, we\u2019re excited to finally unveil the official Fluss logo \u2014 a surfing otter! \ud83e\udda6\ud83c\udf0a\\n\\n\x3c!-- truncate --\x3e\\n\\nI don\'t want to interpret the symbolic meaning behind the Logo in a rigid way. Instead, I\u2019d like to share the story behind its design \u2014 how we defined the direction and iterated step by step toward the final version. This process involved numerous subjective decisions made by the team. You might prefer one of the discarded versions, or disagree with some of the design choices. However, we want to share the journey with our community, and we hope it can also serve as a reference or inspiration for other open-source projects when designing their logos.\\n\\n## Logo symbolism: what do we want to convey?\\n[Fluss](https://github.com/alibaba/fluss) is an open-source streaming storage system designed for analytical workloads, with the goal of serving as a real-time data layer for Lakehouse architectures. Therefore, ***fluidity*** is the first key message we want to convey.\\n\\nSecondly, Fluss\u2019s vision is to **\\"Bring better analytics to data streams, and bring better freshness to data Lakehouses\\"**. This requires the ability to adapt to multiple data lake formats and support a variety of query engines and compute engines. So, ***adaptability*** is the second core idea we want to convey.\\n\\nAdditionally, as an open-source project planning to be donated to the Apache Software Foundation, building an open, diverse, and collaborative community has always been one of our core goals. In line with this value, we wanted the brand to express ***friendliness***, which ultimately led us to choose animal as the logo design direction.\\n\\nWe have also considered using abstract graphics for logo design. However, we noticed that many open-source projects that start with abstract logos, eventually introduce animal mascots to convey the friendliness of the community. These mascots not only play an important role in community outreach but also often replace the main logo in many contexts, becoming more representative visual symbols \u2014 such as Go\u2019s gopher, Rust\u2019s crab, and Airbyte\u2019s octopus. Animal images are easier to get closer to users and easier to spread and extend. For example, the small squirrel of the Flink community, not only becomes a symbol of the project but also inspires a wide range of community gifts.\\n\\n![](assets/fluss_logo/image1.png)\\n\\nTherefore, we made clear the direction in the early stage of Fluss Logo design: use an Animal image to convey our concept of **fluidity**, **adaptability**, and **friendliness**. The project name \\"Fluss\\" comes from the German word for \\"river\\" (Fluss), and we also hope that this animal has a close relationship with the river. Combined with these characteristics, the first animal that comes to our mind is the **otter**. The otter is an animal that lives in rivers and is known for its flexible swimming posture and strong adaptability. At the same time, because of its smart and lovely appearance, it is widely used in many kinds of dolls and cartoon images, is a symbol of affinity. This fits exactly the idea Fluss conveys.\\n\\n\\n![](assets/fluss_logo/image2.png)\\n\\n## Design iterations for more than 30 versions\\n\\nAfter clarifying the design direction and core values of Fluss Logo, we began to communicate with the design team in depth and launched multiple rounds of iteration. However, in the early versions of the design, we still couldn\u2019t find the right feel \u2014 something was missing: the fluidity.\\n\\n\\n![](assets/fluss_logo/image3.png)\\n\\nTherefore, we used ChatGPT to find inspiration, and tried a variety of prompt words, and this picture with a surfing wave quickly caught our eyes: dynamic, energy, speed, and flow, which is what we want! Although this otter looks like a Jerry rat... our design team quickly got our idea.\\n\\n![](assets/fluss_logo/image4.png)\\n\\nBased on the core elements of \u201cotter\u201d + \u201csurfing\u201d, we started a new round of design, and we iterated more than 20 versions based on this and finally evolved to the version you see today.\\n\\n![](assets/fluss_logo/image5.png)\\n\\nSubsequently, we also made many variations of the logo to adapt to different environments and background colors.\\n\\n\\n\\n![](assets/fluss_logo/image6.png)\\n\\n## Get feedback from community users\\n\\nDesigning a logo is a highly subjective process. But for an open-source project, the goal isn\'t to please the founding team \u2014 it\'s to resonate with the community. That\u2019s why we\'ve always valued community feedback and used it to guide our iterations.\\n\\nSome of the community feedback I loved very much:\\n\\n- \\"The Fluss otter seems to be a sibling to Flink\'s squirrel.\\"\\n- \\"I see Paimon in it!\\"\\n- \\"It\'s soooo cute!\\"\\n\\nGive it some \u2764\ufe0f via \u2b50 on GitHub if you like it as well!\\nhttps://github.com/alibaba/fluss\\n\\n## Community Egg\\n\\nAt the time of the Fluss logo release, we also submitted an [incubation proposal](https://lists.apache.org/thread/osg23opm9x95xm318160808r984k0wk9) to the Apache Software Foundation, thanks to the strong support from our incubation mentors Yu Li, tison, Jingsong Lee, Becket Qin, and Jean-Baptiste. We\'re confident that Fluss will soon become one of the Apache projects.\\n\\nIn parallel, we\u2019ve designed and launched a series of Fluss community gifts, including stickers, t-shirts, and coffee cups. These items will be available at upcoming community events \u2014 stay tuned and pick your swags!\\n\\n![](assets/fluss_logo/image7.png)"},{"id":"/releases/0.6","metadata":{"permalink":"/blog/releases/0.6","source":"@site/blog/releases/0.6.md","title":"Announcing Fluss 0.6","description":"\x3c!--","date":"2025-03-10T00:00:00.000Z","tags":[{"inline":false,"label":"releases","permalink":"/blog/tags/releases","description":"Content related release announcement."}],"hasTruncateMarker":true,"authors":[{"name":"Jark Wu","title":"PMC member of Apache Fluss","url":"https://github.com/wuchong","imageURL":"https://github.com/wuchong.png","key":"jark","page":null}],"frontMatter":{"title":"Announcing Fluss 0.6","authors":["jark"],"date":"2025-03-10T00:00:00.000Z","tags":["releases"]},"unlisted":false,"prevItem":{"title":"The Story of Fluss Logo","permalink":"/blog/unveil-fluss-logo"},"nextItem":{"title":"Towards A Unified Streaming & Lakehouse Architecture","permalink":"/blog/unified-streaming-lakehouse"}},"content":"\x3c!--\\n Licensed to the Apache Software Foundation (ASF) under one\\n or more contributor license agreements.  See the NOTICE file\\n distributed with this work for additional information\\n regarding copyright ownership.  The ASF licenses this file\\n to you under the Apache License, Version 2.0 (the\\n \\"License\\"); you may not use this file except in compliance\\n with the License.  You may obtain a copy of the License at\\n\\n      http://www.apache.org/licenses/LICENSE-2.0\\n\\n Unless required by applicable law or agreed to in writing, software\\n distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n See the License for the specific language governing permissions and\\n limitations under the License.\\n--\x3e\\n\\nThe Fluss community is pleased to announce the official release of **Fluss 0.6.0**. This version has undergone over\\nthree months of intensive development, bringing together the expertise and efforts of 45 contributors worldwide,\\nwith more than 200 code commits completed. Our heartfelt thanks go out to every contributor for their invaluable support!\\n\\n![Release Announcement](../assets/0.6/announce.png)\\n\\n\x3c!-- truncate --\x3e\\n\\nThis release introduces several exciting features:\\n\\n- **Column Compression**: Reduces storage space by up to **6x** while preserving column pruning performance!\\n- **Merge Engine**: Introduces flexible merge strategies for primary key data, addressing diverse real-time processing needs.\\n- **Prefix Lookup**: Delta Join functionality is now ready on the Fluss side!\\n\\nThese new features not only significantly enhance the functionality of Fluss but also represent a solid step\\nforward in our journey toward building the next-generation analytical stream storage solution.\\n\\n## Column Compression\\n\\nFluss uses the [Apache Arrow](https://arrow.apache.org/) columnar format for it\'s underlying log file storage, fully leveraging Arrow\'s streaming\\ncolumnar capabilities to achieve highly efficient streaming reads and column pruning. Column pruning in Fluss is performed on the server side,\\nwith end-to-end zero-copy optimization that allows the required column data to be sent directly to the network without\\nloading it into memory from disk. This design not only significantly improves performance but also drastically reduces\\nnetwork I/O costs and resource overhead. In previous [benchmark tests](../2024-12-12-fluss-intro.md#columnar-stream),\\nwhen 90% of the columns were pruned, Fluss achieved a 10x increase in read throughput, demonstrating its exceptional\\nperformance in streaming data processing and transmission.\\n\\nWhile column pruning effectively reduces network I/O costs, disk storage costs remain high. To address this,\\nwe have introduced **column compression** in this version, supporting two highly efficient compression algorithms: **ZSTD** and **LZ4**.\\nThese algorithms significantly reduce data storage requirements, thereby lowering storage costs substantially.\\nSince both compression and decompression are performed on the client side, the amount of data transmitted over the network is reduced,\\nfurther decreasing network I/O costs. Notably, compression is applied independently to each column, ensuring that the original column pruning performance is preserved and that streaming read efficiency remains unaffected.\\n\\nTo validate the actual effectiveness of this feature, we conducted benchmark tests using a typical business scenario from Alibaba Taobao.\\nIn the tests, we used datasets of the same scale and Flink jobs with identical resources, writing to Fluss with and without ZSTD compression,\\nand compared the write throughput. Subsequently, we read data from the table and tested the read throughput.\\nThe results showed that column compression not only reduced storage space by approximately 6x but also improved read and write throughput.\\n\\n![Compression Benchmark](../assets/0.6/compression1.jpg)\\n\\nHowever, enabling compression had no noticeable impact on Flink\'s read/write CPU and memory usage.\\n\\n![Compression Benchmark](../assets/0.6/compression2.jpg)\\n\\nThe performance of column pruning on compressed data was also tested. The results show that as the number of pruned columns increases,\\na multiple-fold performance improvement is still achieved, maintaining the original column pruning efficiency.\\n\\n![Compression Benchmark](../assets/0.6/compression3.jpg)\\n\\nGiven the significant cost savings and performance improvements achieved by column compression in general use cases,\\nZSTD compression is enabled by default for log tables in Fluss 0.6. Users can disable compression by\\nsetting the parameter `\'table.log.arrow.compression.type\'=\'NONE\'` on the table.\\n\\n## Merge Engine\\n\\nIn this version, Fluss introduces a new Merge Engine feature for primary key tables to flexibly support merging strategies for data with the same primary key.\\nThe default Merge Engine strategy for primary key tables is to retain the latest record for each primary key.\\nUsers can also choose alternative Merge Engines, including the currently supported **FirstRow Merge Engine** and **Versioned Merge Engine**.\\nSupport for the **Aggregate Merge Engine** is planned for future releases.\\n\\n### FirstRow Merge Engine\\n\\nBy setting the table property `\'table.merge-engine\' = \'first_row\'`, users can retain the first record for each primary key.\\nWhen this configuration is enabled, the primary key table will generate an append-only changelog. This allows downstream\\nFlink jobs subscribing to the table to receive an append-only stream, enabling the use of operators that do not support retraction messages,\\nsuch as Window Aggregations and Interval Joins. This feature is commonly used as a replacement for log deduplication in streaming processing,\\neffectively reducing costs and system complexity.\\n\\n```sql\\n-- create first_row primary key table\\nCREATE TABLE T (\\n    k  INT,\\n    v1 DOUBLE,\\n    v2 STRING,\\n    PRIMARY KEY (k) NOT ENFORCED\\n) WITH (\\n    \'table.merge-engine\' = \'first_row\'\\n);\\n\\nINSERT INTO T VALUES (1, 2.0, \'t1\');\\nINSERT INTO T VALUES (1, 3.0, \'t2\');\\n\\nSELECT * FROM T WHERE k = 1;\\n\\n-- Output\\n-- +---+-----+------+\\n-- | k | v1  | v2   |\\n-- +---+-----+------+\\n-- | 1 | 2.0 | t1   |\\n-- +---+-----+------+\\n```\\n\\n### Versioned Merge Engine\\n\\nThe Versioned Merge Engine supports data updates based on version numbers (or event timestamps). It ensures that only the record\\nwith the highest version number (or event timestamp) for each primary key is retained. This mechanism is particularly useful for\\ndeduplicating or merging out-of-order data while guaranteeing eventual consistency with the upstream data source.\\nIn Flink streaming processing, this feature can be used as a replacement for `Rank` or `Deduplication` operations, simplifying workflows and reducing costs effectively.\\n\\n```sql\\n-- create a versioned primary key table, `ts` as the version column\\nCREATE TABLE VERSIONED (\\n    a INT NOT NULL PRIMARY KEY NOT ENFORCED,\\n    b STRING,\\n    ts BIGINT\\n ) WITH (\\n    \'table.merge-engine\' = \'versioned\',\\n    \'table.merge-engine.versioned.ver-column\' = \'ts\'\\n);\\nINSERT INTO VERSIONED (a, b, ts) VALUES (1, \'v1\', 1000);\\n\\n-- insert a record with ts < 1000, ignored\\nINSERT INTO VERSIONED (a, b, ts) VALUES (1, \'v2\', 999);\\nSELECT * FROM VERSIONED WHERE a = 1;\\n-- Output\\n-- +---+-----+------+\\n-- | a | b   | ts   |\\n-- +---+-----+------+\\n-- | 1 | v1  | 1000 |\\n-- +---+-----+------+\\n\\n\\n-- insert a record with ts > 1000, updated\\nINSERT INTO VERSIONED (a, b, ts) VALUES (1, \'v3\', 2000);\\nSELECT * FROM VERSIONED WHERE a = 1;\\n-- Output\\n-- +---+-----+------+\\n-- | a | b   | ts   |\\n-- +---+-----+------+\\n-- | 1 | v3  | 2000 |\\n-- +---+-----+------+\\n```\\n\\n## Prefix Lookup for Delta Join\\n\\nIn the scenario of building wide tables with Flink, optimizing Stream-Stream Join using Delta Join is one of the primary use cases for Fluss.\\nWe have also contributed this functionality to version 0.6. Delta Join can be simply understood as a \\"bilateral driven lookup join\\", that is:\\nwhen data arrives from the left stream, the right table is queried using the join key; when data arrives from the right stream, the left table is queried using the join key.\\nThis approach eliminates join state like Lookup Join, while preserving the semantics of a Stream-Stream Join (any updates on either side triggers an update to the join result).\\nDelta Join addresses challenges such as high cost, unstable job, checkpoint timeout, slow restart recovery, etc., in the traditional Stream-Stream Join.\\n\\n![Delta Join](../assets/0.6/delta-join.jpg)\\n\\nOverall, Delta Join relies on three core functionalities:\\n- **CDC Stream Read for Source Table**: The foundational capability of Fluss.\\n- **Lookup on Join Keys for Source Table**: Introduced in Fluss 0.6 with Prefix Lookup support.\\n- **Delta Join Operator in Flink SQL**: Proposed in [FLIP-486](https://cwiki.apache.org/confluence/display/FLINK/FLIP-486%3A+Introduce+A+New+DeltaJoin), planned for Flink 2.1.\\n\\nOnce FLIP-486 is completed, users will be able to achieve Delta Join using the following SQL in conjunction with Fluss\'s Prefix Lookup functionality:\\n\\n```sql\\nCREATE TABLE fluss_left_table (\\n  a1 BIGINT,\\n  b1 BIGINT,\\n  c1 INT,\\n  d1 INT,\\n  PRIMARY KEY (c1,d1,a1) NOT ENFORCED  -- bucket key as a prefix of primary key\\n) WITH (\\n  \'bucket.key\' = \'c1,d1\' -- define bucket key\\n);\\n\\nCREATE TABLE fluss_right_table (\\n  a2 BIGINT,\\n  b2 BIGINT,\\n  c2 INT,\\n  d2 INT,\\n  PRIMARY KEY (c2,d2,a2) NOT ENFORCED  -- bucket key as a prefix of primary key\\n) WITH (\\n  \'bucket.key\' = \'c2,d2\' -- define bucket key\\n);\\n\\n-- it will be optimized to delta join, where the join key is the bucket key of the two tables\\nSELECT * FROM fluss_left_table INNER JOIN fluss_right_table\\n  ON c1 = c2 AND d1 = d2\\n```\\n\\nFlink performs lookups on Fluss tables using the Join Key, which serves as the Bucket Key for the Fluss table.\\nThis allows it to leverage the prefix index of the primary key in the Fluss table, enabling highly efficient lookup queries.\\nThis feature in Fluss is referred to as Prefix Lookup. Currently, Prefix Lookup can also be used to perform one-to-many lookup queries.\\nFor more details, please refer to the [Prefix Lookup](/docs/engine-flink/lookups/#prefix-lookup) documentation.\\n\\n## Stability & Performance Improvements\\n\\nIn this version, we have focused on enhancing the stability and performance of the system, resolving over 50 issues and improvements,\\nand conducting in-depth optimizations on core modules. For example:\\n\\n- Server-side Optimization : By introducing a delayed response mechanism, CPU consumption in low-traffic scenarios has been significantly reduced, thereby improving resource utilization efficiency.\\n- Client-side Optimization : A unified memory management mechanism has been implemented to effectively prevent Out-of-Memory (OOM) issues in high-traffic scenarios while reducing the impact of garbage collection (GC) on system performance.\\n\\nThese improvements have significantly enhanced Fluss\'s reliability and performance in high-concurrency, large-data-volume scenarios, enabling it to handle analytical stream storage workload more efficiently.\\n\\n## Lakehouse Storage\\n\\nIn previous versions, if a table in Fluss needed to enable the Lakehouse storage capability, it had to be enabled when table is created.\\nOtherwise, enabling this feature later would require deleting and recreating the table. This limitation arose because enabling Lakehouse storage changes the key encoding format and bucket sharding strategy, making existing tables incompatible with the new configuration.\\n\\nIn this version, we resolve this by detecting the cluster\'s default data lake format and adopting its key encoding and bucketing strategy,\\nallowing Lakehouse storage to be enabled dynamically after table creation. This eliminates the need for table recreation, improving usability.\\nAdditionally, Paimon dependency is upgraded to version 1.0.1 in this release.\\n\\n## Flink Integration\\n\\nThis version introduces the following enhancements to the Flink connector:\\n\\n1. **Sink Support for Ignoring Retractions:**\\n  Both Primary Key Tables and Log Tables now support the `\'sink.ignore-delete\'` parameter in their Sink implementations. This enables better compatibility with scenarios involving retraction messages, meeting the demands of more complex streaming data processing.\\n2. **Enhanced Partition Table Operations:**\\n  Partitioned tables now support `ALTER TABLE ADD/DROP PARTITION` and `SHOW PARTITIONS` operations, further improving the flexibility and usability of partition management.\\n3. **Sink Interface Upgrade:**\\n  The `SinkFunction` has been upgraded to the `SinkV2` interface, laying the groundwork for full compatibility with Flink 2.0 in the next version. This ensures the system\'s scalability and compatibility in future releases.\\n\\n## Upgrade Notes\\nThe Fluss community try to ensure compatibility during upgrades. However, upgrading from Fluss 0.5 to 0.6 is an incompatible upgrade.\\nStarting with version 0.6, we will officially provide backward compatibility to ensure smoother and more reliable upgrades in future releases.\\nTherefore, version 0.6 is the recommended version for adoption and ecosystem integration.\\n\\n\\n## Future Plan\\n\\nIn the next version, we will focus on the development of the following core features:\\n\\n1. **New Lake-Stream Integrated Architecture:**\\n   A completely new architecture designed for large-scale production environments, featuring plug-in support for mainstream lake formats such as Iceberg and Hudi. This addresses key pain points in Tiering Service performance, scalability, and operational efficiency, providing a more reliable Lakehouse integrated solution for enterprise use cases.\\n\\n2. **Authentication and Authorization:**\\n   Introducing plugin-based authentication and fine-grained access control to meet the stringent data security requirements of enterprises.\\n\\n3. **Kafka Compatibility:**\\n   Compatibility with the Kafka network protocol, enabling seamless integration with the Kafka ecosystem.\\n\\nFor more details about the next release roadmap, please visit the community [discussion page](https://github.com/alibaba/fluss/discussions/556). Your suggestions and contributions are highly welcomed!\\n\\nFluss is under active development. Be sure to stay updated on the project, give it a try and if you like it, don\u2019t forget to give it some \u2764\ufe0f via \u2b50 on [GitHub](https://github.com/alibaba/fluss)\\n\\n## List of Contributors\\n\\nThe Fluss community would like to express gratitude to all the 45 contributors who made this release possible:\\n\\nBenchao Li, ForwardXu, Gang Yang, Georgios Andrianakis, Giannis Polyzos, Hongshun Wang, Jark Wu, Kerwin, Leonard Xu, LiJingwei, Liu Xiao, MehulBatra, Michael Koepf, Nicholas Jiang, Ron, RunningDB, Sagar Sumit, SeungMin, Shuo Cheng, Stan, SteNicholas, Tyrantlucifer, Vipamp, WangS-C, WenjunMin, Wenston Xin, Xiaojian Sun, Yang Guo, Yubin Li, Yuepeng Pan, Zmm, benjobs, gongzhongqiang, gyang94, jon-qj, luoyuxia, moses, psxjoy, wangwj, wudi, xiaozhou, yunhong, yuxia Luo, \u7801\u754c\u63a2\u7d22, \u9053\u541b"},{"id":"unified-streaming-lakehouse","metadata":{"permalink":"/blog/unified-streaming-lakehouse","source":"@site/blog/2025-01-28-towards-stream-lake-arch.md","title":"Towards A Unified Streaming & Lakehouse Architecture","description":"\x3c!--","date":"2025-01-28T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"name":"Luo Yuxia","title":"PMC member of Apache Fluss","url":"https://github.com/luoyuxia","imageURL":"https://github.com/luoyuxia.png","key":"yuxia","page":null}],"frontMatter":{"slug":"unified-streaming-lakehouse","title":"Towards A Unified Streaming & Lakehouse Architecture","sidebar_label":"Toward Streaming Lakehouse","authors":["yuxia"]},"unlisted":false,"prevItem":{"title":"Announcing Fluss 0.6","permalink":"/blog/releases/0.6"},"nextItem":{"title":"Introducing Fluss: Streaming Storage for Real-Time Analytics","permalink":"/blog/fluss-intro"}},"content":"\x3c!--\\n Licensed to the Apache Software Foundation (ASF) under one\\n or more contributor license agreements.  See the NOTICE file\\n distributed with this work for additional information\\n regarding copyright ownership.  The ASF licenses this file\\n to you under the Apache License, Version 2.0 (the\\n \\"License\\"); you may not use this file except in compliance\\n with the License.  You may obtain a copy of the License at\\n\\n      http://www.apache.org/licenses/LICENSE-2.0\\n\\n Unless required by applicable law or agreed to in writing, software\\n distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n See the License for the specific language governing permissions and\\n limitations under the License.\\n--\x3e\\n\\nThe unification of Lakehouse and streaming storage represents a major trend in the future development of modern data lakes and streaming storage systems. Designed specifically for real-time analytics, Fluss has embraced a unified Streaming and Lakehouse architecture from its inception, enabling seamless integration into existing Lakehouse architectures. \\n\\nFluss is designed to address the demands of real-time analytics with the following key capabilities:\\n- **Real-Time Stream Reading and Writing:** Supports millisecond-level end-to-end latency.\\n- **Columnar Stream:** Optimizes storage and query efficiency.\\n- **Streaming Updates:** Enables low-latency updates to data streams.\\n- **Changelog Generation:** Supports changelog generation and consumption.\\n- **Real-Time Lookup Queries:** Facilitates instant lookup queries on primary keys.\\n- **Streaming & Lakehouse Unification:** Seamlessly integrates streaming and lakehouse storage for unified data processing.\\n\\n\x3c!-- truncate --\x3e\\n\\n### Fluss Unified Streaming & Lakehouse Architecture\\n![Unification Solutions](assets/unified_lake_stream_arch/img1.png)\\nThe Fluss architecture is designed to provide millisecond-level end-to-end latency, ensuring high-performance real-time data writing and reading. A core component of this architecture is the **Tiering Service**, which continuously offloads data in Fluss into a standard lakehouse format, such as `Apache Paimon` or `Apache Iceberg`. This tiering ensures that external query engines can directly analyze data in the Lakehouse format, enabling efficient batch and real-time analytics.\\nIn this architecture:\\n- **Latest Data in Fluss:** Fluss stores the most recent, high-fidelity data for real-time analytics. \\n- **Historical Data in Paimon:** Older data is compacted and stored in Apache Paimon for large-scale historical analysis.\\n\\nBy leveraging Apache Flink, the integration of Fluss and Paimon supports Union Reads, which combine real-time data in Fluss with historical data in Paimon. This enables analytical queries with second-level freshness, allowing businesses to benefit from up-to-date insights while maintaining access to extensive historical datasets.\\n\\nThe Streaming/Lakehouse unification design of Fluss provides:\\n- **Unified Architecture:** Simplifies the Lakehouse ecosystem by combining the strengths of streaming and Lakehouse storage. \\n- **Enhanced Real-Time Capabilities:** Millisecond-level latency ensures data is always fresh for critical use cases. \\n- **Seamless Compatibility:** Native support for standard lakehouse formats ensures interoperability with existing analytics engines. \\n- **Optimized Data Management:** Combines real-time and historical data seamlessly for comprehensive analytics workflows.\\n\\nFluss is **a next-generation approach to streaming storage, purpose-built to complement Lakehouse architectures** and drive the adoption of a streaming and Lakehouse unification across industries.\\n\\n### Unified metadata\\n![Unification Solutions](assets/unified_lake_stream_arch/img2.png)\\n\\nIn traditional architectures, streaming storage systems like Kafka and Lakehouse storage solutions like Apache Paimon operated as distinct entities, each maintaining its own metadata. For computing engines such as Apache Flink, this separation presented two significant challenges:\\n1. **Dual Catalogs:** Users were required to create and manage two separate catalogs\u2014one for streaming storage and another for lake storage. \\n2. **Manual Switching:** Accessing data involved manually switching between catalogs to determine whether to query stream storage or lake storage, resulting in operational complexity and inefficiencies.\\n\\n### Unified Access in Fluss\\nIn the Fluss, although Fluss and Paimon still maintain independent metadata, they expose a unified catalog and a single table abstraction to the computing engine, such as Apache Flink. This unified approach offers several key advantages:\\n- **Simplified Data Access:** Users can seamlessly access both Lakehouse storage (Paimon) and streaming storage (Fluss) through a single catalog, eliminating the need to manage or switch between separate catalogs. \\n- **Integrated Querying:** The unified table abstraction allows direct access to real-time data in Fluss and historical data in Paimon. For scenarios requiring both, users can leverage `Union Reads`, combining data from Fluss and Paimon to enable comprehensive analytics with second-level data freshness. \\n- **Operational Efficiency:** By presenting a cohesive interface, the architecture reduces operational complexity, making it easier for users to work with real-time and historical data within a single workflow.\\n\\nThis unified approach streamlines the interaction between computing engines and storage layers, enhancing both usability and productivity while supporting the high-performance demands of modern analytics workflows.\\n\\n###  Alignment of Data Distribution\\n![Unification Solutions](assets/unified_lake_stream_arch/img3.png)\\n\\nIn the Fluss, the data distribution between Fluss and Lakehouse storage (e.g., Apache Paimon) is strictly aligned. Fluss supports partitioned tables and buckets, and its bucketing algorithm is fully consistent with Paimon\u2019s. This ensures that a given piece of data is always allocated to the same bucket in both systems, creating a one-to-one correspondence between Fluss buckets and Paimon buckets.\\nThis strong consistency in data distribution provides two significant benefits:\\n\\n#### 1. Elimination of Shuffle Overhead During Tiering\\nWhen tiering data from Fluss into Paimon format:\\n- A Fluss bucket (e.g., bucket1) can be tiered directly into the corresponding Paimon bucket (bucket1). \\n- There is no need to read data from a Fluss bucket, calculate which Paimon bucket each piece of data belongs to, and then write it to the appropriate Paimon bucket.\\n\\nBy bypassing this intermediate redistribution step, the architecture avoids costly shuffle overhead, significantly improving compaction efficiency.\\n\\n#### 2. Prevention of Data Inconsistencies\\nData consistency is maintained through the use of an identical bucketing algorithm in both Fluss and Paimon. This algorithm calculates the bucket assignment for each piece of data as follows:\\n> bucket_id = hash(row) % bucket_num\\n\\nBy employing the same hash function and algorithm, Fluss and Paimon ensure consistent bucket assignment. If differing algorithms were used, inconsistencies would arise. For example:\\n- For a primary key table, a data row a might be assigned to bucket1 in Fluss but to bucket2 in Paimon. \\n- During tiering, if rows were mistakenly placed in bucket1 in Paimon (per Fluss\u2019s assignment), users would fail to locate the data in Paimon due to the mismatch.\\n\\nBy maintaining strong alignment in data distribution, the architecture eliminates this risk, ensuring data consistency across Fluss and Paimon while simplifying compaction workflows. This alignment underscores the robustness and efficiency of the Fluss unification design.\\n### Streaming Reading: More Efficient Data Tracing\\n\\nIn the Fluss, historical data resides in Lakehouse storage, while real-time data is maintained in Fluss. During streaming reads, this architecture enables a seamless combination of historical and real-time data access:\\n\\n- **Historical Data Access:** Fluss retrieves historical data directly from the Lakehouse storage, leveraging its inherent advantages, such as:\\n   - **Efficient Filter Pushdown:** Enables query engines to apply filtering conditions at the storage layer, reducing the amount of data read and improving performance. \\n   - **Column Pruning:** Allows retrieval of only the necessary columns, optimizing data transfer and query efficiency. \\n   - **High Compression Ratios:** Minimizes storage overhead while maintaining fast retrieval speeds.\\n- **Real-Time Data Access:** Fluss concurrently reads the latest real-time data from its own storage, ensuring up-to-the-millisecond freshness.\\n\\nBy combining the strengths of Lakehouse storage for efficient historical data retrieval and Fluss for real-time streaming data, this architecture delivers a highly performant and scalable solution for streaming read scenarios. This integration ensures that users benefit from low-latency data freshness and optimized query performance across both historical and real-time datasets.\\n### Batch Reading: Data Freshness in Seconds\\n![Unification Solutions](assets/unified_lake_stream_arch/img4.png)\\n\\nHistorical data is stored in the Lakehouse, and real-time data is stored in Fluss. In batch reading scenarios, computing engines (such as Flink) can perform union reading of data in Fluss and lake storage to achieve analysis of data freshness in seconds.\\n\\n### Apache Flink and Fluss\\n![Unification Solutions](assets/unified_lake_stream_arch/img5.png)\\nFluss exposes a unified API to Flink users, allowing them to choose whether to use union reads or read-only reads on the Lakehouse, using the following SQL:\\n```sql\\nSELECT * FROM orders\\n```\\nThis reads the complete data of the orders table and Flink will union read the data in Fluss and the Lakehouse.\\nIf the user only needs to read data on the data lake, you can add the `$lake` suffix after the table to be read. The SQL is as follows\\n```sql\\n\\n-- analytical queries\\nSELECT COUNT(*), MAX(t), SUM(amount) \\nFROM orders$lake\\n\\n-- query system tables\\nSELECT * FROM orders$lake$snapshots\\n```\\nFor scenarios where data on a data lake is read-only, Fluss inherits all the optimizations and capabilities of the lake format as a Flink source, such as runtime filters, system table queries, and time travel.\\n\\n## Benefits Of The Unified Architecture\\nNext, using Apache Paimon as an example, we will illustrate the advantages of using Fluss to build a unified architecture. We will highlight how Fluss enhances the capabilities of Paimon, creating a unified solution that combines the strengths of both systems for efficient real-time and historical data processing.\\n\\n### Second-level Timeliness\\n\\nIn Apache Paimon, data visibility is traditionally determined by the Flink checkpoint interval, which typically operates at one minute-level granularity. However, by integrating Fluss with Paimon to build unfiied architecture, this dependency is eliminated.\\n\\nWith this unification, data becomes visible immediately upon entering Fluss, significantly improving data timeliness to second-level latency. This enhancement ensures that real-time insights can be derived more quickly, meeting the demands of time-sensitive applications while maintaining seamless access to both historical and real-time data.\\n\\n### Consistent Data Freshness Across All Layers\\n![Unification Solutions](assets/unified_lake_stream_arch/img6.png)\\nIn the process of building a data warehouse, it is common to organize and manage data by layering, such as Bronze, Silver and Gold following the medallion architecture. As data flows through these layers, maintaining data freshness becomes a critical consideration.\\n\\nWhen Paimon is used as the sole storage solution for each layer, data visibility depends on the Flink checkpoint interval. This introduces cumulative delays:\\n- The changelog for a given layer becomes visible only after the completion of a Flink checkpoint. \\n- As this changelog propagates to subsequent layers, the data freshness delay increases with each checkpoint interval.\\n\\nFor example, with a Flink checkpoint interval of 1 minute:\\n- The Bronze layer experiences 1-minute delay. \\n- The Silver layer adds another 1-minute delay, totaling 2 minutes. \\n- The Gold layer adds yet another 1-minute delay, resulting in a cumulative 3-minute delay.\\n\\nWith Fluss and Paimon though we get:\\n- **Immediate Data Visibility:** Data in Fluss becomes visible immediately upon ingestion, without waiting for a Flink checkpoint to complete. The changelog is instantly transferred to the next layer. \\n- **Consistent Data Freshness:** The data freshness across all layers is consistent and measured in seconds, eliminating cumulative delays.\\n\\nAdditionally, if the Fluss Tiering Service is configured with a tiering cycle of 1 minute, the data delay for Paimon storage at each layer is limited to 1 minute, regardless of the number of layers. This ensures:\\n- **Real-Time Data Processing:** Layers can propagate and process data with minimal delay. \\n- **Optimized Streaming & Lakehouse Unification:** The architecture balances the strengths of real-time and batch-oriented storage.\\n\\nThis integration significantly improves the performance and usability of layered data warehouses, enabling faster insights and better responsiveness to real-time demands.\\n\\n### More efficient and higher throughput changelog generation\\nIn Apache Paimon, there are currently two commonly used methods for generating changelogs (excluding the Input Producer, which requires more stringent data source requirements and is not considered here):\\n#### Lookup Changelog Producer\\n   - **Advantages:** Offers high timeliness for generating changelogs. \\n   - **Challenges:** Consumes significant computational resources, leading to higher operational costs. \\n#### Full Compaction Producer:\\n   - **Advantages:** Does not require additional resource consumption, as changelogs are generated during the Full Compaction process. \\n   - **Challenges:** Suffers from poor timeliness, as changelog generation is delayed until a Full Compaction is triggered, which typically occurs after several checkpoints.\\n   \\n#### Fluss and Paimon: Optimizing Changelog Generation\\nThe Fluss and Paimon architecture strikes a balance between timeliness and performance in changelog generation:\\n- **Changelog Timeliness:** Fluss generates changelogs in seconds, ensuring near real-time visibility of data changes. \\n- **Efficient Conversion:** The Fluss Tiering Service efficiently writes Fluss changelogs directly into the Paimon changelog format. This conversion process is highly optimized, avoiding resource-intensive operations such as lookups. Instead, it relies on direct read-and-write operations, significantly reducing overhead.\\n\\n#### Key Benefits of Fluss and Paimon Changelog Generation\\n1. **Improved Timeliness:** Near real-time changelog generation ensures data freshness and faster insights. \\n2. **Resource Efficiency:** By eliminating the need for computationally expensive operations, the architecture reduces resource consumption while maintaining high performance. \\n3. **Seamless Integration:** The direct compatibility between Fluss changelogs and Paimon changelog formats simplifies the process, enhancing system efficiency and reducing operational complexity.\\n\\nThis architecture provides an elegant solution for use cases that demand both low-latency data updates and optimized resource utilization, making it a robust choice for modern data processing needs.\\n\\n### Enabling Multi-Writer Support for Paimon Partial Updates\\nPartial updates are a critical feature in Apache Paimon, particularly for managing large, wide tables. However, in the current Paimon architecture, performing partial updates on wide tables presents significant challenges:\\n- **Single Writer Limitation:** To ensure consistency, all partial updates to a table must be consolidated into a single SQL job. This requires using a UNION statement to combine all partial update operations into a single pipeline, ensuring only one writer is responsible for updating the table.\\n- **Operational Complexity:** Consolidating all updates into one job makes it difficult to manage and tune individual update operations, leading to challenges in scalability and flexibility.\\n\\nWith Fluss and Paimon integration these limitations are eliminated\\n\\n![Unification Solutions](assets/unified_lake_stream_arch/img7.png)\\n\\n- **Intermediate Synchronization via Fluss:** All updates pass through Fluss, which serves as an intermediary layer to synchronize changes with Paimon. \\n- **Support for Concurrent Updates:** Fluss enables concurrent updates from multiple SQL jobs, removing the need to consolidate updates into a single job. \\n- **Fine-Grained Job Management:** With the ability to execute multiple independent SQL jobs for updating any number of columns in a wide table, users can perform job-level tuning and management, improving operational efficiency and flexibility.\\n\\nThis enables organizations to unlock several key advantages:\\n- **High Real-Time Performance:** Lakehouse storage achieves second-level data freshness, meeting the demands of real-time data applications. \\n- **Unified Streaming and Lakehouse Processing:** Data is written once into the architecture and can seamlessly support both batch and streaming use cases, reducing duplication of effort. \\n- **Lower Operational Costs:** Simplified maintenance and reduced complexity of partial updates, lower storage costs by minimizing data duplication and reduced computational costs by eliminating redundant processing pipelines.\\n\\nThe Fluss and Paimon architecture offers a robust solution for managing partial updates in wide tables while delivering significant improvements in performance, scalability, and operational efficiency.\\n\\nAll the above highlight the power of a unified streaming and Lakehouse architecture in modern data systems, ensuring real-time capabilities with streamlined workflows.\\n\\n## Future plans\\nThe Fluss community is actively working to enhance the streaming and Lakehouse unification capabilities, focusing on the following key areas.\\n### Expanding Union Read Ecosystem\\nCurrently, Union Read functionality is integrated with Apache Flink, enabling seamless querying of real-time and historical data. Moving forward, the community plans to extend this capability to support additional query engines, such as Apache Spark and StarRocks, further broadening its ecosystem compatibility and adoption.\\n\\n### Diversifying Lake Storage Formats\\nAt present, Fluss supports Apache Paimon as its primary lake storage. To meet diverse user requirements, the community aims to add support for more lake formats, including **Apache Iceberg** and **Apache Hudi**, thereby providing flexibility and interoperability with a wider range of Lakehouse ecosystems.\\n\\n### Optimizing Arrow-to-Parquet Conversion\\nFluss leverages Apache Arrow as its storage format, while many Lakehouse formats, such as Paimon, Iceberg, and Hudi, utilize parquet for storage. The Apache Arrow community has developed a mature and efficient solution for converting Arrow data to Parquet. In the future, Fluss will integrate these advancements to enable high-performance Arrow-to-Parquet conversions, significantly reducing the computational overhead of the tiering service and enhancing overall efficiency."},{"id":"fluss-intro","metadata":{"permalink":"/blog/fluss-intro","source":"@site/blog/2024-12-12-fluss-intro.md","title":"Introducing Fluss: Streaming Storage for Real-Time Analytics","description":"\x3c!--","date":"2024-12-12T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"name":"Jark Wu","title":"PMC member of Apache Fluss","url":"https://github.com/wuchong","imageURL":"https://github.com/wuchong.png","key":"jark","page":null}],"frontMatter":{"slug":"fluss-intro","title":"Introducing Fluss: Streaming Storage for Real-Time Analytics","sidebar_label":"Introducing Fluss","authors":["jark"]},"unlisted":false,"prevItem":{"title":"Towards A Unified Streaming & Lakehouse Architecture","permalink":"/blog/unified-streaming-lakehouse"},"nextItem":{"title":"Why Fluss? Top 4 Challenges of Using Kafka for Real-Time Analytics","permalink":"/blog/why-fluss"}},"content":"\x3c!--\\n Licensed to the Apache Software Foundation (ASF) under one\\n or more contributor license agreements.  See the NOTICE file\\n distributed with this work for additional information\\n regarding copyright ownership.  The ASF licenses this file\\n to you under the Apache License, Version 2.0 (the\\n \\"License\\"); you may not use this file except in compliance\\n with the License.  You may obtain a copy of the License at\\n\\n      http://www.apache.org/licenses/LICENSE-2.0\\n\\n Unless required by applicable law or agreed to in writing, software\\n distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n See the License for the specific language governing permissions and\\n limitations under the License.\\n--\x3e\\n\\nWe have discussed the challenges of using Kafka for real-time analytics in our previous [blog post](/blog/why-fluss/).\\nToday, we are excited to introduce Fluss, a cutting-edge streaming storage system designed to power real-time analytics.\\nWe are going to explore Fluss\'s architecture, design principles, key features, and how it addresses the challenges of using Kafka for real-time analytics.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Key Features of Fluss\\n\\n### Columnar Stream\\n![FF Announcement](assets/fluss_intro/img1.jpg)\\nFluss is, as anticipated, a columnar-based streaming storage, leveraging the [Apache Arrow IPC Streaming Format](https://arrow.apache.org/docs/python/ipc.html) for its underlying file storage.\\nThis enables Fluss to achieve highly efficient column pruning while maintaining millisecond-level streaming reads and writes capabilities.\\n\\nThe performance benefits of Fluss are evident in its benchmarks against Kafka. In the comparison, the horizontal axis represents the number of columns read, while the vertical axis shows the read throughput. The results clearly demonstrate that Fluss\'s read performance scales proportionally with the reduction in columns read. For example, when the number of columns is reduced by 90%, Fluss achieves a 10x increase in read throughput.\\n\\nA key advantage of Fluss is that column pruning is performed server-side and only the needed column data will be transferred to client-side. This architectural design not only enhances performance but also reduces network costs and resource consumption, making Fluss a highly efficient solution for real-time streaming analytics.\\n\\n### Real-Time Updates and Changelog\\n![FF Announcement](assets/fluss_intro/img2.jpg)\\nReal-time updates and Changelog are very important features required by streaming analytics and Flink.\\nAt its core, Fluss streaming storage is built on a Log Tablet, with a key-value (KV) index constructed over the Log.\\nThe relationship between the Log and KV mirrors the concept of stream-table duality:\\nupdates to the KV generate a changelog that is written to the Log. In the event of a failure, data from the Log is used to recover the KV.\\n\\nThe KV index is implemented as a Log-Structured Merge (LSM) tree to support large-scale real-time updates,\\nand also support partial-update which can be used to build wide tables very efficiently. In addition, the changelog\\ngenerated by KV can be read directly by Flink without additional deduplication cost, which saves a lot of computing resources.\\n\\n\\n### Queryable\\n![FF Announcement](assets/fluss_intro/img3.jpg)\\nThe built-in KV indexes enable high-performance primary key lookups, making Fluss suitable for real-time processing tasks such as dimension table joins. Users can also perform direct data exploration with Fluss, including queries with operations like LIMIT and COUNT, to make debugging data in Fluss very easy.\\n\\n### Unification of Stream and Lakehouse\\n\\n![FF Announcement](assets/fluss_intro/img4.jpg)\\nOne of Fluss\'s standout features is the unification of stream and Lakehouse. In traditional Lambda architecture, you have to duplicate data in both real-time layer and batch layer. While Fluss eliminates this redundancy by unifying \\"data stored in stream\\" and \\"data stored in lake\\". This unification ensures consistent data and metadata while reducing storage costs and simplifying data workflows.\\n\\nAt its core, Fluss incorporates a `compaction service` that ensures seamless integration between stream and lake storage. This service automatically and continuously converts Fluss data into the data lake format.\\nA key feature here is called \\"Shared Data\\". The Lakehouse storage serves as the historical data layer for the streaming storage, which is optimized for storing long-term data with minute-level latencies. On the other hand, streaming storage serves as the real-time data layer for Lakehouse storage, which is optimized for storing short-term data with millisecond-level latencies.\\nThe data is shared with each other, and is exposed as a single table.\\nFor streaming queries on the table, it firstly uses the Lakehouse storage as historical data to have efficient catch-up read performance, and then seamlessly transitions to the streaming storage for real-time data, ensuring no duplicate data is read.\\nFor batch queries on the table, streaming storage supplements real-time data for Lakehouse storage, enabling second-level freshness for Lakehouse analytics.\\nThis capability, termed **Union Read**, allows both layers to work in tandem for highly efficient and accurate data access.\\n\\nIn addition, the automatically converted data lake tables fully adhere to open table format protocols, ensuring compatibility with existing query engines such as Apache Spark, StarRocks, and Trino.\\nThese engines can directly query data on Lakehouse storage, seamlessly integrating it into users\' existing Lakehouse architectures.\\n\\nFluss has already completed integration with Apache Paimon, and integration with Apache Iceberg is underway. This commitment to compatibility ensures that Fluss remains a flexible and powerful component in the modern data stack, bridging real-time and historical data for unified analytics and storage efficiency.\\n\\n## Overall Architecture\\n![FF Announcement](assets/why_fluss/img9.jpg)\\nThis is the overall architecture of Fluss, a cutting-edge streaming storage solution designed specifically for real-time analytics. Fluss operates a server cluster to provide high-performance real-time read and write capabilities, while leveraging remote storage for data tiering to optimize storage costs. Additionally, Fluss integrates seamlessly with Lakehouse architectures, enabling robust query capabilities and a unified data ecosystem.\\n\\nThe core features of Fluss include real-time streaming reads and writes, column pruning, streaming updates, changelog subscription, real-time lookup queries, and integration of stream and Lakehouse.\\n\\nWith its robust architecture and comprehensive feature set, Fluss empowers organizations to bridge the gap between real-time and historical data, enabling efficient, scalable, and cost-effective solutions for real-time analytics.\\n\\n## Use Case: Delta Join\\n![FF Announcement](assets/fluss_intro/img5.jpg)\\nThe combination of Fluss\u2019s key features lends itself to a highly effective use case: Delta Join. Apache Flink provides a foundational feature called Stream-Stream Join, widely used for building wide tables. However, it is often one of the most challenging operations in Flink.\\nStream-stream joins require maintaining the full upstream data in the state, leading to significant resource consumption. For instance, one of Taobao\'s largest Flink jobs is a stream-stream join (exposing associated orders), which needs to maintains a state of over 50 TB. This brings many challenges, such as high cost, unstable job, checkpoint timeout, slow restart recovery, etc.\\n\\nTo address these challenges, we developed a new Flink join operator implementation called Delta Join, leveraging Fluss\u2019s streaming read and secondary-index lookup capabilities. Delta Join operates like a \\"bilateral driven lookup join\\", that is:\\nwhen data arrives from the left stream, the right table is queried using the join key;\\nwhen data arrives from the right stream, the left table is queried using the join key.\\nThis approach eliminates state like Lookup Join, while preserving the semantics of a Stream-Stream Join (any updates on either side triggers an update to the join result).\\n\\nWe tested Taobao\'s largest stream-stream join job and migrated to delta join. The results were impressive:\\n- **Eliminate State:** Migrating from stream-stream join to delta join eliminated the need for `50 TB` join state, leading to enhanced job stability and avoid checkpoint timeouts.\\n- **Resource Optimization:** Flink resource cost decreased by 10x, dropping from `2300 CU` to `200 CU`, while maintaining the same throughput.\\n- **Faster Backfilling:** By leveraging Fluss\u2019s Lakehouse integration, we can use the converted Paimon/Iceberg table and Sort-Merge Join in batch mode for data backfilling. Re-processing one day data was reduced from `4 hours` to `30 minutes`.\\n\\nBesides the numbers, Delta Join\'s most significant benefit lies in its user flexibility.\\nUnlike traditional stream-stream joins, where the state is tightly coupled with Flink job, operating as an opaque \'black box\', any job modification requires a costly and time-consuming state rebuild.\\nDelta Join addresses this by decoupling the state from the job, facilitating effortless job modifications without the need to reconstruct the state.\\nThis separation not only boosts backfilling efficiency but also makes state data easily accessible in Fluss for analysis, thereby enhancing business agility and increasing developer productivity.\\n\\nTo formalize this innovation, we submitted the Delta Join [FLIP-486 proposal](https://cwiki.apache.org/confluence/display/FLINK/FLIP-486%3A+Introduce+A+New+DeltaJoin) to the Apache Flink community. We invite those interested to review and contribute to this exciting advancement.\\n\\nDelta Join, powered by Fluss, represents a breakthrough in real-time analytics by significantly reducing resource consumption, enhancing performance, and unlocking flexibility in stateful stream processing.\\n\\n## Future Plan\\n\\n![FF Announcement](assets/fluss_intro/img6.jpg)\\nThe future planning for Fluss involves three key aspects, each corresponding to its relationship with three open-source software projects:\\n\\n- **Apache Kafka Protocol Compatibility:** This is aimed at helping existing streaming data migrate to Fluss more effectively.\\n- **Storage for Apache Flink:** Fluss aims to be the best storage for Apache Flink and streaming analytics, offering deep optimization with Flink across the storage, optimizer and execution layer. Delta Join marks our first major step, with many more exciting features on the way.\\n- **Real-Time Data Layer for Apache Iceberg:** By unifying stream and Lakehouse, Fluss is committed to providing a robust real-time data layer for Apache Iceberg and Apache Paimon. This vision includes creating a unified storage solution that supports both real-time and offline analytics.\\n\\nThere are more exciting work happening in the community. You can check out [Fluss Roadmap](/roadmap/) for a comprehensive future planning."},{"id":"why-fluss","metadata":{"permalink":"/blog/why-fluss","source":"@site/blog/2024-12-11-why-fluss.md","title":"Why Fluss? Top 4 Challenges of Using Kafka for Real-Time Analytics","description":"\x3c!--","date":"2024-12-11T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"name":"Jark Wu","title":"PMC member of Apache Fluss","url":"https://github.com/wuchong","imageURL":"https://github.com/wuchong.png","key":"jark","page":null}],"frontMatter":{"slug":"why-fluss","title":"Why Fluss? Top 4 Challenges of Using Kafka for Real-Time Analytics","sidebar_label":"Why Fluss? Top 4 Challenges of Kafka","authors":["jark"]},"unlisted":false,"prevItem":{"title":"Introducing Fluss: Streaming Storage for Real-Time Analytics","permalink":"/blog/fluss-intro"},"nextItem":{"title":"Fluss is Now Open Source","permalink":"/blog/fluss-open-source"}},"content":"\x3c!--\\n Licensed to the Apache Software Foundation (ASF) under one\\n or more contributor license agreements.  See the NOTICE file\\n distributed with this work for additional information\\n regarding copyright ownership.  The ASF licenses this file\\n to you under the Apache License, Version 2.0 (the\\n \\"License\\"); you may not use this file except in compliance\\n with the License.  You may obtain a copy of the License at\\n\\n      http://www.apache.org/licenses/LICENSE-2.0\\n\\n Unless required by applicable law or agreed to in writing, software\\n distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n See the License for the specific language governing permissions and\\n limitations under the License.\\n--\x3e\\n\\nThe industry is undergoing a clear and significant shift as big data computing transitions from offline to real-time processing.\\nThis transition is revolutionizing various sectors, including the E-commerce, automotive networking, finance, and beyond,\\nwhere real-time data applications are becoming integral to operations. This evolution enables organizations to unlock greater\\nvalue by leveraging real-time insights to drive business impact and enhance decision-making.\\n\\n\x3c!-- truncate --\x3e\\n\\n![FF Announcement](assets/why_fluss/img1.jpg)\\nThe evolution of big data technology is becoming increasingly impactful, reshaping the computing architecture landscape.\\nThe traditional Hive-based data warehouse has given way to modern architectures, starting with Lakehouse models and progressing\\nto the Paimon streaming Lakehouse, which has gained significant traction in markets like China and more recently started expanding to EU/US markets.\\nThe core driver behind these architectural innovations is the need to improve data processing timeliness. The data freshness is improved from traditional\\nT+1 (next-day readiness) to T+1 hours, and now T+1 minutes. However, Lakehouse architectures, being file-system-based,\\ninherently face minute-level latency as their practical upper limit.\\n\\nYet, many critical use cases, such as search and recommendation systems, advertisement attribution, and anomaly detection, demand second-level latency.\\nWhile big data technologies have advanced significantly, there remains a notable gap: a user-friendly, second-level storage solution tailored for big data analytics.\\n\\nIn most real-time data scenarios, Apache Kafka has emerged as the go-to second-level storage solution. Its integration with Apache Flink represents the dominant architecture for building real-time data warehouses. However, while widely adopted, this combination presents significant challenges for achieving true real-time analytics at scale. The limitations of using Kafka for big data analytics highlight the need for a more robust solution to meet the demands of modern real-time use cases.\\n\\n## Kafka Falls Short in Real-Time Analytics\\n\\n### No Support for Updates\\n\\nThe first significant challenge with Apache Kafka is its lack of support for updates, a critical feature for data warehouses. In data warehousing, updates are often essential to correct or amend data. However, Kafka\'s inability to handle updates results in duplicate records for the same primary key. When consumed by a computing engine, this duplication necessitates costly deduplication processes to ensure accurate results.\\n\\n![FF Announcement](assets/why_fluss/img2.jpg)\\n\\nIn Apache Flink, for instance, handling this issue requires materializing all upstream data in state, which is resource-intensive. Every time data is consumed from Kafka, the deduplication process incurs a substantial overhead, significantly increasing computational and storage costs. This limitation not only impacts performance but also hampers the reusability of Kafka-stored data for downstream business processes.\\n\\n### Lack of Querying Capabilities\\n\\nThe second major limitation of Apache Kafka is hard to debug, primarily due to its lack of native querying capabilities. In data warehousing, querying is a fundamental feature that facilitates troubleshooting and ad-hoc analysis to understand data trends. Unfortunately, Kafka operates much like a black box, making it challenging to perform these critical tasks without additional tools or layers.\\n\\n![FF Announcement](assets/why_fluss/img3.jpg)\\n\\nTo address this limitation, the industry has adopted two primary approaches, each with its own trade-offs:\\n\\n- **Synchronizing Kafka Data to an OLAP System:** This allows for querying data using the OLAP system\u2019s capabilities. However, this approach introduces additional components into the architecture, increasing both complexity and cost. Moreover, it risks data inconsistencies due to synchronization delays. \\n- **Querying Kafka Directly Using Trino:** While Trino can query Kafka, it relies solely on full scans, which are inefficient for large-scale operations. For instance, a simple query on just 1GB of Kafka data can take up to one minute, rendering it impractical for large datasets or real-time requirements.\\n\\nThese limitations make Kafka unsuitable for efficient and scalable data exploration in modern data warehousing workflows.\\n\\n### Difficulty with Data Backfilling\\n\\n![FF Announcement](assets/why_fluss/img4.jpg)\\nThe third significant issue with Apache Kafka is processing historical data, also known as data backfilling, a common requirement in data warehousing.\\nFor instance, in logistics, it usually needs to process and analyze historical data from several months ago. However, Kafka only retain data for a few days due to the high cost of storage.\\nEven though Kafka community has introduced [Tiered Storage](https://cwiki.apache.org/confluence/display/KAFKA/KIP-405%3A+Kafka+Tiered+Storage) to address the long-term data issue, it still has limitations.\\nReading historical data still require all the data to pass through Kafka brokers, which can lead to brokers unstable and [disrupt live traffics](https://www.warpstream.com/blog/tiered-storage-wont-fix-kafka#increased-complexity-and-operational-burden).\\n\\nThese limitations highlight Kafka\u2019s inefficiency as a solution for data backfilling in large-scale and long-term analytical use cases, further underscoring the need for more robust alternatives in real-time data warehousing.\\n\\n### Excessive Network Costs\\nThe final major challenge with Apache Kafka lies in its high network costs, which account for an estimated 88% of Kafka\'s overall operational expenses. In data warehousing, the \\"one write, multiple read\\" pattern is common, with each consumer often requiring only a subset of the data. However, Kafka\'s design mandates that consumers read the entire dataset, regardless of how much is actually needed.\\n\\n![FF Announcement](assets/why_fluss/img5.jpg)\\n\\nFor instance, among the tens of thousands of Flink SQL jobs at Alibaba, only 49% of the upstream columns are utilized per job on average. Despite this, consumers must read all the columns and pay 100% networking cost, which is highly inefficient and wasteful.\\n\\nIn summary, using Kafka for real-time analytics presents several critical issues: **(1) lack of support for updates**, **(2) absence of querying capabilities**, **(3) difficulty with data backfilling**, and **(4) excessive network costs**. These limitations make the combination of Flink and Kafka less than ideal for real-time data warehousing.\\nBut why Kafka lacks these abilities? Is it possible to add the abilities to Kafka?\\n\\n## Kafka is not designed for Analytics\\n\\n![FF Announcement](assets/why_fluss/img6.jpg)\\nThe root cause of these challenges lies in the fundamental design philosophy of Kafka: **Kafka is designed for streaming events, NOT for streaming analytics**. Each system has its own focus and strengths, and Kafka\'s design is optimized for message queue scenarios rather than analytical workloads.\\n\\nIn Kafka, data is typically stored in a row-oriented format, such as CSV, JSON, or Avro.\\nWhile this is highly efficient for use cases involving message streaming, it becomes a bottleneck for analytical scenarios.\\nData analysis requires handling large volumes of data, and therefore heavily relies on data skipping capabilities from storage, such as column pruning and predicate pushdown.\\nFor this reason, columnar storage is far better suited for analytical workloads, whereas Kafka\'s row-based storage is not designed to meet these demands effectively.\\n\\n## Introducing Fluss\\n\\n![FF Announcement](assets/why_fluss/img7.jpg)\\nWhen visualizing the data ecosystem as a four-quadrant matrix, an intriguing pattern emerges.\\nThe left side of the matrix represents operational systems, while the right side represents analytical systems.\\nThe top half is dedicated to stream storage, and the bottom half to table storage.\\nIn operational systems, both databases and stream storage predominantly use row-based formats, as row storage is more efficient for transactional workloads. Conversely, in analytical systems like Apache Iceberg and Snowflake, columnar storage is preferred due to its superior performance in analytical scenarios.\\n\\nInterestingly, the upper-right quadrant of the matrix remains empty, indicating a significant gap in the market: a streaming storage for analytical scenarios. Unsurprisingly, such a storage would likely adopt a columnar format to effectively address the needs of real-time analytics.\\n\\nTo fill the gap and resolve problems in Flink and streaming analytics, we embarked on a journey two years ago to create a streaming storage. We named the project \\"**FL**ink **U**nified **S**treaming **S**torage\\", and from its initials, we derived the name **Fluss**.\\n![FF Announcement](assets/why_fluss/img8.jpg)\\nInterestingly, Flink is derived from the German word for \\"**agile**\\", and Fluss, which translates to \\"**river**\\" in German, resonates deeply with the project\u2019s vision.\\nIt symbolizes the streaming data is continuously flowing, distributing and converging into data lakes, just like a river.\\nThe launch of Fluss coincides with Flink\'s 10th anniversary, making it a fitting tribute to the project\u2019s heritage and its enduring contribution to the streaming data landscape.\\n\\n![FF Announcement](assets/why_fluss/img9.jpg)\\n\\nLearn more details about Fluss in the next [blog post](/blog/fluss-intro/), where we will delve deeper into Fluss\'s architecture, design principles, key features, and explore how it addresses the challenges of using Kafka for real-time analytics."},{"id":"fluss-open-source","metadata":{"permalink":"/blog/fluss-open-source","source":"@site/blog/2024-11-29-fluss-open-source.md","title":"Fluss is Now Open Source","description":"\x3c!--","date":"2024-11-29T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"name":"Jark Wu","title":"PMC member of Apache Fluss","url":"https://github.com/wuchong","imageURL":"https://github.com/wuchong.png","key":"jark","page":null},{"name":"Giannis Polyzos","title":"PMC member of Apache Fluss","url":"https://github.com/polyzos","imageURL":"https://github.com/polyzos.png","key":"giannis","page":null}],"frontMatter":{"slug":"fluss-open-source","title":"Fluss is Now Open Source","authors":["jark","giannis"]},"unlisted":false,"prevItem":{"title":"Why Fluss? Top 4 Challenges of Using Kafka for Real-Time Analytics","permalink":"/blog/why-fluss"}},"content":"\x3c!--\\n Licensed to the Apache Software Foundation (ASF) under one\\n or more contributor license agreements.  See the NOTICE file\\n distributed with this work for additional information\\n regarding copyright ownership.  The ASF licenses this file\\n to you under the Apache License, Version 2.0 (the\\n \\"License\\"); you may not use this file except in compliance\\n with the License.  You may obtain a copy of the License at\\n\\n      http://www.apache.org/licenses/LICENSE-2.0\\n\\n Unless required by applicable law or agreed to in writing, software\\n distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n See the License for the specific language governing permissions and\\n limitations under the License.\\n--\x3e\\n\\nEarlier this year at Flink Forward 2024 Berlin we [announced Fluss](https://www.ververica.com/blog/introducing-fluss) and today we are thrilled to announce open-sourcing the project.\\nFluss is a **streaming storage system** designed to power real-time analytics. It aspires to change how organizations approach real-time data by acting as the **real-time data layer** for the Lakehouse. \\nIts cutting-edge design enables businesses to achieve **sub-second latency**, **high throughput**, and **cost efficiency** for data analytics, making it the ideal solution for modern data-driven applications.\\n\\nWe have historically invested a lot of effort into advancing the data streaming ecosystem, being major contributors to [Apache Flink\xae](https://flink.apache.org/), [Apache Flink CDC](https://www.ververica.com/blog/ververica-donates-flink-cdc-empowering-real-time-data-integration-for-the-community), and [Apache Paimon](https://paimon.apache.org/).\\nAs part of our commitment, Fluss is now open source under the Apache 2.0 license and is available on [GitHub](https://github.com/alibaba/fluss), inviting users to create the next generation of real-time architectures.\\n\\n\\n![FF Announcement](assets/fluss_announcement/ff_os.png)\\n\\n\x3c!-- truncate --\x3e\\n\\n### Real-Time Streaming Storage for the Lakehouse Era\\nThe need for real-time insights has grown exponentially - especially with the recent explosion of AI. Still, the tools and architectures we\u2019ve relied on for years weren\u2019t designed with streaming-first analytical workflows in mind. Traditional architectures often involve complex integrations between message queues like Kafka, processing engines like Flink, and storage systems that are more batch-oriented than real-time. This approach not only increases latency but also adds operational overhead and cost. Fluss offers a **unified streaming storage layer** purpose-built for **real-time analytics**.\\n\\nAt its core, Fluss combines the best of **streaming** and **analytical storage**. Its **columnar stream** design is optimized for real-time analytical queries, enabling lightning-fast data access and updates. With support for **real-time data ingestion** and integrations with **real-time lakehouse solutions** such as Paimon, Fluss ensures that data is always fresh and ready for analysis, making it ideal for applications where latency is critical.\\n\\nA critical aspect of Fluss is also its deep integration with Apache Flink\u24c7 the de facto gold standard for stream processing. This integration combines a **stream processor** and a **real-time storage layer**, eliminating the need for separate message queues like Kafka in analytics-focused architectures. Fluss simplifies pipelines, reduces costs, and improves performance for **high-throughput**, **low-latency analytics**.\\n\\n### Building the Future of Analytics with Fluss\\nFluss is designed to support a wide range of use cases, from powering dashboards and monitoring systems to enabling **streaming ETL** and **real-time intelligence pipelines** for the modern AI era. Its ability to provide real-time updates makes it a natural fit for **streaming data warehouses**, where fresh data is essential for decision-making.\\n\\nBy serving as the real-time data layer on the Lakehouse, Fluss supports both **streaming-first** architectures and **unified batch and stream processing**. This flexibility should be appealing for organizations looking to modernize their analytics stack while keeping costs low and performance high.\\n\\n### What\u2019s Next for Fluss?\\nOpen-sourcing Fluss is just the beginning and it will be donated to the Apache Software Foundation (ASF). We\u2019re committed to working closely with the community to expand its capabilities and adoption. You can find more information about the project\u2019s roadmap [here](https://alibaba.github.io/fluss-docs/roadmap/).\\n\\n![Banner](assets/fluss_announcement/banner.png)\\n\\n### Join the Journey\\nWe invite you to join us and help grow our community around the project. Explore Fluss, contribute to its development, and build the next generation of data pipelines.\\n\\nMake sure to keep an eye on the project, give it a try and if you like it, don\u2019t forget to give it some \u2764\ufe0f via \u2b50 on [GitHub](https://github.com/alibaba/fluss)\\n\\n### Getting Started\\n- Visit the [GitHub repository](https://github.com/alibaba/fluss).\\n- Check out the [quickstart guide](https://alibaba.github.io/fluss-docs/docs/quickstart/flink/).\\n\\n### Additional Resources\\n- Announcement Blog Post: [Introducing Fluss: Unified Streaming Storage For Next-Generation Data Analytics](https://www.ververica.com/blog/introducing-fluss)\\n- Flink Forward Keynote Announcement: [The Future - Introducing Fluss](https://www.ververica.academy/courses/3d163483-5040-4d60-b5b3-755c3277edf7/activities/1af290fd-05bc-4fab-90be-6ed4628399be)\\n- Flink Forward Fluss Presentation: [Is Kafka the Best Storage for Streaming Analytics?](https://www.ververica.academy/courses/3d163483-5040-4d60-b5b3-755c3277edf7/activities/a366d118-6c53-47ef-91bb-289fc2462b07)"}]}}')}}]);