"use strict";(self.webpackChunkfluss_website=self.webpackChunkfluss_website||[]).push([[9724],{7717:(e,a,s)=>{s.r(a),s.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"maintenance/tiered-storage/lakehouse-storage","title":"Lakehouse Storage","description":"\x3c!--","source":"@site/docs/maintenance/tiered-storage/lakehouse-storage.md","sourceDirName":"maintenance/tiered-storage","slug":"/maintenance/tiered-storage/lakehouse-storage","permalink":"/docs/next/maintenance/tiered-storage/lakehouse-storage","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/fluss/edit/main/website/docs/maintenance/tiered-storage/lakehouse-storage.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Lakehouse Storage","sidebar_position":3},"sidebar":"docsSidebar","previous":{"title":"Remote Storage","permalink":"/docs/next/maintenance/tiered-storage/remote-storage"},"next":{"title":"Quickstart Guides","permalink":"/docs/next/maintenance/observability/quickstart"}}');var n=s(4848),r=s(8453);const i={title:"Lakehouse Storage",sidebar_position:3},o="Lakehouse Storage",l={},d=[{value:"Enable Lakehouse Storage",id:"enable-lakehouse-storage",level:2},{value:"Lakehouse Storage Cluster Configurations",id:"lakehouse-storage-cluster-configurations",level:3},{value:"Modify <code>server.yaml</code>",id:"modify-serveryaml",level:4},{value:"Add other jars required by datalake",id:"add-other-jars-required-by-datalake",level:4},{value:"Start The Datalake Tiering Service",id:"start-the-datalake-tiering-service",level:3},{value:"Prerequisites",id:"prerequisites",level:4},{value:"Prepare required jars",id:"prepare-required-jars",level:4},{value:"Start Datalake Tiering Service",id:"start-datalake-tiering-service",level:4},{value:"Enable Lakehouse Storage Per Table",id:"enable-lakehouse-storage-per-table",level:3}];function c(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.header,{children:(0,n.jsx)(a.h1,{id:"lakehouse-storage",children:"Lakehouse Storage"})}),"\n",(0,n.jsx)(a.p,{children:"Lakehouse represents a new, open architecture that combines the best elements of data lakes and data warehouses.\nLakehouse combines data lake scalability and cost-effectiveness with data warehouse reliability and performance."}),"\n",(0,n.jsx)(a.p,{children:"Fluss leverages the well-known Lakehouse storage solutions like Apache Paimon, Apache Iceberg, Apache Hudi, Delta Lake as\nthe tiered storage layer. Currently, only Apache Paimon is supported, but more kinds of Lakehouse storage support are on the way."}),"\n",(0,n.jsx)(a.p,{children:"Fluss's datalake tiering service will tier Fluss's data to the Lakehouse storage continuously. The data in Lakehouse storage can be read both by Fluss's client in a streaming manner and accessed directly\nby external systems such as Flink, Spark, StarRocks and others. With data tiered in Lakehouse storage, Fluss\ncan gain much storage cost reduction and analytics performance improvement."}),"\n",(0,n.jsx)(a.h2,{id:"enable-lakehouse-storage",children:"Enable Lakehouse Storage"}),"\n",(0,n.jsx)(a.p,{children:"Lakehouse Storage is disabled by default, you must enable it manually."}),"\n",(0,n.jsx)(a.h3,{id:"lakehouse-storage-cluster-configurations",children:"Lakehouse Storage Cluster Configurations"}),"\n",(0,n.jsxs)(a.h4,{id:"modify-serveryaml",children:["Modify ",(0,n.jsx)(a.code,{children:"server.yaml"})]}),"\n",(0,n.jsxs)(a.p,{children:["First, you must configure the lakehouse storage in ",(0,n.jsx)(a.code,{children:"server.yaml"}),". Take Paimon as an example, you must configure the following configurations:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-yaml",children:"# Paimon configuration\ndatalake.format: paimon\n\n# the catalog config about Paimon, assuming using Filesystem catalog\ndatalake.paimon.metastore: filesystem\ndatalake.paimon.warehouse: /tmp/paimon_data_warehouse\n"})}),"\n",(0,n.jsxs)(a.p,{children:["Fluss processes Paimon configurations by removing the ",(0,n.jsx)(a.code,{children:"datalake.paimon."})," prefix and then use the remaining configuration (without the prefix ",(0,n.jsx)(a.code,{children:"datalake.paimon."}),") to create the Paimon catalog. Checkout the ",(0,n.jsx)(a.a,{href:"https://paimon.apache.org/docs/1.1/maintenance/configurations/",children:"Paimon documentation"})," for more details on the available configurations."]}),"\n",(0,n.jsx)(a.p,{children:"For example, if you want to configure to use Hive catalog, you can configure like following:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-yaml",children:"datalake.format: paimon\ndatalake.paimon.metastore: hive\ndatalake.paimon.uri: thrift://<hive-metastore-host-name>:<port>\ndatalake.paimon.warehouse: hdfs:///path/to/warehouse\n"})}),"\n",(0,n.jsx)(a.h4,{id:"add-other-jars-required-by-datalake",children:"Add other jars required by datalake"}),"\n",(0,n.jsxs)(a.p,{children:["While Fluss includes the core Paimon library, additional jars may still need to be manually added to ",(0,n.jsx)(a.code,{children:"${FLUSS_HOME}/plugins/paimon/"})," according to your needs.\nFor example, for OSS filesystem support, you need to put ",(0,n.jsx)(a.code,{children:"paimon-oss-<paimon_version>.jar"})," into directory ",(0,n.jsx)(a.code,{children:"${FLUSS_HOME}/plugins/paimon/"}),"."]}),"\n",(0,n.jsx)(a.h3,{id:"start-the-datalake-tiering-service",children:"Start The Datalake Tiering Service"}),"\n",(0,n.jsx)(a.p,{children:"Then, you must start the datalake tiering service to tier Fluss's data to the lakehouse storage."}),"\n",(0,n.jsx)(a.h4,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"A running Flink cluster (currently only Flink is supported as the tiering backend)"}),"\n",(0,n.jsxs)(a.li,{children:["Download ",(0,n.jsx)(a.a,{href:"https://repo1.maven.org/maven2/com/alibaba/fluss/fluss-flink-tiering/0.8-SNAPSHOT/fluss-flink-tiering-0.8-SNAPSHOT.jar",children:"fluss-flink-tiering-0.8-SNAPSHOT.jar"})]}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"prepare-required-jars",children:"Prepare required jars"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Put ",(0,n.jsx)(a.a,{href:"/downloads",children:"fluss-flink connector jar"})," into ",(0,n.jsx)(a.code,{children:"${FLINK_HOME}/lib"}),", you should choose a connector version matching your Flink version. If you're using Flink 1.20, please use ",(0,n.jsx)(a.a,{href:"https://repo1.maven.org/maven2/com/alibaba/fluss/fluss-flink-1.20/0.8-SNAPSHOT/fluss-flink-1.20-0.8-SNAPSHOT.jar",children:"fluss-flink-1.20-0.8-SNAPSHOT.jar"})]}),"\n",(0,n.jsxs)(a.li,{children:["If you are using ",(0,n.jsx)(a.a,{href:"http://aws.amazon.com/s3/",children:"Amazon S3"}),", ",(0,n.jsx)(a.a,{href:"https://www.aliyun.com/product/oss",children:"Aliyun OSS"})," or ",(0,n.jsx)(a.a,{href:"https://hadoop.apache.org/docs/stable/",children:"HDFS(Hadoop Distributed File System)"})," as Fluss's ",(0,n.jsx)(a.a,{href:"/docs/next/maintenance/tiered-storage/remote-storage",children:"remote storage"}),",\nyou should download the corresponding ",(0,n.jsx)(a.a,{href:"/downloads#filesystem-jars",children:"Fluss filesystem jar"})," and also put it into ",(0,n.jsx)(a.code,{children:"${FLINK_HOME}/lib"})]}),"\n",(0,n.jsxs)(a.li,{children:["Put ",(0,n.jsx)(a.a,{href:"https://repo1.maven.org/maven2/com/alibaba/fluss/fluss-lake-paimon/0.8-SNAPSHOT/fluss-lake-paimon-0.8-SNAPSHOT.jar",children:"fluss-lake-paimon jar"})," into ",(0,n.jsx)(a.code,{children:"${FLINK_HOME}/lib"}),", currently only paimon is supported, so you can only choose ",(0,n.jsx)(a.code,{children:"fluss-lake-paimon"})]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://flink.apache.org/downloads/",children:"Download"})," pre-bundled Hadoop jar ",(0,n.jsx)(a.code,{children:"flink-shaded-hadoop-2-uber-*.jar"})," and put into ",(0,n.jsx)(a.code,{children:"${FLINK_HOME}/lib"})]}),"\n",(0,n.jsxs)(a.li,{children:["Put Paimon's ",(0,n.jsx)(a.a,{href:"https://paimon.apache.org/docs/1.1/project/download/",children:"filesystem jar"})," into ",(0,n.jsx)(a.code,{children:"${FLINK_HOME}/lib"}),", if you use s3 to store paimon data, please put ",(0,n.jsx)(a.code,{children:"paimon-s3"})," jar into ",(0,n.jsx)(a.code,{children:"${FLINK_HOME}/lib"})]}),"\n",(0,n.jsx)(a.li,{children:"The other jars that Paimon may require, for example, if you use HiveCatalog, you will need to put hive related jars"}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"start-datalake-tiering-service",children:"Start Datalake Tiering Service"}),"\n",(0,n.jsxs)(a.p,{children:["After the Flink Cluster has been started, you can execute the ",(0,n.jsx)(a.code,{children:"fluss-flink-tiering-0.8-SNAPSHOT.jar"})," by using the following command to start datalake tiering service:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:"<FLINK_HOME>/bin/flink run /path/to/fluss-flink-tiering-0.8-SNAPSHOT.jar \\\n    --fluss.bootstrap.servers localhost:9123 \\\n    --datalake.format paimon \\\n    --datalake.paimon.metastore filesystem \\\n    --datalake.paimon.warehouse /tmp/paimon\n"})}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"Note:"})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["The ",(0,n.jsx)(a.code,{children:"fluss.bootstrap.servers"})," should be the bootstrap server address of your Fluss cluster. You must configure all options with the ",(0,n.jsx)(a.code,{children:"datalake."})," prefix in the ",(0,n.jsx)(a.a,{href:"#modify-serveryaml",children:"server.yaml"})," file to run the tiering service. In this case, these parameters are ",(0,n.jsx)(a.code,{children:"--datalake.format"}),", ",(0,n.jsx)(a.code,{children:"--datalake.paimon.metastore"}),", and ",(0,n.jsx)(a.code,{children:"--datalake.paimon.warehouse"}),"."]}),"\n",(0,n.jsx)(a.li,{children:"The Flink tiering service is stateless, and you can run multiple tiering services simultaneously to tier tables in Fluss.\nThese tiering services are coordinated by the Fluss cluster to ensure exactly-once semantics when tiering data to the lake storage. This means you can freely scale the service up or down according to your workload."}),"\n",(0,n.jsxs)(a.li,{children:["This follows the standard practice for ",(0,n.jsx)(a.a,{href:"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/cli/",children:"submitting jobs to Flink"}),", where you can use the ",(0,n.jsx)(a.code,{children:"-D"})," parameter to specify Flink-related configurations.\nFor example, if you want to set the tiering service job name to ",(0,n.jsx)(a.code,{children:"My Fluss Tiering Service1"})," and use ",(0,n.jsx)(a.code,{children:"3"})," as the job parallelism, you can use the following command:"]}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:'<FLINK_HOME>/bin/flink run \\\n    -Dpipeline.name="My Fluss Tiering Service1" \\\n    -Dparallelism.default=3 \\\n    /path/to/fluss-flink-tiering-0.8-SNAPSHOT.jar \\\n    --fluss.bootstrap.servers localhost:9123 \\\n    --datalake.format paimon \\\n    --datalake.paimon.metastore filesystem \\\n    --datalake.paimon.warehouse /tmp/paimon\n'})}),"\n",(0,n.jsx)(a.h3,{id:"enable-lakehouse-storage-per-table",children:"Enable Lakehouse Storage Per Table"}),"\n",(0,n.jsxs)(a.p,{children:["To enable lakehouse storage for a table, the table must be created with the option ",(0,n.jsx)(a.code,{children:"'table.datalake.enabled' = 'true'"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["Another option ",(0,n.jsx)(a.code,{children:"table.datalake.freshness"}),", allows per-table configuration of data freshness in the datalake.\nIt defines the maximum amount of time that the datalake table's content should lag behind updates to the Fluss table.\nBased on this target freshness, the Fluss tiering service automatically moves data from the Fluss table and updates to the datalake table, so that the data in the datalake table is kept up to date within this target.\nThe default is ",(0,n.jsx)(a.code,{children:"3min"}),", if the data does not need to be as fresh, you can specify a longer target freshness time to reduce costs."]})]})}function h(e={}){const{wrapper:a}={...(0,r.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},8453:(e,a,s)=>{s.d(a,{R:()=>i,x:()=>o});var t=s(6540);const n={},r=t.createContext(n);function i(e){const a=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:i(e.components),t.createElement(r.Provider,{value:a},e.children)}}}]);